{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "I've been building some algorithms from scratch in order to re-familiarize myself with algorithms and practice coding. Thus far, I've done K-Means, K-Nearest Neighbors, and Linear Regression. Today, I'm going to do a neural network. Fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a neural network, I'm first going to need some neurons. Each neuron should take inputs, multiply those inputs by some weights, add them together, add a bias term, and then perform an activation (run the sum through a function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron():\n",
    "    def __init__(self, w, b, x):\n",
    "        self.weights = np.array(w)\n",
    "        self.biases = np.array(b)\n",
    "        self.x = np.array(x)\n",
    "    \n",
    "    def sigmoid_activate(self):\n",
    "        #Formula for sigmoid activation: 1/(1+e^x)\n",
    "        s= self.weights*self.x+self.biases\n",
    "        return 1/(1+2.718281828459045**-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neuron and pass it our data. For this neuron, the data will be a single number, 3. We can also give it a weight and a bias value so that it can properly computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =neuron(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a neuron. It takes our data (3), but hasn't yet added a weight and bias, and run that through the activation function. To do that, we'll call the activation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " y =x.sigmoid_activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933071490757153"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but suppose we wanted multiple neurons..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "two_neurons = []\n",
    "two_neurons.append(neuron(1,2,3))\n",
    "two_neurons.append(neuron(4,5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my first layer is called \"two_neurons\". I've passed it data, which it has stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the activations of those neurons. I'm going to store it in a list. This isn't the most efficient way of doing things, but it's more of a demonstration that the concept is right before going deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "for n in two_neurons:\n",
    "    activations.append(np.array(n.sigmoid_activate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.99330715), array(1.)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the activations of one layer as the inputs of the next layer. This is the essence of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_layer = neuron([3,2],[2,1],activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99317233, 0.95257413])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_layer.sigmoid_activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I guess that's technically a tiny little neural network! It doesn't do anything useful, and it has arbitrarily assigned weights and biases that don't update, but it has all of the components there! Next, we should:\n",
    "1. Give it a concrete task\n",
    "2. Have it give an output to match that task\n",
    "3. Give it a cost function to see how close the outputs are\n",
    "4. Implement backpropagation, so that we can update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
