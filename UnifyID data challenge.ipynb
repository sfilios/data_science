{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began this notebook by addressing the first problem in the challenge, to deploy a neural network on the MNIST fashion data set. I recently completed a tutorial on Kaggle in which I completed such an exercise, so I took that and applied it here. It was originally a fork of this notebook https://www.kaggle.com/dansbecker/exercise-dropout-and-strides-for-larger-models#\n",
    "\n",
    "I then realized that I was unfamiliar with the idea that the learning in a deep neural network could be plotted using information theory. After reading through the paper at the required computations, I discovered that I was unaware of a way to have tensorflow return values necessary to calculate mutual information. I would need to be able to bin the arctan outputs of each node/neuron, and do this for each layer. This would give me the joint distributions, from which I could calculate mutual information. This would be an intensive project that stretched my abilities.\n",
    "\n",
    "Fortunately, as I was reading blog posts and papers on this topic, I came across a paper that published their work on github, and had done almost the same project. Because I was instructed that prior art was fair use, I decided to deploy that repository here as well. Clearly, they did all the heavy lifting and I merely repurposed it to use the fashion_MNIST instead of the MNIST. This repository can be found at https://github.com/artemyk/ibsgd and came from this paper https://openreview.net/pdf?id=ry_WPG-A- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code represnts my first effort to deploy the NN before implementing the information plane\n",
    "\n",
    "#import the libraries that I will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "#because the images are 28X28 pixels, we set variables for those here for later use\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#because we have 10 possible fashion items, our final number of classes is 10.\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to extract the labels and reshape the pixel intensity data (not as a column for each, but \n",
    "#as a 28X28 grid again) before applying model\n",
    "# def prep_data(rawx, rawy):    \n",
    "#     num_images = rawx.shape[0] # the number of images is equal to the size of one of the columns\n",
    "#     out_y = keras.utils.to_categorical(rawy, num_classes)\n",
    "#     #    x_as_array = raw.values[:,:] #take all the rows, and take all columns\n",
    "#     x_shaped_array = rawx.reshape(num_images, img_rows, img_cols, 1) #reshape the X values into a 4d array.\n",
    "#     out_x = x_shaped_array / 255 #divide by a large number to make the outputs between 0 and 1. Helps with optimizing.\n",
    "#     return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the MNIST READER python file from the fashion MNIST github repo\n",
    "# https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "\n",
    "# def load_mnist(path, kind='train'):\n",
    "#     import os\n",
    "#     import gzip\n",
    "#     import numpy as np\n",
    "\n",
    "#     \"\"\"Load MNIST data from `path`\"\"\"\n",
    "#     labels_path = os.path.join(path,\n",
    "#                                '%s-labels-idx1-ubyte.gz'\n",
    "#                                % kind)\n",
    "#     images_path = os.path.join(path,\n",
    "#                                '%s-images-idx3-ubyte.gz'\n",
    "#                                % kind)\n",
    "\n",
    "#     with gzip.open(labels_path, 'rb') as lbpath:\n",
    "#         labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "#                                offset=8)\n",
    "\n",
    "#     with gzip.open(images_path, 'rb') as imgpath:\n",
    "#         images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "#                                offset=16).reshape(len(labels), 784)\n",
    "\n",
    "#     return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of the work provided in the previously cited paper, at https://openreview.net/pdf?id=ry_WPG-A- (Cox et al., 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from pathlib2 import Path\n",
    "from collections import namedtuple\n",
    "\n",
    "def get_mnist():\n",
    "    # Returns two namedtuples, with MNIST training and testing data\n",
    "    #   trn.X is training data\n",
    "    #   trn.y is trainiing class, with numbers from 0 to 9\n",
    "    #   trn.Y is training class, but coded as a 10-dim vector with one entry set to 1\n",
    "    # similarly for tst\n",
    "    nb_classes = 10     \n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    X_train = np.reshape(X_train, [X_train.shape[0], -1]).astype('float32') / 255.\n",
    "    X_test  = np.reshape(X_test , [X_test.shape[0] , -1]).astype('float32') / 255.\n",
    "    #X_train = X_train * 2.0 - 1.0\n",
    "    #X_test  = X_test  * 2.0 - 1.0\n",
    "\n",
    "    Y_train = keras.utils.np_utils.to_categorical(y_train, nb_classes).astype('float32')\n",
    "    Y_test  = keras.utils.np_utils.to_categorical(y_test, nb_classes).astype('float32')\n",
    "\n",
    "    Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "    trn = Dataset(X_train, Y_train, y_train, nb_classes)\n",
    "    tst = Dataset(X_test , Y_test, y_test, nb_classes)\n",
    "\n",
    "    del X_train, X_test, Y_train, Y_test, y_train, y_test\n",
    " \n",
    "    return trn, tst\n",
    "\n",
    "\n",
    "def construct_full_dataset(trn, tst):\n",
    "    Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "    X = np.concatenate((trn.X,tst.X))\n",
    "    y = np.concatenate((trn.y,tst.y))\n",
    "    Y = np.concatenate((trn.Y,tst.Y))\n",
    "    return Dataset(X, Y, y, trn.nb_classes)\n",
    " \n",
    "def load_data():\n",
    "    \"\"\"Load the data\n",
    "    name - the name of the dataset\n",
    "    return object with data and labels\"\"\"\n",
    "    print ('Loading Data...')\n",
    "    C = type('type_C', (object,), {})\n",
    "    data_sets = C()\n",
    "    d = sio.loadmat('datasets/var_u.mat')\n",
    "    F = d['F']\n",
    "    y = d['y']\n",
    "    C = type('type_C', (object,), {})\n",
    "    data_sets = C()\n",
    "    data_sets.data = F\n",
    "    data_sets.labels = np.squeeze(np.concatenate((y[None, :], 1 - y[None, :]), axis=0).T)\n",
    "    return data_sets\n",
    "\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    \"\"\"Shuffle the arrays randomly\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def data_shuffle(data_sets_org, percent_of_train, min_test_data=80, shuffle_data=False):\n",
    "    \"\"\"Divided the data to train and test and shuffle it\"\"\"\n",
    "    perc = lambda i, t: np.rint((i * t) / 100).astype(np.int32)\n",
    "    C = type('type_C', (object,), {})\n",
    "    data_sets = C()\n",
    "    stop_train_index = perc(percent_of_train[0], data_sets_org.data.shape[0])\n",
    "    start_test_index = stop_train_index\n",
    "    if percent_of_train > min_test_data:\n",
    "        start_test_index = perc(min_test_data, data_sets_org.data.shape[0])\n",
    "    data_sets.train = C()\n",
    "    data_sets.test = C()\n",
    "    if shuffle_data:\n",
    "        shuffled_data, shuffled_labels = shuffle_in_unison_inplace(data_sets_org.data, data_sets_org.labels)\n",
    "    else:\n",
    "        shuffled_data, shuffled_labels = data_sets_org.data, data_sets_org.labels\n",
    "    data_sets.train.data = shuffled_data[:stop_train_index, :]\n",
    "    data_sets.train.labels = shuffled_labels[:stop_train_index, :]\n",
    "    data_sets.test.data = shuffled_data[start_test_index:, :]\n",
    "    data_sets.test.labels = shuffled_labels[start_test_index:, :]\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = load_mnist('', kind='train')\n",
    "# X_test, y_test = load_mnist('', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = prep_data(X_train, y_train)\n",
    "# Xval, yval = prep_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and create a model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(24, kernel_size=(3,3), strides = 2, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "# model.add(Conv2D(24, kernel_size=(3,3), strides = 2, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Conv2D(24, kernel_size=(3,3), activation='relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = keras.optimizers.SGD(lr=0.02)\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics =['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/artemyk/ibsgd/blob/iclr2018/simplebinmi.py\n",
    "# Simplified MI computation code from https://github.com/ravidziv/IDNNs\n",
    "import numpy as np\n",
    "\n",
    "def get_unique_probs(x):\n",
    "    uniqueids = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, unique_inverse, unique_counts = np.unique(uniqueids, return_index=False, return_inverse=True, return_counts=True)\n",
    "    return np.asarray(unique_counts / float(sum(unique_counts))), unique_inverse\n",
    "\n",
    "def bin_calc_information(inputdata, layerdata, num_of_bins):\n",
    "    p_xs, unique_inverse_x = get_unique_probs(inputdata)\n",
    "    \n",
    "    bins = np.linspace(-1, 1, num_of_bins, dtype='float32') \n",
    "    digitized = bins[np.digitize(np.squeeze(layerdata.reshape(1, -1)), bins) - 1].reshape(len(layerdata), -1)\n",
    "    p_ts, _ = get_unique_probs( digitized )\n",
    "    \n",
    "    H_LAYER = -np.sum(p_ts * np.log(p_ts))\n",
    "    H_LAYER_GIVEN_INPUT = 0.\n",
    "    for xval in unique_inverse_x:\n",
    "        p_t_given_x, _ = get_unique_probs(digitized[unique_inverse_x == xval, :])\n",
    "        H_LAYER_GIVEN_INPUT += - p_xs[xval] * np.sum(p_t_given_x * np.log(p_t_given_x))\n",
    "    return H_LAYER - H_LAYER_GIVEN_INPUT\n",
    "\n",
    "def bin_calc_information2(labelixs, layerdata, binsize):\n",
    "    # This is even further simplified, where we use np.floor instead of digitize\n",
    "    def get_h(d):\n",
    "        digitized = np.floor(d / binsize).astype('int')\n",
    "        p_ts, _ = get_unique_probs( digitized )\n",
    "        return -np.sum(p_ts * np.log(p_ts))\n",
    "\n",
    "    H_LAYER = get_h(layerdata)\n",
    "    H_LAYER_GIVEN_OUTPUT = 0\n",
    "    for label, ixs in labelixs.items():\n",
    "        H_LAYER_GIVEN_OUTPUT += ixs.mean() * get_h(layerdata[ixs,:])\n",
    "    return H_LAYER, H_LAYER - H_LAYER_GIVEN_OUTPUT\n",
    "    # H_LAYER_GIVEN_INPUT = 0.\n",
    "    # for xval in unique_inverse_x:\n",
    "    #     p_t_given_x, _ = get_unique_probs(digitized[unique_inverse_x == xval, :])\n",
    "    #     H_LAYER_GIVEN_INPUT += - p_xs[xval] * np.sum(p_t_given_x * np.log(p_t_given_x))\n",
    "    # print('here', H_LAYER_GIVEN_INPUT)\n",
    "# return H_LAYER - H_LAYER_GIVEN_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "\n",
    "import utils\n",
    "\n",
    "class LoggingReporter(keras.callbacks.Callback):\n",
    "    def __init__(self, cfg, trn, tst, do_save_func=None, *kargs, **kwargs):\n",
    "        super(LoggingReporter, self).__init__(*kargs, **kwargs)\n",
    "        self.cfg = cfg # Configuration options dictionary\n",
    "        self.trn = trn  # Train data\n",
    "        self.tst = tst  # Test data\n",
    "        \n",
    "        if 'FULL_MI' not in cfg:\n",
    "            self.cfg['FULL_MI'] = False # Whether to compute MI on train and test data, or just test\n",
    "            \n",
    "        if self.cfg['FULL_MI']:\n",
    "            self.full = utils.construct_full_dataset(trn,tst)\n",
    "        \n",
    "        # do_save_func(epoch) should return True if we should save on that epoch\n",
    "        self.do_save_func = do_save_func\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not os.path.exists(self.cfg['SAVE_DIR']):\n",
    "            print(\"Making directory\", self.cfg['SAVE_DIR'])\n",
    "            os.makedirs(self.cfg['SAVE_DIR'])\n",
    "            \n",
    "        # Indexes of the layers which we keep track of. Basically, this will be any layer \n",
    "        # which has a 'kernel' attribute, which is essentially the \"Dense\" or \"Dense\"-like layers\n",
    "        self.layerixs = []\n",
    "    \n",
    "        # Functions return activity of each layer\n",
    "        self.layerfuncs = []\n",
    "        \n",
    "        # Functions return weights of each layer\n",
    "        self.layerweights = []\n",
    "        for lndx, l in enumerate(self.model.layers):\n",
    "            if hasattr(l, 'kernel'):\n",
    "                self.layerixs.append(lndx)\n",
    "                self.layerfuncs.append(K.function(self.model.inputs, [l.output,]))\n",
    "                self.layerweights.append(l.kernel)\n",
    "            \n",
    "        input_tensors = [self.model.inputs[0],\n",
    "                         self.model.sample_weights[0],\n",
    "                         self.model.targets[0],\n",
    "                         K.learning_phase()]\n",
    "        # Get gradients of all the relevant layers at once\n",
    "        grads = self.model.optimizer.get_gradients(self.model.total_loss, self.layerweights)\n",
    "        self.get_gradients = K.function(inputs=input_tensors,\n",
    "                                        outputs=grads)\n",
    "        \n",
    "        # Get cross-entropy loss\n",
    "        self.get_loss = K.function(inputs=input_tensors, outputs=[self.model.total_loss,])\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.do_save_func is not None and not self.do_save_func(epoch):\n",
    "            # Don't log this epoch\n",
    "            self._log_gradients = False\n",
    "        else:\n",
    "            # We will log this epoch.  For each batch in this epoch, we will save the gradients (in on_batch_begin)\n",
    "            # We will then compute means and vars of these gradients\n",
    "            \n",
    "            self._log_gradients = True\n",
    "            self._batch_weightnorm = []\n",
    "                \n",
    "            self._batch_gradients = [ [] for _ in self.model.layers[1:] ]\n",
    "            \n",
    "            # Indexes of all the training data samples. These are shuffled and read-in in chunks of SGD_BATCHSIZE\n",
    "            ixs = list(range(len(self.trn.X)))\n",
    "            np.random.shuffle(ixs)\n",
    "            self._batch_todo_ixs = ixs\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if not self._log_gradients:\n",
    "            # We are not keeping track of batch gradients, so do nothing\n",
    "            return\n",
    "        \n",
    "        # Sample a batch\n",
    "        batchsize = self.cfg['SGD_BATCHSIZE']\n",
    "        cur_ixs = self._batch_todo_ixs[:batchsize]\n",
    "        # Advance the indexing, so next on_batch_begin samples a different batch\n",
    "        self._batch_todo_ixs = self._batch_todo_ixs[batchsize:]\n",
    "        \n",
    "        # Get gradients for this batch\n",
    "        inputs = [self.trn.X[cur_ixs,:],  # Inputs\n",
    "                  [1,]*len(cur_ixs),      # Uniform sample weights\n",
    "                  self.trn.Y[cur_ixs,:],  # Outputs\n",
    "                  1                       # Training phase\n",
    "                 ]\n",
    "        for lndx, g in enumerate(self.get_gradients(inputs)):\n",
    "            # g is gradients for weights of lndx's layer\n",
    "            oneDgrad = np.reshape(g, -1, 1)                  # Flatten to one dimensional vector\n",
    "            self._batch_gradients[lndx].append(oneDgrad)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.do_save_func is not None and not self.do_save_func(epoch):\n",
    "            # Don't log this epoch\n",
    "            return\n",
    "        \n",
    "        # Get overall performance\n",
    "        loss = {}\n",
    "        for cdata, cdataname, istrain in ((self.trn,'trn',1), (self.tst, 'tst',0)):\n",
    "            loss[cdataname] = self.get_loss([cdata.X, [1,]*len(cdata.X), cdata.Y, istrain])[0].flat[0]\n",
    "            \n",
    "        data = {\n",
    "            'weights_norm' : [],   # L2 norm of weights\n",
    "            'gradmean'     : [],   # Mean of gradients\n",
    "            'gradstd'      : [],   # Std of gradients\n",
    "            'activity_tst' : []    # Activity in each layer for test set\n",
    "        }\n",
    "        \n",
    "        for lndx, layerix in enumerate(self.layerixs):\n",
    "            clayer = self.model.layers[layerix]\n",
    "            \n",
    "            data['weights_norm'].append( np.linalg.norm(K.get_value(clayer.kernel)) )\n",
    "            \n",
    "            stackedgrads = np.stack(self._batch_gradients[lndx], axis=1)\n",
    "            data['gradmean'    ].append( np.linalg.norm(stackedgrads.mean(axis=1)) )\n",
    "            data['gradstd'     ].append( np.linalg.norm(stackedgrads.std(axis=1)) )\n",
    "            \n",
    "            if self.cfg['FULL_MI']:\n",
    "                data['activity_tst'].append(self.layerfuncs[lndx]([self.full.X,])[0])\n",
    "            else:\n",
    "                data['activity_tst'].append(self.layerfuncs[lndx]([self.tst.X,])[0])\n",
    "            \n",
    "        fname = self.cfg['SAVE_DIR'] + \"/epoch%08d\"% epoch\n",
    "        print(\"Saving\", fname)\n",
    "        with open(fname, 'wb') as f:\n",
    "            cPickle.dump({'ACTIVATION':self.cfg['ACTIVATION'], 'epoch':epoch, 'data':data, 'loss':loss}, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def Kget_dists(X):\n",
    "    \"\"\"Keras code to compute the pairwise distance matrix for a set of\n",
    "    vectors specifie by the matrix X.\n",
    "    \"\"\"\n",
    "    x2 = K.expand_dims(K.sum(K.square(X), axis=1), 1)\n",
    "    dists = x2 + K.transpose(x2) - 2*K.dot(X, K.transpose(X))\n",
    "    return dists\n",
    "\n",
    "def get_shape(x):\n",
    "    dims = K.cast( K.shape(x)[1], K.floatx() ) \n",
    "    N    = K.cast( K.shape(x)[0], K.floatx() )\n",
    "    return dims, N\n",
    "\n",
    "def entropy_estimator_kl(x, var):\n",
    "    # KL-based upper bound on entropy of mixture of Gaussians with covariance matrix var * I \n",
    "    #  see Kolchinsky and Tracey, Estimating Mixture Entropy with Pairwise Distances, Entropy, 2017. Section 4.\n",
    "    #  and Kolchinsky and Tracey, Nonlinear Information Bottleneck, 2017. Eq. 10\n",
    "    dims, N = get_shape(x)\n",
    "    dists = Kget_dists(x)\n",
    "    dists2 = dists / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs = K.logsumexp(-dists2, axis=1) - K.log(N) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return dims/2 + h\n",
    "\n",
    "def entropy_estimator_bd(x, var):\n",
    "    # Bhattacharyya-based lower bound on entropy of mixture of Gaussians with covariance matrix var * I \n",
    "    #  see Kolchinsky and Tracey, Estimating Mixture Entropy with Pairwise Distances, Entropy, 2017. Section 4.\n",
    "    dims, N = get_shape(x)\n",
    "    val = entropy_estimator_kl(x,4*var)\n",
    "    return val + np.log(0.25)*dims/2\n",
    "\n",
    "def kde_condentropy(output, var):\n",
    "    # Return entropy of a multivariate Gaussian, in nats\n",
    "    dims = output.shape[1]\n",
    "    return (dims/2.0)*(np.log(2*np.pi*var) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "clayer = input_layer\n",
    "for n in cfg['LAYER_DIMS']:\n",
    "   clayer = keras.layers.Dense(n, activation=cfg['ACTIVATION'])(clayer)\n",
    "output_layer = keras.layers.Dense(trn.nb_classes, activation='softmax')(clayer)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "optimizer = keras.optimizers.SGD(lr=cfg['SGD_LEARNINGRATE'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "#import utils\n",
    "#import loggingreporter \n",
    "\n",
    "cfg = {}\n",
    "cfg['SGD_BATCHSIZE']    = 128\n",
    "cfg['SGD_LEARNINGRATE'] = 0.001\n",
    "cfg['NUM_EPOCHS']       = 1000\n",
    "\n",
    "#cfg['ACTIVATION'] = 'relu'\n",
    "cfg['ACTIVATION'] = 'tanh'\n",
    "# How many hidden neurons to put into each of the layers\n",
    "#cfg['LAYER_DIMS'] = [1024, 20, 20, 20]\n",
    "cfg['LAYER_DIMS'] = [32, 28, 24, 20, 16, 12, 8, 8]\n",
    "#cfg['LAYER_DIMS'] = [128, 64, 32, 16, 16] # 0.967 w. 128\n",
    "#cfg['LAYER_DIMS'] = [20, 20, 20, 20, 20, 20] # 0.967 w. 128\n",
    "ARCH_NAME =  '-'.join(map(str,cfg['LAYER_DIMS']))\n",
    "trn, tst = get_mnist()\n",
    "\n",
    "# Where to save activation and weights data\n",
    "cfg['SAVE_DIR'] = 'rawdata/' + cfg['ACTIVATION'] + '_' + ARCH_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.4466 - acc: 0.8488 - val_loss: 0.4946 - val_acc: 0.8295\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000000\n",
      "Epoch 2/1000\n",
      " - 3s - loss: 0.4452 - acc: 0.8494 - val_loss: 0.4957 - val_acc: 0.8306\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000001\n",
      "Epoch 3/1000\n",
      " - 2s - loss: 0.4439 - acc: 0.8499 - val_loss: 0.4917 - val_acc: 0.8323\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000002\n",
      "Epoch 4/1000\n",
      " - 3s - loss: 0.4425 - acc: 0.8503 - val_loss: 0.4917 - val_acc: 0.8313\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000003\n",
      "Epoch 5/1000\n",
      " - 3s - loss: 0.4412 - acc: 0.8507 - val_loss: 0.4904 - val_acc: 0.8334\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000004\n",
      "Epoch 6/1000\n",
      " - 2s - loss: 0.4397 - acc: 0.8512 - val_loss: 0.4885 - val_acc: 0.8339\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000005\n",
      "Epoch 7/1000\n",
      " - 2s - loss: 0.4380 - acc: 0.8516 - val_loss: 0.4887 - val_acc: 0.8306\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000006\n",
      "Epoch 8/1000\n",
      " - 2s - loss: 0.4370 - acc: 0.8520 - val_loss: 0.4875 - val_acc: 0.8332\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000007\n",
      "Epoch 9/1000\n",
      " - 2s - loss: 0.4358 - acc: 0.8524 - val_loss: 0.4850 - val_acc: 0.8331\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000008\n",
      "Epoch 10/1000\n",
      " - 2s - loss: 0.4344 - acc: 0.8530 - val_loss: 0.4870 - val_acc: 0.8346\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000009\n",
      "Epoch 11/1000\n",
      " - 2s - loss: 0.4331 - acc: 0.8533 - val_loss: 0.4827 - val_acc: 0.8349\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000010\n",
      "Epoch 12/1000\n",
      " - 3s - loss: 0.4318 - acc: 0.8539 - val_loss: 0.4831 - val_acc: 0.8363\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000011\n",
      "Epoch 13/1000\n",
      " - 2s - loss: 0.4305 - acc: 0.8545 - val_loss: 0.4810 - val_acc: 0.8371\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000012\n",
      "Epoch 14/1000\n",
      " - 3s - loss: 0.4292 - acc: 0.8549 - val_loss: 0.4800 - val_acc: 0.8363\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000013\n",
      "Epoch 15/1000\n",
      " - 2s - loss: 0.4282 - acc: 0.8553 - val_loss: 0.4790 - val_acc: 0.8367\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000014\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.4271 - acc: 0.8549 - val_loss: 0.4778 - val_acc: 0.8364\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000015\n",
      "Epoch 17/1000\n",
      " - 3s - loss: 0.4257 - acc: 0.8557 - val_loss: 0.4769 - val_acc: 0.8365\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000016\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.4247 - acc: 0.8557 - val_loss: 0.4758 - val_acc: 0.8386\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000017\n",
      "Epoch 19/1000\n",
      " - 3s - loss: 0.4234 - acc: 0.8562 - val_loss: 0.4754 - val_acc: 0.8371\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000018\n",
      "Epoch 20/1000\n",
      " - 3s - loss: 0.4223 - acc: 0.8570 - val_loss: 0.4746 - val_acc: 0.8376\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000019\n",
      "Epoch 21/1000\n",
      " - 3s - loss: 0.4212 - acc: 0.8572 - val_loss: 0.4728 - val_acc: 0.8386\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000020\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 0.4201 - acc: 0.8578 - val_loss: 0.4725 - val_acc: 0.8391\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 0.4189 - acc: 0.8579 - val_loss: 0.4728 - val_acc: 0.8387\n",
      "Epoch 24/1000\n",
      " - 2s - loss: 0.4177 - acc: 0.8587 - val_loss: 0.4715 - val_acc: 0.8399\n",
      "Epoch 25/1000\n",
      " - 1s - loss: 0.4168 - acc: 0.8582 - val_loss: 0.4700 - val_acc: 0.8395\n",
      "Epoch 26/1000\n",
      " - 3s - loss: 0.4157 - acc: 0.8589 - val_loss: 0.4693 - val_acc: 0.8402\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000025\n",
      "Epoch 27/1000\n",
      " - 1s - loss: 0.4147 - acc: 0.8591 - val_loss: 0.4680 - val_acc: 0.8423\n",
      "Epoch 28/1000\n",
      " - 1s - loss: 0.4137 - acc: 0.8596 - val_loss: 0.4666 - val_acc: 0.8404\n",
      "Epoch 29/1000\n",
      " - 2s - loss: 0.4126 - acc: 0.8600 - val_loss: 0.4674 - val_acc: 0.8400\n",
      "Epoch 30/1000\n",
      " - 1s - loss: 0.4116 - acc: 0.8602 - val_loss: 0.4651 - val_acc: 0.8420\n",
      "Epoch 31/1000\n",
      " - 2s - loss: 0.4106 - acc: 0.8600 - val_loss: 0.4645 - val_acc: 0.8406\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000030\n",
      "Epoch 32/1000\n",
      " - 1s - loss: 0.4095 - acc: 0.8604 - val_loss: 0.4636 - val_acc: 0.8410\n",
      "Epoch 33/1000\n",
      " - 1s - loss: 0.4085 - acc: 0.8612 - val_loss: 0.4633 - val_acc: 0.8422\n",
      "Epoch 34/1000\n",
      " - 1s - loss: 0.4075 - acc: 0.8612 - val_loss: 0.4628 - val_acc: 0.8428\n",
      "Epoch 35/1000\n",
      " - 1s - loss: 0.4068 - acc: 0.8614 - val_loss: 0.4625 - val_acc: 0.8432\n",
      "Epoch 36/1000\n",
      " - 3s - loss: 0.4058 - acc: 0.8618 - val_loss: 0.4603 - val_acc: 0.8432\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000035\n",
      "Epoch 37/1000\n",
      " - 1s - loss: 0.4047 - acc: 0.8625 - val_loss: 0.4601 - val_acc: 0.8439\n",
      "Epoch 38/1000\n",
      " - 1s - loss: 0.4039 - acc: 0.8623 - val_loss: 0.4591 - val_acc: 0.8432\n",
      "Epoch 39/1000\n",
      " - 1s - loss: 0.4028 - acc: 0.8622 - val_loss: 0.4622 - val_acc: 0.8424\n",
      "Epoch 40/1000\n",
      " - 1s - loss: 0.4020 - acc: 0.8630 - val_loss: 0.4582 - val_acc: 0.8451\n",
      "Epoch 41/1000\n",
      " - 3s - loss: 0.4011 - acc: 0.8634 - val_loss: 0.4579 - val_acc: 0.8431\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000040\n",
      "Epoch 42/1000\n",
      " - 2s - loss: 0.4002 - acc: 0.8634 - val_loss: 0.4561 - val_acc: 0.8436\n",
      "Epoch 43/1000\n",
      " - 1s - loss: 0.3995 - acc: 0.8637 - val_loss: 0.4554 - val_acc: 0.8453\n",
      "Epoch 44/1000\n",
      " - 1s - loss: 0.3984 - acc: 0.8642 - val_loss: 0.4544 - val_acc: 0.8450\n",
      "Epoch 45/1000\n",
      " - 1s - loss: 0.3976 - acc: 0.8643 - val_loss: 0.4548 - val_acc: 0.8428\n",
      "Epoch 46/1000\n",
      " - 2s - loss: 0.3968 - acc: 0.8645 - val_loss: 0.4542 - val_acc: 0.8445\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000045\n",
      "Epoch 47/1000\n",
      " - 1s - loss: 0.3958 - acc: 0.8649 - val_loss: 0.4541 - val_acc: 0.8458\n",
      "Epoch 48/1000\n",
      " - 1s - loss: 0.3951 - acc: 0.8652 - val_loss: 0.4520 - val_acc: 0.8458\n",
      "Epoch 49/1000\n",
      " - 1s - loss: 0.3941 - acc: 0.8662 - val_loss: 0.4508 - val_acc: 0.8464\n",
      "Epoch 50/1000\n",
      " - 1s - loss: 0.3935 - acc: 0.8653 - val_loss: 0.4526 - val_acc: 0.8430\n",
      "Epoch 51/1000\n",
      " - 2s - loss: 0.3926 - acc: 0.8659 - val_loss: 0.4500 - val_acc: 0.8454\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000050\n",
      "Epoch 52/1000\n",
      " - 1s - loss: 0.3917 - acc: 0.8661 - val_loss: 0.4496 - val_acc: 0.8444\n",
      "Epoch 53/1000\n",
      " - 1s - loss: 0.3912 - acc: 0.8667 - val_loss: 0.4489 - val_acc: 0.8455\n",
      "Epoch 54/1000\n",
      " - 2s - loss: 0.3901 - acc: 0.8667 - val_loss: 0.4495 - val_acc: 0.8444\n",
      "Epoch 55/1000\n",
      " - 1s - loss: 0.3893 - acc: 0.8672 - val_loss: 0.4481 - val_acc: 0.8469\n",
      "Epoch 56/1000\n",
      " - 2s - loss: 0.3886 - acc: 0.8668 - val_loss: 0.4480 - val_acc: 0.8446\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000055\n",
      "Epoch 57/1000\n",
      " - 1s - loss: 0.3879 - acc: 0.8676 - val_loss: 0.4468 - val_acc: 0.8480\n",
      "Epoch 58/1000\n",
      " - 1s - loss: 0.3869 - acc: 0.8679 - val_loss: 0.4459 - val_acc: 0.8477\n",
      "Epoch 59/1000\n",
      " - 1s - loss: 0.3862 - acc: 0.8682 - val_loss: 0.4445 - val_acc: 0.8479\n",
      "Epoch 60/1000\n",
      " - 1s - loss: 0.3856 - acc: 0.8682 - val_loss: 0.4444 - val_acc: 0.8468\n",
      "Epoch 61/1000\n",
      " - 2s - loss: 0.3847 - acc: 0.8681 - val_loss: 0.4437 - val_acc: 0.8473\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000060\n",
      "Epoch 62/1000\n",
      " - 2s - loss: 0.3838 - acc: 0.8690 - val_loss: 0.4436 - val_acc: 0.8475\n",
      "Epoch 63/1000\n",
      " - 2s - loss: 0.3832 - acc: 0.8693 - val_loss: 0.4426 - val_acc: 0.8483\n",
      "Epoch 64/1000\n",
      " - 1s - loss: 0.3823 - acc: 0.8695 - val_loss: 0.4430 - val_acc: 0.8482\n",
      "Epoch 65/1000\n",
      " - 1s - loss: 0.3817 - acc: 0.8698 - val_loss: 0.4423 - val_acc: 0.8481\n",
      "Epoch 66/1000\n",
      " - 4s - loss: 0.3811 - acc: 0.8689 - val_loss: 0.4410 - val_acc: 0.8488\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000065\n",
      "Epoch 67/1000\n",
      " - 2s - loss: 0.3803 - acc: 0.8698 - val_loss: 0.4408 - val_acc: 0.8493\n",
      "Epoch 68/1000\n",
      " - 2s - loss: 0.3795 - acc: 0.8699 - val_loss: 0.4414 - val_acc: 0.8467\n",
      "Epoch 69/1000\n",
      " - 1s - loss: 0.3788 - acc: 0.8707 - val_loss: 0.4394 - val_acc: 0.8495\n",
      "Epoch 70/1000\n",
      " - 1s - loss: 0.3780 - acc: 0.8702 - val_loss: 0.4388 - val_acc: 0.8481\n",
      "Epoch 71/1000\n",
      " - 2s - loss: 0.3776 - acc: 0.8700 - val_loss: 0.4395 - val_acc: 0.8482\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000070\n",
      "Epoch 72/1000\n",
      " - 1s - loss: 0.3768 - acc: 0.8709 - val_loss: 0.4399 - val_acc: 0.8487\n",
      "Epoch 73/1000\n",
      " - 1s - loss: 0.3760 - acc: 0.8710 - val_loss: 0.4369 - val_acc: 0.8503\n",
      "Epoch 74/1000\n",
      " - 1s - loss: 0.3756 - acc: 0.8714 - val_loss: 0.4383 - val_acc: 0.8487\n",
      "Epoch 75/1000\n",
      " - 1s - loss: 0.3746 - acc: 0.8721 - val_loss: 0.4359 - val_acc: 0.8505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1000\n",
      " - 2s - loss: 0.3740 - acc: 0.8717 - val_loss: 0.4369 - val_acc: 0.8500\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000075\n",
      "Epoch 77/1000\n",
      " - 1s - loss: 0.3733 - acc: 0.8722 - val_loss: 0.4356 - val_acc: 0.8492\n",
      "Epoch 78/1000\n",
      " - 1s - loss: 0.3726 - acc: 0.8724 - val_loss: 0.4370 - val_acc: 0.8490\n",
      "Epoch 79/1000\n",
      " - 1s - loss: 0.3720 - acc: 0.8721 - val_loss: 0.4369 - val_acc: 0.8493\n",
      "Epoch 80/1000\n",
      " - 1s - loss: 0.3713 - acc: 0.8724 - val_loss: 0.4346 - val_acc: 0.8491\n",
      "Epoch 81/1000\n",
      " - 3s - loss: 0.3710 - acc: 0.8722 - val_loss: 0.4346 - val_acc: 0.8509\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000080\n",
      "Epoch 82/1000\n",
      " - 2s - loss: 0.3700 - acc: 0.8729 - val_loss: 0.4342 - val_acc: 0.8503\n",
      "Epoch 83/1000\n",
      " - 1s - loss: 0.3695 - acc: 0.8733 - val_loss: 0.4331 - val_acc: 0.8510\n",
      "Epoch 84/1000\n",
      " - 1s - loss: 0.3687 - acc: 0.8735 - val_loss: 0.4320 - val_acc: 0.8505\n",
      "Epoch 85/1000\n",
      " - 2s - loss: 0.3684 - acc: 0.8735 - val_loss: 0.4313 - val_acc: 0.8521\n",
      "Epoch 86/1000\n",
      " - 3s - loss: 0.3674 - acc: 0.8737 - val_loss: 0.4307 - val_acc: 0.8501\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000085\n",
      "Epoch 87/1000\n",
      " - 2s - loss: 0.3669 - acc: 0.8741 - val_loss: 0.4301 - val_acc: 0.8512\n",
      "Epoch 88/1000\n",
      " - 1s - loss: 0.3662 - acc: 0.8739 - val_loss: 0.4318 - val_acc: 0.8508\n",
      "Epoch 89/1000\n",
      " - 1s - loss: 0.3658 - acc: 0.8746 - val_loss: 0.4293 - val_acc: 0.8516\n",
      "Epoch 90/1000\n",
      " - 2s - loss: 0.3650 - acc: 0.8742 - val_loss: 0.4296 - val_acc: 0.8507\n",
      "Epoch 91/1000\n",
      " - 3s - loss: 0.3645 - acc: 0.8745 - val_loss: 0.4310 - val_acc: 0.8517\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000090\n",
      "Epoch 92/1000\n",
      " - 1s - loss: 0.3640 - acc: 0.8752 - val_loss: 0.4314 - val_acc: 0.8512\n",
      "Epoch 93/1000\n",
      " - 1s - loss: 0.3632 - acc: 0.8753 - val_loss: 0.4290 - val_acc: 0.8506\n",
      "Epoch 94/1000\n",
      " - 2s - loss: 0.3627 - acc: 0.8752 - val_loss: 0.4278 - val_acc: 0.8512\n",
      "Epoch 95/1000\n",
      " - 1s - loss: 0.3621 - acc: 0.8754 - val_loss: 0.4271 - val_acc: 0.8511\n",
      "Epoch 96/1000\n",
      " - 2s - loss: 0.3615 - acc: 0.8751 - val_loss: 0.4287 - val_acc: 0.8512\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000095\n",
      "Epoch 97/1000\n",
      " - 1s - loss: 0.3613 - acc: 0.8760 - val_loss: 0.4271 - val_acc: 0.8509\n",
      "Epoch 98/1000\n",
      " - 1s - loss: 0.3605 - acc: 0.8760 - val_loss: 0.4262 - val_acc: 0.8521\n",
      "Epoch 99/1000\n",
      " - 1s - loss: 0.3598 - acc: 0.8762 - val_loss: 0.4265 - val_acc: 0.8520\n",
      "Epoch 100/1000\n",
      " - 2s - loss: 0.3593 - acc: 0.8764 - val_loss: 0.4275 - val_acc: 0.8514\n",
      "Epoch 101/1000\n",
      " - 3s - loss: 0.3587 - acc: 0.8768 - val_loss: 0.4242 - val_acc: 0.8525\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000100\n",
      "Epoch 102/1000\n",
      " - 2s - loss: 0.3580 - acc: 0.8766 - val_loss: 0.4261 - val_acc: 0.8539\n",
      "Epoch 103/1000\n",
      " - 1s - loss: 0.3575 - acc: 0.8769 - val_loss: 0.4251 - val_acc: 0.8524\n",
      "Epoch 104/1000\n",
      " - 1s - loss: 0.3571 - acc: 0.8768 - val_loss: 0.4233 - val_acc: 0.8527\n",
      "Epoch 105/1000\n",
      " - 1s - loss: 0.3562 - acc: 0.8776 - val_loss: 0.4285 - val_acc: 0.8529\n",
      "Epoch 106/1000\n",
      " - 1s - loss: 0.3559 - acc: 0.8778 - val_loss: 0.4234 - val_acc: 0.8526\n",
      "Epoch 107/1000\n",
      " - 1s - loss: 0.3554 - acc: 0.8778 - val_loss: 0.4229 - val_acc: 0.8537\n",
      "Epoch 108/1000\n",
      " - 1s - loss: 0.3551 - acc: 0.8786 - val_loss: 0.4221 - val_acc: 0.8523\n",
      "Epoch 109/1000\n",
      " - 1s - loss: 0.3545 - acc: 0.8780 - val_loss: 0.4217 - val_acc: 0.8531\n",
      "Epoch 110/1000\n",
      " - 2s - loss: 0.3538 - acc: 0.8781 - val_loss: 0.4212 - val_acc: 0.8532\n",
      "Epoch 111/1000\n",
      " - 3s - loss: 0.3535 - acc: 0.8784 - val_loss: 0.4211 - val_acc: 0.8537\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000110\n",
      "Epoch 112/1000\n",
      " - 1s - loss: 0.3528 - acc: 0.8789 - val_loss: 0.4214 - val_acc: 0.8533\n",
      "Epoch 113/1000\n",
      " - 1s - loss: 0.3522 - acc: 0.8792 - val_loss: 0.4212 - val_acc: 0.8537\n",
      "Epoch 114/1000\n",
      " - 1s - loss: 0.3520 - acc: 0.8792 - val_loss: 0.4202 - val_acc: 0.8543\n",
      "Epoch 115/1000\n",
      " - 1s - loss: 0.3512 - acc: 0.8791 - val_loss: 0.4189 - val_acc: 0.8536\n",
      "Epoch 116/1000\n",
      " - 1s - loss: 0.3508 - acc: 0.8789 - val_loss: 0.4192 - val_acc: 0.8542\n",
      "Epoch 117/1000\n",
      " - 1s - loss: 0.3502 - acc: 0.8798 - val_loss: 0.4194 - val_acc: 0.8546\n",
      "Epoch 118/1000\n",
      " - 1s - loss: 0.3497 - acc: 0.8790 - val_loss: 0.4185 - val_acc: 0.8537\n",
      "Epoch 119/1000\n",
      " - 1s - loss: 0.3492 - acc: 0.8798 - val_loss: 0.4183 - val_acc: 0.8559\n",
      "Epoch 120/1000\n",
      " - 1s - loss: 0.3489 - acc: 0.8796 - val_loss: 0.4177 - val_acc: 0.8559\n",
      "Epoch 121/1000\n",
      " - 3s - loss: 0.3484 - acc: 0.8800 - val_loss: 0.4173 - val_acc: 0.8555\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000120\n",
      "Epoch 122/1000\n",
      " - 2s - loss: 0.3475 - acc: 0.8800 - val_loss: 0.4179 - val_acc: 0.8543\n",
      "Epoch 123/1000\n",
      " - 1s - loss: 0.3472 - acc: 0.8805 - val_loss: 0.4172 - val_acc: 0.8548\n",
      "Epoch 124/1000\n",
      " - 1s - loss: 0.3468 - acc: 0.8807 - val_loss: 0.4168 - val_acc: 0.8547\n",
      "Epoch 125/1000\n",
      " - 1s - loss: 0.3463 - acc: 0.8808 - val_loss: 0.4169 - val_acc: 0.8556\n",
      "Epoch 126/1000\n",
      " - 1s - loss: 0.3459 - acc: 0.8808 - val_loss: 0.4157 - val_acc: 0.8545\n",
      "Epoch 127/1000\n",
      " - 1s - loss: 0.3454 - acc: 0.8811 - val_loss: 0.4167 - val_acc: 0.8560\n",
      "Epoch 128/1000\n",
      " - 2s - loss: 0.3448 - acc: 0.8811 - val_loss: 0.4152 - val_acc: 0.8551\n",
      "Epoch 129/1000\n",
      " - 1s - loss: 0.3445 - acc: 0.8812 - val_loss: 0.4147 - val_acc: 0.8557\n",
      "Epoch 130/1000\n",
      " - 1s - loss: 0.3439 - acc: 0.8811 - val_loss: 0.4160 - val_acc: 0.8568\n",
      "Epoch 131/1000\n",
      " - 3s - loss: 0.3436 - acc: 0.8815 - val_loss: 0.4139 - val_acc: 0.8555\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000130\n",
      "Epoch 132/1000\n",
      " - 2s - loss: 0.3431 - acc: 0.8816 - val_loss: 0.4156 - val_acc: 0.8566\n",
      "Epoch 133/1000\n",
      " - 1s - loss: 0.3427 - acc: 0.8821 - val_loss: 0.4139 - val_acc: 0.8559\n",
      "Epoch 134/1000\n",
      " - 1s - loss: 0.3420 - acc: 0.8822 - val_loss: 0.4136 - val_acc: 0.8572\n",
      "Epoch 135/1000\n",
      " - 1s - loss: 0.3418 - acc: 0.8820 - val_loss: 0.4173 - val_acc: 0.8537\n",
      "Epoch 136/1000\n",
      " - 1s - loss: 0.3411 - acc: 0.8826 - val_loss: 0.4133 - val_acc: 0.8548\n",
      "Epoch 137/1000\n",
      " - 1s - loss: 0.3407 - acc: 0.8821 - val_loss: 0.4122 - val_acc: 0.8573\n",
      "Epoch 138/1000\n",
      " - 1s - loss: 0.3403 - acc: 0.8827 - val_loss: 0.4144 - val_acc: 0.8544\n",
      "Epoch 139/1000\n",
      " - 1s - loss: 0.3401 - acc: 0.8821 - val_loss: 0.4119 - val_acc: 0.8570\n",
      "Epoch 140/1000\n",
      " - 1s - loss: 0.3394 - acc: 0.8833 - val_loss: 0.4126 - val_acc: 0.8561\n",
      "Epoch 141/1000\n",
      " - 3s - loss: 0.3390 - acc: 0.8827 - val_loss: 0.4127 - val_acc: 0.8558\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000140\n",
      "Epoch 142/1000\n",
      " - 2s - loss: 0.3386 - acc: 0.8832 - val_loss: 0.4122 - val_acc: 0.8550\n",
      "Epoch 143/1000\n",
      " - 2s - loss: 0.3382 - acc: 0.8828 - val_loss: 0.4125 - val_acc: 0.8577\n",
      "Epoch 144/1000\n",
      " - 1s - loss: 0.3378 - acc: 0.8831 - val_loss: 0.4115 - val_acc: 0.8572\n",
      "Epoch 145/1000\n",
      " - 1s - loss: 0.3373 - acc: 0.8842 - val_loss: 0.4116 - val_acc: 0.8570\n",
      "Epoch 146/1000\n",
      " - 1s - loss: 0.3372 - acc: 0.8841 - val_loss: 0.4108 - val_acc: 0.8586\n",
      "Epoch 147/1000\n",
      " - 1s - loss: 0.3367 - acc: 0.8836 - val_loss: 0.4093 - val_acc: 0.8583\n",
      "Epoch 148/1000\n",
      " - 1s - loss: 0.3361 - acc: 0.8837 - val_loss: 0.4097 - val_acc: 0.8578\n",
      "Epoch 149/1000\n",
      " - 1s - loss: 0.3357 - acc: 0.8838 - val_loss: 0.4101 - val_acc: 0.8568\n",
      "Epoch 150/1000\n",
      " - 2s - loss: 0.3352 - acc: 0.8840 - val_loss: 0.4089 - val_acc: 0.8578\n",
      "Epoch 151/1000\n",
      " - 3s - loss: 0.3350 - acc: 0.8840 - val_loss: 0.4088 - val_acc: 0.8579\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000150\n",
      "Epoch 152/1000\n",
      " - 1s - loss: 0.3344 - acc: 0.8842 - val_loss: 0.4094 - val_acc: 0.8570\n",
      "Epoch 153/1000\n",
      " - 1s - loss: 0.3343 - acc: 0.8844 - val_loss: 0.4092 - val_acc: 0.8572\n",
      "Epoch 154/1000\n",
      " - 1s - loss: 0.3335 - acc: 0.8843 - val_loss: 0.4085 - val_acc: 0.8588\n",
      "Epoch 155/1000\n",
      " - 1s - loss: 0.3335 - acc: 0.8844 - val_loss: 0.4081 - val_acc: 0.8572\n",
      "Epoch 156/1000\n",
      " - 1s - loss: 0.3327 - acc: 0.8854 - val_loss: 0.4079 - val_acc: 0.8578\n",
      "Epoch 157/1000\n",
      " - 1s - loss: 0.3325 - acc: 0.8848 - val_loss: 0.4089 - val_acc: 0.8584\n",
      "Epoch 158/1000\n",
      " - 1s - loss: 0.3321 - acc: 0.8851 - val_loss: 0.4100 - val_acc: 0.8594\n",
      "Epoch 159/1000\n",
      " - 1s - loss: 0.3316 - acc: 0.8857 - val_loss: 0.4072 - val_acc: 0.8595\n",
      "Epoch 160/1000\n",
      " - 1s - loss: 0.3312 - acc: 0.8852 - val_loss: 0.4065 - val_acc: 0.8574\n",
      "Epoch 161/1000\n",
      " - 3s - loss: 0.3313 - acc: 0.8854 - val_loss: 0.4058 - val_acc: 0.8578\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000160\n",
      "Epoch 162/1000\n",
      " - 1s - loss: 0.3305 - acc: 0.8854 - val_loss: 0.4047 - val_acc: 0.8596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/1000\n",
      " - 1s - loss: 0.3303 - acc: 0.8861 - val_loss: 0.4058 - val_acc: 0.8596\n",
      "Epoch 164/1000\n",
      " - 1s - loss: 0.3298 - acc: 0.8866 - val_loss: 0.4060 - val_acc: 0.8595\n",
      "Epoch 165/1000\n",
      " - 1s - loss: 0.3289 - acc: 0.8865 - val_loss: 0.4062 - val_acc: 0.8565\n",
      "Epoch 166/1000\n",
      " - 1s - loss: 0.3291 - acc: 0.8865 - val_loss: 0.4064 - val_acc: 0.8582\n",
      "Epoch 167/1000\n",
      " - 1s - loss: 0.3287 - acc: 0.8864 - val_loss: 0.4041 - val_acc: 0.8591\n",
      "Epoch 168/1000\n",
      " - 1s - loss: 0.3283 - acc: 0.8866 - val_loss: 0.4053 - val_acc: 0.8591\n",
      "Epoch 169/1000\n",
      " - 1s - loss: 0.3283 - acc: 0.8862 - val_loss: 0.4064 - val_acc: 0.8582\n",
      "Epoch 170/1000\n",
      " - 1s - loss: 0.3277 - acc: 0.8872 - val_loss: 0.4048 - val_acc: 0.8590\n",
      "Epoch 171/1000\n",
      " - 3s - loss: 0.3271 - acc: 0.8866 - val_loss: 0.4041 - val_acc: 0.8611\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000170\n",
      "Epoch 172/1000\n",
      " - 1s - loss: 0.3269 - acc: 0.8865 - val_loss: 0.4035 - val_acc: 0.8607\n",
      "Epoch 173/1000\n",
      " - 1s - loss: 0.3265 - acc: 0.8873 - val_loss: 0.4077 - val_acc: 0.8593\n",
      "Epoch 174/1000\n",
      " - 1s - loss: 0.3262 - acc: 0.8878 - val_loss: 0.4040 - val_acc: 0.8579\n",
      "Epoch 175/1000\n",
      " - 1s - loss: 0.3259 - acc: 0.8873 - val_loss: 0.4044 - val_acc: 0.8585\n",
      "Epoch 176/1000\n",
      " - 1s - loss: 0.3255 - acc: 0.8873 - val_loss: 0.4037 - val_acc: 0.8593\n",
      "Epoch 177/1000\n",
      " - 1s - loss: 0.3251 - acc: 0.8875 - val_loss: 0.4041 - val_acc: 0.8582\n",
      "Epoch 178/1000\n",
      " - 1s - loss: 0.3245 - acc: 0.8880 - val_loss: 0.4042 - val_acc: 0.8572\n",
      "Epoch 179/1000\n",
      " - 1s - loss: 0.3244 - acc: 0.8877 - val_loss: 0.4040 - val_acc: 0.8594\n",
      "Epoch 180/1000\n",
      " - 1s - loss: 0.3242 - acc: 0.8876 - val_loss: 0.4042 - val_acc: 0.8579\n",
      "Epoch 181/1000\n",
      " - 3s - loss: 0.3236 - acc: 0.8877 - val_loss: 0.4019 - val_acc: 0.8594\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000180\n",
      "Epoch 182/1000\n",
      " - 1s - loss: 0.3234 - acc: 0.8879 - val_loss: 0.4011 - val_acc: 0.8611\n",
      "Epoch 183/1000\n",
      " - 1s - loss: 0.3230 - acc: 0.8889 - val_loss: 0.4015 - val_acc: 0.8601\n",
      "Epoch 184/1000\n",
      " - 1s - loss: 0.3226 - acc: 0.8882 - val_loss: 0.4017 - val_acc: 0.8604\n",
      "Epoch 185/1000\n",
      " - 1s - loss: 0.3223 - acc: 0.8882 - val_loss: 0.4013 - val_acc: 0.8607\n",
      "Epoch 186/1000\n",
      " - 1s - loss: 0.3219 - acc: 0.8895 - val_loss: 0.4011 - val_acc: 0.8612\n",
      "Epoch 187/1000\n",
      " - 1s - loss: 0.3215 - acc: 0.8891 - val_loss: 0.4017 - val_acc: 0.8611\n",
      "Epoch 188/1000\n",
      " - 1s - loss: 0.3212 - acc: 0.8888 - val_loss: 0.4003 - val_acc: 0.8618\n",
      "Epoch 189/1000\n",
      " - 1s - loss: 0.3212 - acc: 0.8893 - val_loss: 0.4014 - val_acc: 0.8585\n",
      "Epoch 190/1000\n",
      " - 1s - loss: 0.3208 - acc: 0.8888 - val_loss: 0.4007 - val_acc: 0.8608\n",
      "Epoch 191/1000\n",
      " - 3s - loss: 0.3204 - acc: 0.8895 - val_loss: 0.4012 - val_acc: 0.8626\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000190\n",
      "Epoch 192/1000\n",
      " - 1s - loss: 0.3203 - acc: 0.8896 - val_loss: 0.4007 - val_acc: 0.8605\n",
      "Epoch 193/1000\n",
      " - 1s - loss: 0.3198 - acc: 0.8899 - val_loss: 0.4023 - val_acc: 0.8599\n",
      "Epoch 194/1000\n",
      " - 1s - loss: 0.3194 - acc: 0.8894 - val_loss: 0.4003 - val_acc: 0.8584\n",
      "Epoch 195/1000\n",
      " - 1s - loss: 0.3192 - acc: 0.8894 - val_loss: 0.4009 - val_acc: 0.8604\n",
      "Epoch 196/1000\n",
      " - 1s - loss: 0.3187 - acc: 0.8904 - val_loss: 0.3990 - val_acc: 0.8609\n",
      "Epoch 197/1000\n",
      " - 1s - loss: 0.3185 - acc: 0.8901 - val_loss: 0.3991 - val_acc: 0.8610\n",
      "Epoch 198/1000\n",
      " - 1s - loss: 0.3181 - acc: 0.8901 - val_loss: 0.4003 - val_acc: 0.8618\n",
      "Epoch 199/1000\n",
      " - 1s - loss: 0.3177 - acc: 0.8903 - val_loss: 0.4005 - val_acc: 0.8609\n",
      "Epoch 200/1000\n",
      " - 1s - loss: 0.3177 - acc: 0.8900 - val_loss: 0.3978 - val_acc: 0.8612\n",
      "Epoch 201/1000\n",
      " - 3s - loss: 0.3170 - acc: 0.8903 - val_loss: 0.3983 - val_acc: 0.8609\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000200\n",
      "Epoch 202/1000\n",
      " - 1s - loss: 0.3172 - acc: 0.8903 - val_loss: 0.3988 - val_acc: 0.8615\n",
      "Epoch 203/1000\n",
      " - 1s - loss: 0.3166 - acc: 0.8904 - val_loss: 0.3981 - val_acc: 0.8617\n",
      "Epoch 204/1000\n",
      " - 1s - loss: 0.3162 - acc: 0.8905 - val_loss: 0.3980 - val_acc: 0.8609\n",
      "Epoch 205/1000\n",
      " - 1s - loss: 0.3160 - acc: 0.8909 - val_loss: 0.3974 - val_acc: 0.8600\n",
      "Epoch 206/1000\n",
      " - 1s - loss: 0.3155 - acc: 0.8908 - val_loss: 0.3994 - val_acc: 0.8599\n",
      "Epoch 207/1000\n",
      " - 1s - loss: 0.3154 - acc: 0.8915 - val_loss: 0.3977 - val_acc: 0.8620\n",
      "Epoch 208/1000\n",
      " - 1s - loss: 0.3152 - acc: 0.8915 - val_loss: 0.3977 - val_acc: 0.8601\n",
      "Epoch 209/1000\n",
      " - 1s - loss: 0.3147 - acc: 0.8912 - val_loss: 0.3977 - val_acc: 0.8598\n",
      "Epoch 210/1000\n",
      " - 1s - loss: 0.3146 - acc: 0.8920 - val_loss: 0.3968 - val_acc: 0.8619\n",
      "Epoch 211/1000\n",
      " - 2s - loss: 0.3143 - acc: 0.8914 - val_loss: 0.3971 - val_acc: 0.8624\n",
      "Epoch 212/1000\n",
      " - 1s - loss: 0.3140 - acc: 0.8918 - val_loss: 0.3980 - val_acc: 0.8619\n",
      "Epoch 213/1000\n",
      " - 1s - loss: 0.3136 - acc: 0.8916 - val_loss: 0.3985 - val_acc: 0.8614\n",
      "Epoch 214/1000\n",
      " - 1s - loss: 0.3133 - acc: 0.8921 - val_loss: 0.3959 - val_acc: 0.8619\n",
      "Epoch 215/1000\n",
      " - 1s - loss: 0.3132 - acc: 0.8915 - val_loss: 0.3961 - val_acc: 0.8620\n",
      "Epoch 216/1000\n",
      " - 1s - loss: 0.3127 - acc: 0.8915 - val_loss: 0.3978 - val_acc: 0.8620\n",
      "Epoch 217/1000\n",
      " - 2s - loss: 0.3128 - acc: 0.8926 - val_loss: 0.3975 - val_acc: 0.8606\n",
      "Epoch 218/1000\n",
      " - 1s - loss: 0.3123 - acc: 0.8919 - val_loss: 0.3966 - val_acc: 0.8624\n",
      "Epoch 219/1000\n",
      " - 1s - loss: 0.3120 - acc: 0.8915 - val_loss: 0.3966 - val_acc: 0.8622\n",
      "Epoch 220/1000\n",
      " - 1s - loss: 0.3116 - acc: 0.8926 - val_loss: 0.3960 - val_acc: 0.8626\n",
      "Epoch 221/1000\n",
      " - 1s - loss: 0.3113 - acc: 0.8928 - val_loss: 0.3952 - val_acc: 0.8612\n",
      "Epoch 222/1000\n",
      " - 1s - loss: 0.3110 - acc: 0.8923 - val_loss: 0.3974 - val_acc: 0.8605\n",
      "Epoch 223/1000\n",
      " - 1s - loss: 0.3107 - acc: 0.8927 - val_loss: 0.3950 - val_acc: 0.8623\n",
      "Epoch 224/1000\n",
      " - 1s - loss: 0.3105 - acc: 0.8924 - val_loss: 0.3964 - val_acc: 0.8614\n",
      "Epoch 225/1000\n",
      " - 1s - loss: 0.3102 - acc: 0.8927 - val_loss: 0.3945 - val_acc: 0.8627\n",
      "Epoch 226/1000\n",
      " - 1s - loss: 0.3101 - acc: 0.8930 - val_loss: 0.3952 - val_acc: 0.8612\n",
      "Epoch 227/1000\n",
      " - 1s - loss: 0.3097 - acc: 0.8930 - val_loss: 0.3945 - val_acc: 0.8632\n",
      "Epoch 228/1000\n",
      " - 1s - loss: 0.3093 - acc: 0.8933 - val_loss: 0.3951 - val_acc: 0.8613\n",
      "Epoch 229/1000\n",
      " - 1s - loss: 0.3090 - acc: 0.8928 - val_loss: 0.3942 - val_acc: 0.8618\n",
      "Epoch 230/1000\n",
      " - 1s - loss: 0.3090 - acc: 0.8933 - val_loss: 0.3960 - val_acc: 0.8629\n",
      "Epoch 231/1000\n",
      " - 1s - loss: 0.3085 - acc: 0.8935 - val_loss: 0.3945 - val_acc: 0.8622\n",
      "Epoch 232/1000\n",
      " - 1s - loss: 0.3082 - acc: 0.8935 - val_loss: 0.3943 - val_acc: 0.8630\n",
      "Epoch 233/1000\n",
      " - 1s - loss: 0.3079 - acc: 0.8941 - val_loss: 0.3953 - val_acc: 0.8631\n",
      "Epoch 234/1000\n",
      " - 1s - loss: 0.3077 - acc: 0.8933 - val_loss: 0.3939 - val_acc: 0.8607\n",
      "Epoch 235/1000\n",
      " - 1s - loss: 0.3073 - acc: 0.8932 - val_loss: 0.3930 - val_acc: 0.8633\n",
      "Epoch 236/1000\n",
      " - 1s - loss: 0.3074 - acc: 0.8936 - val_loss: 0.3930 - val_acc: 0.8637\n",
      "Epoch 237/1000\n",
      " - 2s - loss: 0.3068 - acc: 0.8941 - val_loss: 0.3951 - val_acc: 0.8632\n",
      "Epoch 238/1000\n",
      " - 2s - loss: 0.3068 - acc: 0.8939 - val_loss: 0.3930 - val_acc: 0.8636\n",
      "Epoch 239/1000\n",
      " - 1s - loss: 0.3065 - acc: 0.8945 - val_loss: 0.3937 - val_acc: 0.8644\n",
      "Epoch 240/1000\n",
      " - 1s - loss: 0.3061 - acc: 0.8942 - val_loss: 0.3940 - val_acc: 0.8614\n",
      "Epoch 241/1000\n",
      " - 1s - loss: 0.3059 - acc: 0.8950 - val_loss: 0.3928 - val_acc: 0.8625\n",
      "Epoch 242/1000\n",
      " - 1s - loss: 0.3055 - acc: 0.8944 - val_loss: 0.3942 - val_acc: 0.8628\n",
      "Epoch 243/1000\n",
      " - 2s - loss: 0.3053 - acc: 0.8952 - val_loss: 0.3938 - val_acc: 0.8626\n",
      "Epoch 244/1000\n",
      " - 2s - loss: 0.3052 - acc: 0.8951 - val_loss: 0.3921 - val_acc: 0.8622\n",
      "Epoch 245/1000\n",
      " - 2s - loss: 0.3049 - acc: 0.8950 - val_loss: 0.3967 - val_acc: 0.8620\n",
      "Epoch 246/1000\n",
      " - 2s - loss: 0.3047 - acc: 0.8949 - val_loss: 0.3923 - val_acc: 0.8639\n",
      "Epoch 247/1000\n",
      " - 2s - loss: 0.3043 - acc: 0.8949 - val_loss: 0.3938 - val_acc: 0.8618\n",
      "Epoch 248/1000\n",
      " - 2s - loss: 0.3043 - acc: 0.8950 - val_loss: 0.3927 - val_acc: 0.8640\n",
      "Epoch 249/1000\n",
      " - 2s - loss: 0.3041 - acc: 0.8952 - val_loss: 0.3931 - val_acc: 0.8623\n",
      "Epoch 250/1000\n",
      " - 1s - loss: 0.3036 - acc: 0.8954 - val_loss: 0.3922 - val_acc: 0.8643\n",
      "Epoch 251/1000\n",
      " - 2s - loss: 0.3033 - acc: 0.8956 - val_loss: 0.3952 - val_acc: 0.8600\n",
      "Epoch 252/1000\n",
      " - 1s - loss: 0.3032 - acc: 0.8953 - val_loss: 0.3922 - val_acc: 0.8642\n",
      "Epoch 253/1000\n",
      " - 1s - loss: 0.3031 - acc: 0.8956 - val_loss: 0.3919 - val_acc: 0.8639\n",
      "Epoch 254/1000\n",
      " - 1s - loss: 0.3023 - acc: 0.8955 - val_loss: 0.3928 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/1000\n",
      " - 2s - loss: 0.3025 - acc: 0.8959 - val_loss: 0.3915 - val_acc: 0.8631\n",
      "Epoch 256/1000\n",
      " - 2s - loss: 0.3022 - acc: 0.8954 - val_loss: 0.3917 - val_acc: 0.8634\n",
      "Epoch 257/1000\n",
      " - 1s - loss: 0.3021 - acc: 0.8956 - val_loss: 0.3921 - val_acc: 0.8643\n",
      "Epoch 258/1000\n",
      " - 2s - loss: 0.3017 - acc: 0.8958 - val_loss: 0.3959 - val_acc: 0.8632\n",
      "Epoch 259/1000\n",
      " - 2s - loss: 0.3011 - acc: 0.8962 - val_loss: 0.3900 - val_acc: 0.8633\n",
      "Epoch 260/1000\n",
      " - 1s - loss: 0.3010 - acc: 0.8964 - val_loss: 0.3933 - val_acc: 0.8637\n",
      "Epoch 261/1000\n",
      " - 2s - loss: 0.3009 - acc: 0.8968 - val_loss: 0.3907 - val_acc: 0.8647\n",
      "Epoch 262/1000\n",
      " - 1s - loss: 0.3007 - acc: 0.8966 - val_loss: 0.3916 - val_acc: 0.8642\n",
      "Epoch 263/1000\n",
      " - 1s - loss: 0.3005 - acc: 0.8959 - val_loss: 0.3909 - val_acc: 0.8642\n",
      "Epoch 264/1000\n",
      " - 1s - loss: 0.3004 - acc: 0.8965 - val_loss: 0.3915 - val_acc: 0.8639\n",
      "Epoch 265/1000\n",
      " - 1s - loss: 0.3000 - acc: 0.8965 - val_loss: 0.3907 - val_acc: 0.8634\n",
      "Epoch 266/1000\n",
      " - 1s - loss: 0.2996 - acc: 0.8965 - val_loss: 0.3929 - val_acc: 0.8613\n",
      "Epoch 267/1000\n",
      " - 1s - loss: 0.2994 - acc: 0.8966 - val_loss: 0.3918 - val_acc: 0.8629\n",
      "Epoch 268/1000\n",
      " - 1s - loss: 0.2990 - acc: 0.8972 - val_loss: 0.3903 - val_acc: 0.8640\n",
      "Epoch 269/1000\n",
      " - 1s - loss: 0.2987 - acc: 0.8970 - val_loss: 0.3900 - val_acc: 0.8642\n",
      "Epoch 270/1000\n",
      " - 1s - loss: 0.2988 - acc: 0.8963 - val_loss: 0.3914 - val_acc: 0.8627\n",
      "Epoch 271/1000\n",
      " - 1s - loss: 0.2987 - acc: 0.8968 - val_loss: 0.3904 - val_acc: 0.8633\n",
      "Epoch 272/1000\n",
      " - 1s - loss: 0.2981 - acc: 0.8966 - val_loss: 0.3892 - val_acc: 0.8647\n",
      "Epoch 273/1000\n",
      " - 1s - loss: 0.2982 - acc: 0.8973 - val_loss: 0.3919 - val_acc: 0.8645\n",
      "Epoch 274/1000\n",
      " - 1s - loss: 0.2977 - acc: 0.8972 - val_loss: 0.3974 - val_acc: 0.8614\n",
      "Epoch 275/1000\n",
      " - 1s - loss: 0.2977 - acc: 0.8976 - val_loss: 0.3914 - val_acc: 0.8651\n",
      "Epoch 276/1000\n",
      " - 1s - loss: 0.2975 - acc: 0.8970 - val_loss: 0.3915 - val_acc: 0.8649\n",
      "Epoch 277/1000\n",
      " - 1s - loss: 0.2973 - acc: 0.8978 - val_loss: 0.3886 - val_acc: 0.8649\n",
      "Epoch 278/1000\n",
      " - 1s - loss: 0.2970 - acc: 0.8983 - val_loss: 0.3899 - val_acc: 0.8654\n",
      "Epoch 279/1000\n",
      " - 1s - loss: 0.2969 - acc: 0.8977 - val_loss: 0.3900 - val_acc: 0.8651\n",
      "Epoch 280/1000\n",
      " - 1s - loss: 0.2965 - acc: 0.8976 - val_loss: 0.3915 - val_acc: 0.8628\n",
      "Epoch 281/1000\n",
      " - 1s - loss: 0.2959 - acc: 0.8976 - val_loss: 0.3901 - val_acc: 0.8651\n",
      "Epoch 282/1000\n",
      " - 1s - loss: 0.2962 - acc: 0.8980 - val_loss: 0.3908 - val_acc: 0.8636\n",
      "Epoch 283/1000\n",
      " - 1s - loss: 0.2959 - acc: 0.8974 - val_loss: 0.3889 - val_acc: 0.8659\n",
      "Epoch 284/1000\n",
      " - 1s - loss: 0.2955 - acc: 0.8979 - val_loss: 0.3914 - val_acc: 0.8628\n",
      "Epoch 285/1000\n",
      " - 1s - loss: 0.2955 - acc: 0.8982 - val_loss: 0.3918 - val_acc: 0.8629\n",
      "Epoch 286/1000\n",
      " - 1s - loss: 0.2952 - acc: 0.8989 - val_loss: 0.3892 - val_acc: 0.8662\n",
      "Epoch 287/1000\n",
      " - 1s - loss: 0.2951 - acc: 0.8985 - val_loss: 0.3902 - val_acc: 0.8635\n",
      "Epoch 288/1000\n",
      " - 1s - loss: 0.2948 - acc: 0.8981 - val_loss: 0.3891 - val_acc: 0.8650\n",
      "Epoch 289/1000\n",
      " - 1s - loss: 0.2947 - acc: 0.8982 - val_loss: 0.3916 - val_acc: 0.8626\n",
      "Epoch 290/1000\n",
      " - 1s - loss: 0.2945 - acc: 0.8980 - val_loss: 0.3882 - val_acc: 0.8658\n",
      "Epoch 291/1000\n",
      " - 1s - loss: 0.2940 - acc: 0.8987 - val_loss: 0.3889 - val_acc: 0.8645\n",
      "Epoch 292/1000\n",
      " - 1s - loss: 0.2940 - acc: 0.8994 - val_loss: 0.3887 - val_acc: 0.8648\n",
      "Epoch 293/1000\n",
      " - 1s - loss: 0.2937 - acc: 0.8991 - val_loss: 0.3887 - val_acc: 0.8651\n",
      "Epoch 294/1000\n",
      " - 2s - loss: 0.2934 - acc: 0.8999 - val_loss: 0.3881 - val_acc: 0.8643\n",
      "Epoch 295/1000\n",
      " - 2s - loss: 0.2933 - acc: 0.8993 - val_loss: 0.3892 - val_acc: 0.8653\n",
      "Epoch 296/1000\n",
      " - 2s - loss: 0.2929 - acc: 0.8990 - val_loss: 0.3889 - val_acc: 0.8660\n",
      "Epoch 297/1000\n",
      " - 1s - loss: 0.2927 - acc: 0.8985 - val_loss: 0.3883 - val_acc: 0.8653\n",
      "Epoch 298/1000\n",
      " - 2s - loss: 0.2929 - acc: 0.8992 - val_loss: 0.3879 - val_acc: 0.8667\n",
      "Epoch 299/1000\n",
      " - 2s - loss: 0.2923 - acc: 0.8991 - val_loss: 0.3893 - val_acc: 0.8637\n",
      "Epoch 300/1000\n",
      " - 2s - loss: 0.2919 - acc: 0.8994 - val_loss: 0.3916 - val_acc: 0.8654\n",
      "Epoch 301/1000\n",
      " - 3s - loss: 0.2922 - acc: 0.8997 - val_loss: 0.3867 - val_acc: 0.8657\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000300\n",
      "Epoch 302/1000\n",
      " - 1s - loss: 0.2919 - acc: 0.8997 - val_loss: 0.3889 - val_acc: 0.8650\n",
      "Epoch 303/1000\n",
      " - 1s - loss: 0.2916 - acc: 0.8994 - val_loss: 0.3869 - val_acc: 0.8661\n",
      "Epoch 304/1000\n",
      " - 1s - loss: 0.2914 - acc: 0.8996 - val_loss: 0.3883 - val_acc: 0.8648\n",
      "Epoch 305/1000\n",
      " - 1s - loss: 0.2913 - acc: 0.8997 - val_loss: 0.3876 - val_acc: 0.8658\n",
      "Epoch 306/1000\n",
      " - 1s - loss: 0.2911 - acc: 0.9003 - val_loss: 0.3865 - val_acc: 0.8669\n",
      "Epoch 307/1000\n",
      " - 1s - loss: 0.2907 - acc: 0.9000 - val_loss: 0.3888 - val_acc: 0.8650\n",
      "Epoch 308/1000\n",
      " - 1s - loss: 0.2906 - acc: 0.9002 - val_loss: 0.3889 - val_acc: 0.8652\n",
      "Epoch 309/1000\n",
      " - 1s - loss: 0.2904 - acc: 0.8997 - val_loss: 0.3869 - val_acc: 0.8658\n",
      "Epoch 310/1000\n",
      " - 1s - loss: 0.2902 - acc: 0.9004 - val_loss: 0.3870 - val_acc: 0.8660\n",
      "Epoch 311/1000\n",
      " - 1s - loss: 0.2899 - acc: 0.9000 - val_loss: 0.3879 - val_acc: 0.8654\n",
      "Epoch 312/1000\n",
      " - 1s - loss: 0.2897 - acc: 0.9007 - val_loss: 0.3869 - val_acc: 0.8665\n",
      "Epoch 313/1000\n",
      " - 1s - loss: 0.2896 - acc: 0.9002 - val_loss: 0.3865 - val_acc: 0.8668\n",
      "Epoch 314/1000\n",
      " - 1s - loss: 0.2893 - acc: 0.9007 - val_loss: 0.3879 - val_acc: 0.8646\n",
      "Epoch 315/1000\n",
      " - 1s - loss: 0.2892 - acc: 0.9001 - val_loss: 0.3884 - val_acc: 0.8663\n",
      "Epoch 316/1000\n",
      " - 1s - loss: 0.2891 - acc: 0.9003 - val_loss: 0.3877 - val_acc: 0.8655\n",
      "Epoch 317/1000\n",
      " - 1s - loss: 0.2890 - acc: 0.9006 - val_loss: 0.3905 - val_acc: 0.8635\n",
      "Epoch 318/1000\n",
      " - 1s - loss: 0.2886 - acc: 0.9006 - val_loss: 0.3924 - val_acc: 0.8646\n",
      "Epoch 319/1000\n",
      " - 1s - loss: 0.2883 - acc: 0.9010 - val_loss: 0.3951 - val_acc: 0.8633\n",
      "Epoch 320/1000\n",
      " - 1s - loss: 0.2883 - acc: 0.9006 - val_loss: 0.3868 - val_acc: 0.8663\n",
      "Epoch 321/1000\n",
      " - 1s - loss: 0.2880 - acc: 0.9006 - val_loss: 0.3896 - val_acc: 0.8648\n",
      "Epoch 322/1000\n",
      " - 1s - loss: 0.2880 - acc: 0.9009 - val_loss: 0.3874 - val_acc: 0.8668\n",
      "Epoch 323/1000\n",
      " - 1s - loss: 0.2878 - acc: 0.9008 - val_loss: 0.3890 - val_acc: 0.8660\n",
      "Epoch 324/1000\n",
      " - 1s - loss: 0.2873 - acc: 0.9012 - val_loss: 0.3887 - val_acc: 0.8639\n",
      "Epoch 325/1000\n",
      " - 1s - loss: 0.2872 - acc: 0.9011 - val_loss: 0.3876 - val_acc: 0.8649\n",
      "Epoch 326/1000\n",
      " - 1s - loss: 0.2870 - acc: 0.9018 - val_loss: 0.3874 - val_acc: 0.8661\n",
      "Epoch 327/1000\n",
      " - 1s - loss: 0.2869 - acc: 0.9014 - val_loss: 0.3867 - val_acc: 0.8656\n",
      "Epoch 328/1000\n",
      " - 1s - loss: 0.2865 - acc: 0.9017 - val_loss: 0.3855 - val_acc: 0.8671\n",
      "Epoch 329/1000\n",
      " - 1s - loss: 0.2862 - acc: 0.9015 - val_loss: 0.3868 - val_acc: 0.8639\n",
      "Epoch 330/1000\n",
      " - 1s - loss: 0.2860 - acc: 0.9017 - val_loss: 0.3878 - val_acc: 0.8663\n",
      "Epoch 331/1000\n",
      " - 1s - loss: 0.2860 - acc: 0.9017 - val_loss: 0.3883 - val_acc: 0.8660\n",
      "Epoch 332/1000\n",
      " - 1s - loss: 0.2859 - acc: 0.9018 - val_loss: 0.3868 - val_acc: 0.8658\n",
      "Epoch 333/1000\n",
      " - 1s - loss: 0.2856 - acc: 0.9020 - val_loss: 0.3865 - val_acc: 0.8677\n",
      "Epoch 334/1000\n",
      " - 1s - loss: 0.2852 - acc: 0.9012 - val_loss: 0.3899 - val_acc: 0.8661\n",
      "Epoch 335/1000\n",
      " - 1s - loss: 0.2852 - acc: 0.9018 - val_loss: 0.3874 - val_acc: 0.8657\n",
      "Epoch 336/1000\n",
      " - 1s - loss: 0.2851 - acc: 0.9014 - val_loss: 0.3862 - val_acc: 0.8651\n",
      "Epoch 337/1000\n",
      " - 1s - loss: 0.2850 - acc: 0.9015 - val_loss: 0.3853 - val_acc: 0.8675\n",
      "Epoch 338/1000\n",
      " - 1s - loss: 0.2846 - acc: 0.9020 - val_loss: 0.3875 - val_acc: 0.8632\n",
      "Epoch 339/1000\n",
      " - 1s - loss: 0.2845 - acc: 0.9020 - val_loss: 0.3855 - val_acc: 0.8667\n",
      "Epoch 340/1000\n",
      " - 1s - loss: 0.2844 - acc: 0.9019 - val_loss: 0.3902 - val_acc: 0.8652\n",
      "Epoch 341/1000\n",
      " - 1s - loss: 0.2841 - acc: 0.9019 - val_loss: 0.3852 - val_acc: 0.8664\n",
      "Epoch 342/1000\n",
      " - 1s - loss: 0.2838 - acc: 0.9024 - val_loss: 0.3862 - val_acc: 0.8662\n",
      "Epoch 343/1000\n",
      " - 1s - loss: 0.2839 - acc: 0.9021 - val_loss: 0.3867 - val_acc: 0.8677\n",
      "Epoch 344/1000\n",
      " - 1s - loss: 0.2835 - acc: 0.9021 - val_loss: 0.3871 - val_acc: 0.8677\n",
      "Epoch 345/1000\n",
      " - 1s - loss: 0.2834 - acc: 0.9024 - val_loss: 0.3874 - val_acc: 0.8655\n",
      "Epoch 346/1000\n",
      " - 1s - loss: 0.2832 - acc: 0.9025 - val_loss: 0.3864 - val_acc: 0.8672\n",
      "Epoch 347/1000\n",
      " - 1s - loss: 0.2833 - acc: 0.9023 - val_loss: 0.3859 - val_acc: 0.8665\n",
      "Epoch 348/1000\n",
      " - 1s - loss: 0.2827 - acc: 0.9026 - val_loss: 0.3877 - val_acc: 0.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/1000\n",
      " - 1s - loss: 0.2828 - acc: 0.9030 - val_loss: 0.3876 - val_acc: 0.8673\n",
      "Epoch 350/1000\n",
      " - 1s - loss: 0.2825 - acc: 0.9027 - val_loss: 0.3875 - val_acc: 0.8677\n",
      "Epoch 351/1000\n",
      " - 1s - loss: 0.2823 - acc: 0.9024 - val_loss: 0.3856 - val_acc: 0.8685\n",
      "Epoch 352/1000\n",
      " - 1s - loss: 0.2821 - acc: 0.9037 - val_loss: 0.3857 - val_acc: 0.8671\n",
      "Epoch 353/1000\n",
      " - 1s - loss: 0.2821 - acc: 0.9029 - val_loss: 0.3933 - val_acc: 0.8652\n",
      "Epoch 354/1000\n",
      " - 1s - loss: 0.2818 - acc: 0.9032 - val_loss: 0.3863 - val_acc: 0.8675\n",
      "Epoch 355/1000\n",
      " - 1s - loss: 0.2816 - acc: 0.9033 - val_loss: 0.3882 - val_acc: 0.8660\n",
      "Epoch 356/1000\n",
      " - 1s - loss: 0.2817 - acc: 0.9031 - val_loss: 0.3866 - val_acc: 0.8655\n",
      "Epoch 357/1000\n",
      " - 1s - loss: 0.2809 - acc: 0.9034 - val_loss: 0.3918 - val_acc: 0.8658\n",
      "Epoch 358/1000\n",
      " - 1s - loss: 0.2811 - acc: 0.9031 - val_loss: 0.3847 - val_acc: 0.8679\n",
      "Epoch 359/1000\n",
      " - 1s - loss: 0.2807 - acc: 0.9033 - val_loss: 0.3855 - val_acc: 0.8689\n",
      "Epoch 360/1000\n",
      " - 1s - loss: 0.2804 - acc: 0.9037 - val_loss: 0.3846 - val_acc: 0.8659\n",
      "Epoch 361/1000\n",
      " - 1s - loss: 0.2807 - acc: 0.9031 - val_loss: 0.3847 - val_acc: 0.8670\n",
      "Epoch 362/1000\n",
      " - 1s - loss: 0.2803 - acc: 0.9040 - val_loss: 0.3863 - val_acc: 0.8670\n",
      "Epoch 363/1000\n",
      " - 1s - loss: 0.2800 - acc: 0.9039 - val_loss: 0.3858 - val_acc: 0.8680\n",
      "Epoch 364/1000\n",
      " - 1s - loss: 0.2799 - acc: 0.9035 - val_loss: 0.3900 - val_acc: 0.8657\n",
      "Epoch 365/1000\n",
      " - 1s - loss: 0.2797 - acc: 0.9042 - val_loss: 0.3854 - val_acc: 0.8678\n",
      "Epoch 366/1000\n",
      " - 1s - loss: 0.2799 - acc: 0.9042 - val_loss: 0.3843 - val_acc: 0.8654\n",
      "Epoch 367/1000\n",
      " - 1s - loss: 0.2798 - acc: 0.9041 - val_loss: 0.3858 - val_acc: 0.8676\n",
      "Epoch 368/1000\n",
      " - 1s - loss: 0.2793 - acc: 0.9042 - val_loss: 0.3855 - val_acc: 0.8677\n",
      "Epoch 369/1000\n",
      " - 1s - loss: 0.2791 - acc: 0.9039 - val_loss: 0.3863 - val_acc: 0.8665\n",
      "Epoch 370/1000\n",
      " - 1s - loss: 0.2788 - acc: 0.9044 - val_loss: 0.3845 - val_acc: 0.8673\n",
      "Epoch 371/1000\n",
      " - 1s - loss: 0.2787 - acc: 0.9039 - val_loss: 0.3862 - val_acc: 0.8669\n",
      "Epoch 372/1000\n",
      " - 1s - loss: 0.2787 - acc: 0.9046 - val_loss: 0.3840 - val_acc: 0.8675\n",
      "Epoch 373/1000\n",
      " - 1s - loss: 0.2785 - acc: 0.9043 - val_loss: 0.3854 - val_acc: 0.8686\n",
      "Epoch 374/1000\n",
      " - 1s - loss: 0.2782 - acc: 0.9045 - val_loss: 0.3863 - val_acc: 0.8649\n",
      "Epoch 375/1000\n",
      " - 1s - loss: 0.2782 - acc: 0.9044 - val_loss: 0.3877 - val_acc: 0.8642\n",
      "Epoch 376/1000\n",
      " - 1s - loss: 0.2779 - acc: 0.9044 - val_loss: 0.3848 - val_acc: 0.8663\n",
      "Epoch 377/1000\n",
      " - 1s - loss: 0.2777 - acc: 0.9038 - val_loss: 0.3859 - val_acc: 0.8659\n",
      "Epoch 378/1000\n",
      " - 1s - loss: 0.2774 - acc: 0.9046 - val_loss: 0.3861 - val_acc: 0.8654\n",
      "Epoch 379/1000\n",
      " - 1s - loss: 0.2775 - acc: 0.9035 - val_loss: 0.3852 - val_acc: 0.8662\n",
      "Epoch 380/1000\n",
      " - 1s - loss: 0.2775 - acc: 0.9048 - val_loss: 0.3848 - val_acc: 0.8686\n",
      "Epoch 381/1000\n",
      " - 1s - loss: 0.2774 - acc: 0.9044 - val_loss: 0.3842 - val_acc: 0.8680\n",
      "Epoch 382/1000\n",
      " - 1s - loss: 0.2768 - acc: 0.9043 - val_loss: 0.3842 - val_acc: 0.8684\n",
      "Epoch 383/1000\n",
      " - 1s - loss: 0.2768 - acc: 0.9046 - val_loss: 0.3882 - val_acc: 0.8633\n",
      "Epoch 384/1000\n",
      " - 1s - loss: 0.2766 - acc: 0.9049 - val_loss: 0.3862 - val_acc: 0.8685\n",
      "Epoch 385/1000\n",
      " - 1s - loss: 0.2765 - acc: 0.9053 - val_loss: 0.3837 - val_acc: 0.8681\n",
      "Epoch 386/1000\n",
      " - 1s - loss: 0.2763 - acc: 0.9051 - val_loss: 0.3850 - val_acc: 0.8688\n",
      "Epoch 387/1000\n",
      " - 1s - loss: 0.2764 - acc: 0.9053 - val_loss: 0.3867 - val_acc: 0.8675\n",
      "Epoch 388/1000\n",
      " - 1s - loss: 0.2759 - acc: 0.9057 - val_loss: 0.3855 - val_acc: 0.8646\n",
      "Epoch 389/1000\n",
      " - 1s - loss: 0.2760 - acc: 0.9048 - val_loss: 0.3866 - val_acc: 0.8677\n",
      "Epoch 390/1000\n",
      " - 2s - loss: 0.2757 - acc: 0.9050 - val_loss: 0.3865 - val_acc: 0.8679\n",
      "Epoch 391/1000\n",
      " - 1s - loss: 0.2755 - acc: 0.9052 - val_loss: 0.3890 - val_acc: 0.8663\n",
      "Epoch 392/1000\n",
      " - 1s - loss: 0.2751 - acc: 0.9057 - val_loss: 0.3839 - val_acc: 0.8676\n",
      "Epoch 393/1000\n",
      " - 1s - loss: 0.2753 - acc: 0.9053 - val_loss: 0.3843 - val_acc: 0.8680\n",
      "Epoch 394/1000\n",
      " - 1s - loss: 0.2748 - acc: 0.9061 - val_loss: 0.3848 - val_acc: 0.8682\n",
      "Epoch 395/1000\n",
      " - 1s - loss: 0.2748 - acc: 0.9056 - val_loss: 0.3838 - val_acc: 0.8680\n",
      "Epoch 396/1000\n",
      " - 1s - loss: 0.2747 - acc: 0.9053 - val_loss: 0.3851 - val_acc: 0.8674\n",
      "Epoch 397/1000\n",
      " - 1s - loss: 0.2743 - acc: 0.9054 - val_loss: 0.3858 - val_acc: 0.8688\n",
      "Epoch 398/1000\n",
      " - 1s - loss: 0.2742 - acc: 0.9060 - val_loss: 0.3838 - val_acc: 0.8676\n",
      "Epoch 399/1000\n",
      " - 1s - loss: 0.2741 - acc: 0.9058 - val_loss: 0.3833 - val_acc: 0.8688\n",
      "Epoch 400/1000\n",
      " - 1s - loss: 0.2741 - acc: 0.9057 - val_loss: 0.3866 - val_acc: 0.8682\n",
      "Epoch 401/1000\n",
      " - 3s - loss: 0.2738 - acc: 0.9059 - val_loss: 0.3856 - val_acc: 0.8682\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000400\n",
      "Epoch 402/1000\n",
      " - 1s - loss: 0.2735 - acc: 0.9059 - val_loss: 0.3849 - val_acc: 0.8684\n",
      "Epoch 403/1000\n",
      " - 1s - loss: 0.2737 - acc: 0.9055 - val_loss: 0.3839 - val_acc: 0.8671\n",
      "Epoch 404/1000\n",
      " - 1s - loss: 0.2735 - acc: 0.9060 - val_loss: 0.3837 - val_acc: 0.8677\n",
      "Epoch 405/1000\n",
      " - 1s - loss: 0.2728 - acc: 0.9060 - val_loss: 0.3874 - val_acc: 0.8685\n",
      "Epoch 406/1000\n",
      " - 1s - loss: 0.2734 - acc: 0.9063 - val_loss: 0.3840 - val_acc: 0.8690\n",
      "Epoch 407/1000\n",
      " - 1s - loss: 0.2729 - acc: 0.9058 - val_loss: 0.3852 - val_acc: 0.8676\n",
      "Epoch 408/1000\n",
      " - 1s - loss: 0.2724 - acc: 0.9067 - val_loss: 0.3887 - val_acc: 0.8658\n",
      "Epoch 409/1000\n",
      " - 1s - loss: 0.2729 - acc: 0.9060 - val_loss: 0.3875 - val_acc: 0.8674\n",
      "Epoch 410/1000\n",
      " - 1s - loss: 0.2727 - acc: 0.9062 - val_loss: 0.3845 - val_acc: 0.8686\n",
      "Epoch 411/1000\n",
      " - 1s - loss: 0.2724 - acc: 0.9064 - val_loss: 0.3837 - val_acc: 0.8668\n",
      "Epoch 412/1000\n",
      " - 1s - loss: 0.2722 - acc: 0.9066 - val_loss: 0.3855 - val_acc: 0.8666\n",
      "Epoch 413/1000\n",
      " - 1s - loss: 0.2718 - acc: 0.9069 - val_loss: 0.3867 - val_acc: 0.8671\n",
      "Epoch 414/1000\n",
      " - 1s - loss: 0.2719 - acc: 0.9068 - val_loss: 0.3860 - val_acc: 0.8667\n",
      "Epoch 415/1000\n",
      " - 1s - loss: 0.2717 - acc: 0.9067 - val_loss: 0.3862 - val_acc: 0.8673\n",
      "Epoch 416/1000\n",
      " - 1s - loss: 0.2714 - acc: 0.9068 - val_loss: 0.3866 - val_acc: 0.8654\n",
      "Epoch 417/1000\n",
      " - 1s - loss: 0.2713 - acc: 0.9065 - val_loss: 0.3860 - val_acc: 0.8648\n",
      "Epoch 418/1000\n",
      " - 1s - loss: 0.2710 - acc: 0.9068 - val_loss: 0.3870 - val_acc: 0.8673\n",
      "Epoch 419/1000\n",
      " - 1s - loss: 0.2709 - acc: 0.9067 - val_loss: 0.3868 - val_acc: 0.8686\n",
      "Epoch 420/1000\n",
      " - 1s - loss: 0.2711 - acc: 0.9067 - val_loss: 0.3868 - val_acc: 0.8677\n",
      "Epoch 421/1000\n",
      " - 1s - loss: 0.2709 - acc: 0.9067 - val_loss: 0.3842 - val_acc: 0.8676\n",
      "Epoch 422/1000\n",
      " - 2s - loss: 0.2707 - acc: 0.9066 - val_loss: 0.3843 - val_acc: 0.8677\n",
      "Epoch 423/1000\n",
      " - 1s - loss: 0.2704 - acc: 0.9075 - val_loss: 0.3866 - val_acc: 0.8651\n",
      "Epoch 424/1000\n",
      " - 1s - loss: 0.2704 - acc: 0.9071 - val_loss: 0.3891 - val_acc: 0.8670\n",
      "Epoch 425/1000\n",
      " - 1s - loss: 0.2705 - acc: 0.9071 - val_loss: 0.3847 - val_acc: 0.8684\n",
      "Epoch 426/1000\n",
      " - 1s - loss: 0.2702 - acc: 0.9073 - val_loss: 0.3846 - val_acc: 0.8665\n",
      "Epoch 427/1000\n",
      " - 1s - loss: 0.2699 - acc: 0.9068 - val_loss: 0.3840 - val_acc: 0.8698\n",
      "Epoch 428/1000\n",
      " - 1s - loss: 0.2698 - acc: 0.9078 - val_loss: 0.3870 - val_acc: 0.8683\n",
      "Epoch 429/1000\n",
      " - 1s - loss: 0.2695 - acc: 0.9077 - val_loss: 0.3839 - val_acc: 0.8685\n",
      "Epoch 430/1000\n",
      " - 2s - loss: 0.2696 - acc: 0.9074 - val_loss: 0.3841 - val_acc: 0.8686\n",
      "Epoch 431/1000\n",
      " - 2s - loss: 0.2696 - acc: 0.9071 - val_loss: 0.3851 - val_acc: 0.8685\n",
      "Epoch 432/1000\n",
      " - 1s - loss: 0.2691 - acc: 0.9075 - val_loss: 0.3857 - val_acc: 0.8675\n",
      "Epoch 433/1000\n",
      " - 1s - loss: 0.2691 - acc: 0.9074 - val_loss: 0.3835 - val_acc: 0.8686\n",
      "Epoch 434/1000\n",
      " - 1s - loss: 0.2689 - acc: 0.9075 - val_loss: 0.3859 - val_acc: 0.8676\n",
      "Epoch 435/1000\n",
      " - 1s - loss: 0.2686 - acc: 0.9079 - val_loss: 0.3840 - val_acc: 0.8662\n",
      "Epoch 436/1000\n",
      " - 1s - loss: 0.2684 - acc: 0.9081 - val_loss: 0.3845 - val_acc: 0.8662\n",
      "Epoch 437/1000\n",
      " - 1s - loss: 0.2687 - acc: 0.9079 - val_loss: 0.3857 - val_acc: 0.8671\n",
      "Epoch 438/1000\n",
      " - 1s - loss: 0.2682 - acc: 0.9080 - val_loss: 0.3848 - val_acc: 0.8664\n",
      "Epoch 439/1000\n",
      " - 1s - loss: 0.2681 - acc: 0.9085 - val_loss: 0.3851 - val_acc: 0.8662\n",
      "Epoch 440/1000\n",
      " - 1s - loss: 0.2678 - acc: 0.9081 - val_loss: 0.3857 - val_acc: 0.8678\n",
      "Epoch 441/1000\n",
      " - 1s - loss: 0.2678 - acc: 0.9084 - val_loss: 0.3844 - val_acc: 0.8676\n",
      "Epoch 442/1000\n",
      " - 1s - loss: 0.2679 - acc: 0.9079 - val_loss: 0.3854 - val_acc: 0.8671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/1000\n",
      " - 1s - loss: 0.2673 - acc: 0.9081 - val_loss: 0.3840 - val_acc: 0.8687\n",
      "Epoch 444/1000\n",
      " - 1s - loss: 0.2673 - acc: 0.9090 - val_loss: 0.3846 - val_acc: 0.8675\n",
      "Epoch 445/1000\n",
      " - 1s - loss: 0.2674 - acc: 0.9082 - val_loss: 0.3839 - val_acc: 0.8674\n",
      "Epoch 446/1000\n",
      " - 1s - loss: 0.2671 - acc: 0.9088 - val_loss: 0.3955 - val_acc: 0.8650\n",
      "Epoch 447/1000\n",
      " - 1s - loss: 0.2673 - acc: 0.9083 - val_loss: 0.3847 - val_acc: 0.8694\n",
      "Epoch 448/1000\n",
      " - 1s - loss: 0.2666 - acc: 0.9086 - val_loss: 0.3874 - val_acc: 0.8677\n",
      "Epoch 449/1000\n",
      " - 1s - loss: 0.2667 - acc: 0.9078 - val_loss: 0.3845 - val_acc: 0.8688\n",
      "Epoch 450/1000\n",
      " - 1s - loss: 0.2663 - acc: 0.9088 - val_loss: 0.3848 - val_acc: 0.8676\n",
      "Epoch 451/1000\n",
      " - 1s - loss: 0.2667 - acc: 0.9080 - val_loss: 0.3855 - val_acc: 0.8670\n",
      "Epoch 452/1000\n",
      " - 1s - loss: 0.2663 - acc: 0.9084 - val_loss: 0.3868 - val_acc: 0.8673\n",
      "Epoch 453/1000\n",
      " - 1s - loss: 0.2663 - acc: 0.9086 - val_loss: 0.3853 - val_acc: 0.8688\n",
      "Epoch 454/1000\n",
      " - 1s - loss: 0.2662 - acc: 0.9086 - val_loss: 0.3848 - val_acc: 0.8670\n",
      "Epoch 455/1000\n",
      " - 1s - loss: 0.2656 - acc: 0.9094 - val_loss: 0.3857 - val_acc: 0.8676\n",
      "Epoch 456/1000\n",
      " - 1s - loss: 0.2656 - acc: 0.9088 - val_loss: 0.3855 - val_acc: 0.8691\n",
      "Epoch 457/1000\n",
      " - 1s - loss: 0.2654 - acc: 0.9091 - val_loss: 0.3850 - val_acc: 0.8675\n",
      "Epoch 458/1000\n",
      " - 1s - loss: 0.2654 - acc: 0.9084 - val_loss: 0.3880 - val_acc: 0.8668\n",
      "Epoch 459/1000\n",
      " - 1s - loss: 0.2654 - acc: 0.9093 - val_loss: 0.3843 - val_acc: 0.8680\n",
      "Epoch 460/1000\n",
      " - 1s - loss: 0.2650 - acc: 0.9093 - val_loss: 0.3834 - val_acc: 0.8681\n",
      "Epoch 461/1000\n",
      " - 1s - loss: 0.2649 - acc: 0.9091 - val_loss: 0.3854 - val_acc: 0.8684\n",
      "Epoch 462/1000\n",
      " - 1s - loss: 0.2648 - acc: 0.9088 - val_loss: 0.3869 - val_acc: 0.8661\n",
      "Epoch 463/1000\n",
      " - 1s - loss: 0.2647 - acc: 0.9096 - val_loss: 0.3853 - val_acc: 0.8666\n",
      "Epoch 464/1000\n",
      " - 1s - loss: 0.2645 - acc: 0.9092 - val_loss: 0.3870 - val_acc: 0.8674\n",
      "Epoch 465/1000\n",
      " - 1s - loss: 0.2643 - acc: 0.9099 - val_loss: 0.3905 - val_acc: 0.8657\n",
      "Epoch 466/1000\n",
      " - 1s - loss: 0.2640 - acc: 0.9094 - val_loss: 0.3846 - val_acc: 0.8687\n",
      "Epoch 467/1000\n",
      " - 1s - loss: 0.2643 - acc: 0.9095 - val_loss: 0.3872 - val_acc: 0.8689\n",
      "Epoch 468/1000\n",
      " - 1s - loss: 0.2640 - acc: 0.9090 - val_loss: 0.3864 - val_acc: 0.8682\n",
      "Epoch 469/1000\n",
      " - 1s - loss: 0.2641 - acc: 0.9094 - val_loss: 0.3864 - val_acc: 0.8667\n",
      "Epoch 470/1000\n",
      " - 1s - loss: 0.2637 - acc: 0.9099 - val_loss: 0.3860 - val_acc: 0.8675\n",
      "Epoch 471/1000\n",
      " - 1s - loss: 0.2637 - acc: 0.9092 - val_loss: 0.3848 - val_acc: 0.8673\n",
      "Epoch 472/1000\n",
      " - 1s - loss: 0.2636 - acc: 0.9096 - val_loss: 0.3875 - val_acc: 0.8663\n",
      "Epoch 473/1000\n",
      " - 1s - loss: 0.2631 - acc: 0.9101 - val_loss: 0.3870 - val_acc: 0.8665\n",
      "Epoch 474/1000\n",
      " - 1s - loss: 0.2632 - acc: 0.9101 - val_loss: 0.3871 - val_acc: 0.8668\n",
      "Epoch 475/1000\n",
      " - 1s - loss: 0.2632 - acc: 0.9099 - val_loss: 0.3861 - val_acc: 0.8686\n",
      "Epoch 476/1000\n",
      " - 2s - loss: 0.2630 - acc: 0.9097 - val_loss: 0.3871 - val_acc: 0.8675\n",
      "Epoch 477/1000\n",
      " - 1s - loss: 0.2628 - acc: 0.9099 - val_loss: 0.3853 - val_acc: 0.8689\n",
      "Epoch 478/1000\n",
      " - 1s - loss: 0.2628 - acc: 0.9103 - val_loss: 0.3871 - val_acc: 0.8655\n",
      "Epoch 479/1000\n",
      " - 1s - loss: 0.2625 - acc: 0.9103 - val_loss: 0.3855 - val_acc: 0.8669\n",
      "Epoch 480/1000\n",
      " - 1s - loss: 0.2627 - acc: 0.9093 - val_loss: 0.3836 - val_acc: 0.8688\n",
      "Epoch 481/1000\n",
      " - 1s - loss: 0.2624 - acc: 0.9096 - val_loss: 0.3846 - val_acc: 0.8677\n",
      "Epoch 482/1000\n",
      " - 1s - loss: 0.2622 - acc: 0.9096 - val_loss: 0.3872 - val_acc: 0.8685\n",
      "Epoch 483/1000\n",
      " - 1s - loss: 0.2619 - acc: 0.9095 - val_loss: 0.3850 - val_acc: 0.8689\n",
      "Epoch 484/1000\n",
      " - 1s - loss: 0.2621 - acc: 0.9104 - val_loss: 0.3861 - val_acc: 0.8675\n",
      "Epoch 485/1000\n",
      " - 1s - loss: 0.2618 - acc: 0.9108 - val_loss: 0.3904 - val_acc: 0.8670\n",
      "Epoch 486/1000\n",
      " - 1s - loss: 0.2614 - acc: 0.9105 - val_loss: 0.3876 - val_acc: 0.8693\n",
      "Epoch 487/1000\n",
      " - 1s - loss: 0.2615 - acc: 0.9100 - val_loss: 0.3881 - val_acc: 0.8690\n",
      "Epoch 488/1000\n",
      " - 1s - loss: 0.2615 - acc: 0.9108 - val_loss: 0.3857 - val_acc: 0.8681\n",
      "Epoch 489/1000\n",
      " - 1s - loss: 0.2612 - acc: 0.9105 - val_loss: 0.3844 - val_acc: 0.8694\n",
      "Epoch 490/1000\n",
      " - 1s - loss: 0.2611 - acc: 0.9107 - val_loss: 0.3857 - val_acc: 0.8678\n",
      "Epoch 491/1000\n",
      " - 1s - loss: 0.2608 - acc: 0.9107 - val_loss: 0.3862 - val_acc: 0.8693\n",
      "Epoch 492/1000\n",
      " - 1s - loss: 0.2611 - acc: 0.9104 - val_loss: 0.3866 - val_acc: 0.8685\n",
      "Epoch 493/1000\n",
      " - 1s - loss: 0.2606 - acc: 0.9105 - val_loss: 0.3867 - val_acc: 0.8694\n",
      "Epoch 494/1000\n",
      " - 1s - loss: 0.2603 - acc: 0.9108 - val_loss: 0.3886 - val_acc: 0.8681\n",
      "Epoch 495/1000\n",
      " - 1s - loss: 0.2606 - acc: 0.9108 - val_loss: 0.3883 - val_acc: 0.8688\n",
      "Epoch 496/1000\n",
      " - 2s - loss: 0.2604 - acc: 0.9106 - val_loss: 0.3861 - val_acc: 0.8702\n",
      "Epoch 497/1000\n",
      " - 2s - loss: 0.2605 - acc: 0.9112 - val_loss: 0.3862 - val_acc: 0.8690\n",
      "Epoch 498/1000\n",
      " - 2s - loss: 0.2599 - acc: 0.9111 - val_loss: 0.3855 - val_acc: 0.8695\n",
      "Epoch 499/1000\n",
      " - 1s - loss: 0.2600 - acc: 0.9108 - val_loss: 0.3893 - val_acc: 0.8666\n",
      "Epoch 500/1000\n",
      " - 1s - loss: 0.2600 - acc: 0.9114 - val_loss: 0.3849 - val_acc: 0.8678\n",
      "Epoch 501/1000\n",
      " - 3s - loss: 0.2596 - acc: 0.9107 - val_loss: 0.3862 - val_acc: 0.8680\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000500\n",
      "Epoch 502/1000\n",
      " - 2s - loss: 0.2595 - acc: 0.9113 - val_loss: 0.3900 - val_acc: 0.8648\n",
      "Epoch 503/1000\n",
      " - 2s - loss: 0.2593 - acc: 0.9116 - val_loss: 0.3901 - val_acc: 0.8669\n",
      "Epoch 504/1000\n",
      " - 2s - loss: 0.2592 - acc: 0.9108 - val_loss: 0.3934 - val_acc: 0.8660\n",
      "Epoch 505/1000\n",
      " - 2s - loss: 0.2592 - acc: 0.9111 - val_loss: 0.3861 - val_acc: 0.8671\n",
      "Epoch 506/1000\n",
      " - 2s - loss: 0.2590 - acc: 0.9121 - val_loss: 0.3855 - val_acc: 0.8697\n",
      "Epoch 507/1000\n",
      " - 3s - loss: 0.2587 - acc: 0.9114 - val_loss: 0.3852 - val_acc: 0.8688\n",
      "Epoch 508/1000\n",
      " - 2s - loss: 0.2587 - acc: 0.9115 - val_loss: 0.3854 - val_acc: 0.8690\n",
      "Epoch 509/1000\n",
      " - 2s - loss: 0.2584 - acc: 0.9120 - val_loss: 0.3858 - val_acc: 0.8703\n",
      "Epoch 510/1000\n",
      " - 2s - loss: 0.2584 - acc: 0.9116 - val_loss: 0.3861 - val_acc: 0.8692\n",
      "Epoch 511/1000\n",
      " - 2s - loss: 0.2587 - acc: 0.9113 - val_loss: 0.3859 - val_acc: 0.8683\n",
      "Epoch 512/1000\n",
      " - 2s - loss: 0.2584 - acc: 0.9117 - val_loss: 0.3900 - val_acc: 0.8684\n",
      "Epoch 513/1000\n",
      " - 1s - loss: 0.2581 - acc: 0.9111 - val_loss: 0.3856 - val_acc: 0.8703\n",
      "Epoch 514/1000\n",
      " - 1s - loss: 0.2579 - acc: 0.9118 - val_loss: 0.3851 - val_acc: 0.8693\n",
      "Epoch 515/1000\n",
      " - 1s - loss: 0.2578 - acc: 0.9113 - val_loss: 0.3850 - val_acc: 0.8687\n",
      "Epoch 516/1000\n",
      " - 1s - loss: 0.2581 - acc: 0.9116 - val_loss: 0.3871 - val_acc: 0.8682\n",
      "Epoch 517/1000\n",
      " - 2s - loss: 0.2580 - acc: 0.9122 - val_loss: 0.3868 - val_acc: 0.8699\n",
      "Epoch 518/1000\n",
      " - 2s - loss: 0.2574 - acc: 0.9116 - val_loss: 0.3864 - val_acc: 0.8707\n",
      "Epoch 519/1000\n",
      " - 2s - loss: 0.2577 - acc: 0.9112 - val_loss: 0.3909 - val_acc: 0.8673\n",
      "Epoch 520/1000\n",
      " - 2s - loss: 0.2574 - acc: 0.9118 - val_loss: 0.3862 - val_acc: 0.8702\n",
      "Epoch 521/1000\n",
      " - 2s - loss: 0.2570 - acc: 0.9123 - val_loss: 0.3904 - val_acc: 0.8666\n",
      "Epoch 522/1000\n",
      " - 2s - loss: 0.2574 - acc: 0.9125 - val_loss: 0.3860 - val_acc: 0.8669\n",
      "Epoch 523/1000\n",
      " - 2s - loss: 0.2569 - acc: 0.9124 - val_loss: 0.3865 - val_acc: 0.8674\n",
      "Epoch 524/1000\n",
      " - 2s - loss: 0.2570 - acc: 0.9115 - val_loss: 0.3859 - val_acc: 0.8688\n",
      "Epoch 525/1000\n",
      " - 2s - loss: 0.2567 - acc: 0.9125 - val_loss: 0.3879 - val_acc: 0.8661\n",
      "Epoch 526/1000\n",
      " - 2s - loss: 0.2567 - acc: 0.9115 - val_loss: 0.3859 - val_acc: 0.8703\n",
      "Epoch 527/1000\n",
      " - 2s - loss: 0.2565 - acc: 0.9124 - val_loss: 0.3867 - val_acc: 0.8688\n",
      "Epoch 528/1000\n",
      " - 2s - loss: 0.2565 - acc: 0.9121 - val_loss: 0.3848 - val_acc: 0.8701\n",
      "Epoch 529/1000\n",
      " - 3s - loss: 0.2562 - acc: 0.9122 - val_loss: 0.3919 - val_acc: 0.8657\n",
      "Epoch 530/1000\n",
      " - 2s - loss: 0.2560 - acc: 0.9123 - val_loss: 0.3864 - val_acc: 0.8703\n",
      "Epoch 531/1000\n",
      " - 2s - loss: 0.2560 - acc: 0.9124 - val_loss: 0.3877 - val_acc: 0.8686\n",
      "Epoch 532/1000\n",
      " - 2s - loss: 0.2560 - acc: 0.9124 - val_loss: 0.3885 - val_acc: 0.8683\n",
      "Epoch 533/1000\n",
      " - 2s - loss: 0.2557 - acc: 0.9122 - val_loss: 0.3890 - val_acc: 0.8680\n",
      "Epoch 534/1000\n",
      " - 3s - loss: 0.2556 - acc: 0.9121 - val_loss: 0.3865 - val_acc: 0.8686\n",
      "Epoch 535/1000\n",
      " - 2s - loss: 0.2554 - acc: 0.9132 - val_loss: 0.3886 - val_acc: 0.8692\n",
      "Epoch 536/1000\n",
      " - 1s - loss: 0.2555 - acc: 0.9127 - val_loss: 0.3874 - val_acc: 0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 537/1000\n",
      " - 1s - loss: 0.2549 - acc: 0.9129 - val_loss: 0.3872 - val_acc: 0.8687\n",
      "Epoch 538/1000\n",
      " - 1s - loss: 0.2551 - acc: 0.9127 - val_loss: 0.3905 - val_acc: 0.8671\n",
      "Epoch 539/1000\n",
      " - 1s - loss: 0.2548 - acc: 0.9128 - val_loss: 0.3891 - val_acc: 0.8685\n",
      "Epoch 540/1000\n",
      " - 1s - loss: 0.2549 - acc: 0.9129 - val_loss: 0.3861 - val_acc: 0.8685\n",
      "Epoch 541/1000\n",
      " - 1s - loss: 0.2548 - acc: 0.9130 - val_loss: 0.3873 - val_acc: 0.8698\n",
      "Epoch 542/1000\n",
      " - 1s - loss: 0.2545 - acc: 0.9128 - val_loss: 0.3867 - val_acc: 0.8696\n",
      "Epoch 543/1000\n",
      " - 2s - loss: 0.2546 - acc: 0.9126 - val_loss: 0.3853 - val_acc: 0.8697\n",
      "Epoch 544/1000\n",
      " - 1s - loss: 0.2547 - acc: 0.9128 - val_loss: 0.3888 - val_acc: 0.8682\n",
      "Epoch 545/1000\n",
      " - 1s - loss: 0.2542 - acc: 0.9131 - val_loss: 0.3882 - val_acc: 0.8677\n",
      "Epoch 546/1000\n",
      " - 1s - loss: 0.2543 - acc: 0.9128 - val_loss: 0.3863 - val_acc: 0.8694\n",
      "Epoch 547/1000\n",
      " - 1s - loss: 0.2538 - acc: 0.9136 - val_loss: 0.3865 - val_acc: 0.8690\n",
      "Epoch 548/1000\n",
      " - 1s - loss: 0.2540 - acc: 0.9129 - val_loss: 0.3887 - val_acc: 0.8690\n",
      "Epoch 549/1000\n",
      " - 1s - loss: 0.2536 - acc: 0.9134 - val_loss: 0.3930 - val_acc: 0.8669\n",
      "Epoch 550/1000\n",
      " - 1s - loss: 0.2537 - acc: 0.9135 - val_loss: 0.3879 - val_acc: 0.8702\n",
      "Epoch 551/1000\n",
      " - 1s - loss: 0.2537 - acc: 0.9125 - val_loss: 0.3891 - val_acc: 0.8699\n",
      "Epoch 552/1000\n",
      " - 1s - loss: 0.2532 - acc: 0.9134 - val_loss: 0.3875 - val_acc: 0.8689\n",
      "Epoch 553/1000\n",
      " - 1s - loss: 0.2532 - acc: 0.9138 - val_loss: 0.3866 - val_acc: 0.8696\n",
      "Epoch 554/1000\n",
      " - 1s - loss: 0.2532 - acc: 0.9134 - val_loss: 0.3868 - val_acc: 0.8697\n",
      "Epoch 555/1000\n",
      " - 1s - loss: 0.2531 - acc: 0.9136 - val_loss: 0.3880 - val_acc: 0.8700\n",
      "Epoch 556/1000\n",
      " - 1s - loss: 0.2527 - acc: 0.9131 - val_loss: 0.3875 - val_acc: 0.8704\n",
      "Epoch 557/1000\n",
      " - 1s - loss: 0.2525 - acc: 0.9137 - val_loss: 0.3874 - val_acc: 0.8707\n",
      "Epoch 558/1000\n",
      " - 1s - loss: 0.2528 - acc: 0.9131 - val_loss: 0.3881 - val_acc: 0.8697\n",
      "Epoch 559/1000\n",
      " - 1s - loss: 0.2527 - acc: 0.9137 - val_loss: 0.3872 - val_acc: 0.8681\n",
      "Epoch 560/1000\n",
      " - 1s - loss: 0.2523 - acc: 0.9136 - val_loss: 0.3903 - val_acc: 0.8659\n",
      "Epoch 561/1000\n",
      " - 1s - loss: 0.2520 - acc: 0.9141 - val_loss: 0.3917 - val_acc: 0.8678\n",
      "Epoch 562/1000\n",
      " - 1s - loss: 0.2523 - acc: 0.9133 - val_loss: 0.3871 - val_acc: 0.8703\n",
      "Epoch 563/1000\n",
      " - 1s - loss: 0.2521 - acc: 0.9138 - val_loss: 0.3872 - val_acc: 0.8699\n",
      "Epoch 564/1000\n",
      " - 1s - loss: 0.2523 - acc: 0.9138 - val_loss: 0.3898 - val_acc: 0.8693\n",
      "Epoch 565/1000\n",
      " - 1s - loss: 0.2521 - acc: 0.9134 - val_loss: 0.3878 - val_acc: 0.8695\n",
      "Epoch 566/1000\n",
      " - 1s - loss: 0.2522 - acc: 0.9133 - val_loss: 0.3867 - val_acc: 0.8699\n",
      "Epoch 567/1000\n",
      " - 1s - loss: 0.2515 - acc: 0.9135 - val_loss: 0.3887 - val_acc: 0.8683\n",
      "Epoch 568/1000\n",
      " - 1s - loss: 0.2516 - acc: 0.9139 - val_loss: 0.3895 - val_acc: 0.8681\n",
      "Epoch 569/1000\n",
      " - 1s - loss: 0.2512 - acc: 0.9141 - val_loss: 0.3887 - val_acc: 0.8700\n",
      "Epoch 570/1000\n",
      " - 1s - loss: 0.2513 - acc: 0.9147 - val_loss: 0.3878 - val_acc: 0.8690\n",
      "Epoch 571/1000\n",
      " - 1s - loss: 0.2513 - acc: 0.9142 - val_loss: 0.3888 - val_acc: 0.8689\n",
      "Epoch 572/1000\n",
      " - 2s - loss: 0.2512 - acc: 0.9147 - val_loss: 0.3918 - val_acc: 0.8680\n",
      "Epoch 573/1000\n",
      " - 1s - loss: 0.2515 - acc: 0.9142 - val_loss: 0.3878 - val_acc: 0.8684\n",
      "Epoch 574/1000\n",
      " - 1s - loss: 0.2511 - acc: 0.9140 - val_loss: 0.3880 - val_acc: 0.8692\n",
      "Epoch 575/1000\n",
      " - 1s - loss: 0.2505 - acc: 0.9148 - val_loss: 0.3873 - val_acc: 0.8699\n",
      "Epoch 576/1000\n",
      " - 1s - loss: 0.2507 - acc: 0.9147 - val_loss: 0.3919 - val_acc: 0.8685\n",
      "Epoch 577/1000\n",
      " - 1s - loss: 0.2506 - acc: 0.9136 - val_loss: 0.3946 - val_acc: 0.8650\n",
      "Epoch 578/1000\n",
      " - 1s - loss: 0.2506 - acc: 0.9145 - val_loss: 0.3878 - val_acc: 0.8694\n",
      "Epoch 579/1000\n",
      " - 1s - loss: 0.2503 - acc: 0.9141 - val_loss: 0.3889 - val_acc: 0.8704\n",
      "Epoch 580/1000\n",
      " - 1s - loss: 0.2503 - acc: 0.9144 - val_loss: 0.3913 - val_acc: 0.8685\n",
      "Epoch 581/1000\n",
      " - 1s - loss: 0.2502 - acc: 0.9147 - val_loss: 0.3892 - val_acc: 0.8699\n",
      "Epoch 582/1000\n",
      " - 1s - loss: 0.2502 - acc: 0.9146 - val_loss: 0.3882 - val_acc: 0.8693\n",
      "Epoch 583/1000\n",
      " - 1s - loss: 0.2500 - acc: 0.9139 - val_loss: 0.3877 - val_acc: 0.8711\n",
      "Epoch 584/1000\n",
      " - 1s - loss: 0.2499 - acc: 0.9147 - val_loss: 0.3920 - val_acc: 0.8665\n",
      "Epoch 585/1000\n",
      " - 1s - loss: 0.2499 - acc: 0.9148 - val_loss: 0.3894 - val_acc: 0.8684\n",
      "Epoch 586/1000\n",
      " - 1s - loss: 0.2498 - acc: 0.9143 - val_loss: 0.3900 - val_acc: 0.8693\n",
      "Epoch 587/1000\n",
      " - 1s - loss: 0.2496 - acc: 0.9146 - val_loss: 0.3900 - val_acc: 0.8680\n",
      "Epoch 588/1000\n",
      " - 1s - loss: 0.2494 - acc: 0.9149 - val_loss: 0.3896 - val_acc: 0.8689\n",
      "Epoch 589/1000\n",
      " - 1s - loss: 0.2494 - acc: 0.9148 - val_loss: 0.3916 - val_acc: 0.8671\n",
      "Epoch 590/1000\n",
      " - 1s - loss: 0.2492 - acc: 0.9144 - val_loss: 0.3921 - val_acc: 0.8672\n",
      "Epoch 591/1000\n",
      " - 2s - loss: 0.2491 - acc: 0.9147 - val_loss: 0.3886 - val_acc: 0.8706\n",
      "Epoch 592/1000\n",
      " - 2s - loss: 0.2488 - acc: 0.9152 - val_loss: 0.3889 - val_acc: 0.8704\n",
      "Epoch 593/1000\n",
      " - 2s - loss: 0.2494 - acc: 0.9145 - val_loss: 0.3887 - val_acc: 0.8716\n",
      "Epoch 594/1000\n",
      " - 1s - loss: 0.2486 - acc: 0.9152 - val_loss: 0.3930 - val_acc: 0.8696\n",
      "Epoch 595/1000\n",
      " - 1s - loss: 0.2488 - acc: 0.9149 - val_loss: 0.3884 - val_acc: 0.8697\n",
      "Epoch 596/1000\n",
      " - 2s - loss: 0.2486 - acc: 0.9150 - val_loss: 0.3922 - val_acc: 0.8656\n",
      "Epoch 597/1000\n",
      " - 1s - loss: 0.2487 - acc: 0.9154 - val_loss: 0.3893 - val_acc: 0.8700\n",
      "Epoch 598/1000\n",
      " - 2s - loss: 0.2481 - acc: 0.9152 - val_loss: 0.3888 - val_acc: 0.8697\n",
      "Epoch 599/1000\n",
      " - 2s - loss: 0.2484 - acc: 0.9155 - val_loss: 0.3898 - val_acc: 0.8701\n",
      "Epoch 600/1000\n",
      " - 1s - loss: 0.2479 - acc: 0.9154 - val_loss: 0.3892 - val_acc: 0.8697\n",
      "Epoch 601/1000\n",
      " - 3s - loss: 0.2482 - acc: 0.9150 - val_loss: 0.3910 - val_acc: 0.8697\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000600\n",
      "Epoch 602/1000\n",
      " - 2s - loss: 0.2477 - acc: 0.9152 - val_loss: 0.3903 - val_acc: 0.8681\n",
      "Epoch 603/1000\n",
      " - 1s - loss: 0.2476 - acc: 0.9157 - val_loss: 0.3893 - val_acc: 0.8692\n",
      "Epoch 604/1000\n",
      " - 2s - loss: 0.2475 - acc: 0.9153 - val_loss: 0.3911 - val_acc: 0.8698\n",
      "Epoch 605/1000\n",
      " - 2s - loss: 0.2474 - acc: 0.9154 - val_loss: 0.3906 - val_acc: 0.8709\n",
      "Epoch 606/1000\n",
      " - 2s - loss: 0.2475 - acc: 0.9153 - val_loss: 0.3925 - val_acc: 0.8688\n",
      "Epoch 607/1000\n",
      " - 2s - loss: 0.2472 - acc: 0.9154 - val_loss: 0.3933 - val_acc: 0.8667\n",
      "Epoch 608/1000\n",
      " - 2s - loss: 0.2473 - acc: 0.9152 - val_loss: 0.3910 - val_acc: 0.8692\n",
      "Epoch 609/1000\n",
      " - 2s - loss: 0.2468 - acc: 0.9158 - val_loss: 0.3920 - val_acc: 0.8683\n",
      "Epoch 610/1000\n",
      " - 2s - loss: 0.2470 - acc: 0.9152 - val_loss: 0.3889 - val_acc: 0.8725\n",
      "Epoch 611/1000\n",
      " - 3s - loss: 0.2468 - acc: 0.9159 - val_loss: 0.3929 - val_acc: 0.8686\n",
      "Epoch 612/1000\n",
      " - 2s - loss: 0.2468 - acc: 0.9161 - val_loss: 0.3889 - val_acc: 0.8714\n",
      "Epoch 613/1000\n",
      " - 3s - loss: 0.2464 - acc: 0.9164 - val_loss: 0.3910 - val_acc: 0.8706\n",
      "Epoch 614/1000\n",
      " - 3s - loss: 0.2465 - acc: 0.9161 - val_loss: 0.3888 - val_acc: 0.8693\n",
      "Epoch 615/1000\n",
      " - 3s - loss: 0.2462 - acc: 0.9158 - val_loss: 0.3893 - val_acc: 0.8708\n",
      "Epoch 616/1000\n",
      " - 3s - loss: 0.2464 - acc: 0.9161 - val_loss: 0.3899 - val_acc: 0.8706\n",
      "Epoch 617/1000\n",
      " - 3s - loss: 0.2464 - acc: 0.9162 - val_loss: 0.3925 - val_acc: 0.8688\n",
      "Epoch 618/1000\n",
      " - 2s - loss: 0.2463 - acc: 0.9160 - val_loss: 0.3912 - val_acc: 0.8705\n",
      "Epoch 619/1000\n",
      " - 2s - loss: 0.2460 - acc: 0.9158 - val_loss: 0.3892 - val_acc: 0.8715\n",
      "Epoch 620/1000\n",
      " - 1s - loss: 0.2459 - acc: 0.9162 - val_loss: 0.3922 - val_acc: 0.8703\n",
      "Epoch 621/1000\n",
      " - 2s - loss: 0.2457 - acc: 0.9163 - val_loss: 0.3892 - val_acc: 0.8688\n",
      "Epoch 622/1000\n",
      " - 2s - loss: 0.2459 - acc: 0.9159 - val_loss: 0.3914 - val_acc: 0.8671\n",
      "Epoch 623/1000\n",
      " - 1s - loss: 0.2461 - acc: 0.9163 - val_loss: 0.3906 - val_acc: 0.8699\n",
      "Epoch 624/1000\n",
      " - 1s - loss: 0.2458 - acc: 0.9168 - val_loss: 0.3931 - val_acc: 0.8705\n",
      "Epoch 625/1000\n",
      " - 2s - loss: 0.2454 - acc: 0.9165 - val_loss: 0.3918 - val_acc: 0.8691\n",
      "Epoch 626/1000\n",
      " - 1s - loss: 0.2453 - acc: 0.9166 - val_loss: 0.3939 - val_acc: 0.8675\n",
      "Epoch 627/1000\n",
      " - 1s - loss: 0.2455 - acc: 0.9164 - val_loss: 0.3921 - val_acc: 0.8696\n",
      "Epoch 628/1000\n",
      " - 2s - loss: 0.2451 - acc: 0.9170 - val_loss: 0.3895 - val_acc: 0.8704\n",
      "Epoch 629/1000\n",
      " - 1s - loss: 0.2449 - acc: 0.9164 - val_loss: 0.3900 - val_acc: 0.8716\n",
      "Epoch 630/1000\n",
      " - 2s - loss: 0.2450 - acc: 0.9171 - val_loss: 0.3923 - val_acc: 0.8699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 631/1000\n",
      " - 1s - loss: 0.2449 - acc: 0.9165 - val_loss: 0.3918 - val_acc: 0.8693\n",
      "Epoch 632/1000\n",
      " - 2s - loss: 0.2450 - acc: 0.9162 - val_loss: 0.3900 - val_acc: 0.8698\n",
      "Epoch 633/1000\n",
      " - 2s - loss: 0.2449 - acc: 0.9164 - val_loss: 0.3905 - val_acc: 0.8719\n",
      "Epoch 634/1000\n",
      " - 1s - loss: 0.2445 - acc: 0.9165 - val_loss: 0.3907 - val_acc: 0.8701\n",
      "Epoch 635/1000\n",
      " - 1s - loss: 0.2446 - acc: 0.9166 - val_loss: 0.3903 - val_acc: 0.8691\n",
      "Epoch 636/1000\n",
      " - 1s - loss: 0.2438 - acc: 0.9165 - val_loss: 0.3914 - val_acc: 0.8701\n",
      "Epoch 637/1000\n",
      " - 2s - loss: 0.2444 - acc: 0.9165 - val_loss: 0.3896 - val_acc: 0.8704\n",
      "Epoch 638/1000\n",
      " - 1s - loss: 0.2442 - acc: 0.9162 - val_loss: 0.3914 - val_acc: 0.8711\n",
      "Epoch 639/1000\n",
      " - 2s - loss: 0.2441 - acc: 0.9171 - val_loss: 0.3950 - val_acc: 0.8680\n",
      "Epoch 640/1000\n",
      " - 2s - loss: 0.2440 - acc: 0.9172 - val_loss: 0.3901 - val_acc: 0.8714\n",
      "Epoch 641/1000\n",
      " - 1s - loss: 0.2439 - acc: 0.9170 - val_loss: 0.3905 - val_acc: 0.8709\n",
      "Epoch 642/1000\n",
      " - 2s - loss: 0.2438 - acc: 0.9163 - val_loss: 0.3924 - val_acc: 0.8706\n",
      "Epoch 643/1000\n",
      " - 1s - loss: 0.2437 - acc: 0.9169 - val_loss: 0.3894 - val_acc: 0.8705\n",
      "Epoch 644/1000\n",
      " - 2s - loss: 0.2435 - acc: 0.9168 - val_loss: 0.3903 - val_acc: 0.8704\n",
      "Epoch 645/1000\n",
      " - 2s - loss: 0.2436 - acc: 0.9167 - val_loss: 0.3915 - val_acc: 0.8707\n",
      "Epoch 646/1000\n",
      " - 1s - loss: 0.2429 - acc: 0.9174 - val_loss: 0.3954 - val_acc: 0.8688\n",
      "Epoch 647/1000\n",
      " - 1s - loss: 0.2430 - acc: 0.9171 - val_loss: 0.3946 - val_acc: 0.8687\n",
      "Epoch 648/1000\n",
      " - 2s - loss: 0.2431 - acc: 0.9169 - val_loss: 0.3919 - val_acc: 0.8682\n",
      "Epoch 649/1000\n",
      " - 2s - loss: 0.2430 - acc: 0.9170 - val_loss: 0.3915 - val_acc: 0.8704\n",
      "Epoch 650/1000\n",
      " - 1s - loss: 0.2430 - acc: 0.9175 - val_loss: 0.3920 - val_acc: 0.8685\n",
      "Epoch 651/1000\n",
      " - 2s - loss: 0.2431 - acc: 0.9170 - val_loss: 0.3915 - val_acc: 0.8710\n",
      "Epoch 652/1000\n",
      " - 2s - loss: 0.2426 - acc: 0.9175 - val_loss: 0.3908 - val_acc: 0.8710\n",
      "Epoch 653/1000\n",
      " - 1s - loss: 0.2423 - acc: 0.9175 - val_loss: 0.3937 - val_acc: 0.8689\n",
      "Epoch 654/1000\n",
      " - 2s - loss: 0.2427 - acc: 0.9174 - val_loss: 0.3911 - val_acc: 0.8703\n",
      "Epoch 655/1000\n",
      " - 2s - loss: 0.2424 - acc: 0.9172 - val_loss: 0.3933 - val_acc: 0.8692\n",
      "Epoch 656/1000\n",
      " - 2s - loss: 0.2425 - acc: 0.9175 - val_loss: 0.3939 - val_acc: 0.8684\n",
      "Epoch 657/1000\n",
      " - 2s - loss: 0.2420 - acc: 0.9176 - val_loss: 0.3919 - val_acc: 0.8705\n",
      "Epoch 658/1000\n",
      " - 2s - loss: 0.2418 - acc: 0.9187 - val_loss: 0.3928 - val_acc: 0.8699\n",
      "Epoch 659/1000\n",
      " - 2s - loss: 0.2423 - acc: 0.9175 - val_loss: 0.3914 - val_acc: 0.8705\n",
      "Epoch 660/1000\n",
      " - 1s - loss: 0.2416 - acc: 0.9177 - val_loss: 0.3927 - val_acc: 0.8694\n",
      "Epoch 661/1000\n",
      " - 1s - loss: 0.2415 - acc: 0.9177 - val_loss: 0.3915 - val_acc: 0.8710\n",
      "Epoch 662/1000\n",
      " - 2s - loss: 0.2417 - acc: 0.9177 - val_loss: 0.3923 - val_acc: 0.8688\n",
      "Epoch 663/1000\n",
      " - 2s - loss: 0.2415 - acc: 0.9177 - val_loss: 0.3952 - val_acc: 0.8677\n",
      "Epoch 664/1000\n",
      " - 1s - loss: 0.2412 - acc: 0.9176 - val_loss: 0.3952 - val_acc: 0.8682\n",
      "Epoch 665/1000\n",
      " - 1s - loss: 0.2416 - acc: 0.9174 - val_loss: 0.3940 - val_acc: 0.8702\n",
      "Epoch 666/1000\n",
      " - 1s - loss: 0.2414 - acc: 0.9183 - val_loss: 0.3923 - val_acc: 0.8713\n",
      "Epoch 667/1000\n",
      " - 1s - loss: 0.2414 - acc: 0.9173 - val_loss: 0.3932 - val_acc: 0.8705\n",
      "Epoch 668/1000\n",
      " - 2s - loss: 0.2411 - acc: 0.9176 - val_loss: 0.3932 - val_acc: 0.8704\n",
      "Epoch 669/1000\n",
      " - 2s - loss: 0.2412 - acc: 0.9181 - val_loss: 0.3921 - val_acc: 0.8687\n",
      "Epoch 670/1000\n",
      " - 2s - loss: 0.2406 - acc: 0.9181 - val_loss: 0.3924 - val_acc: 0.8689\n",
      "Epoch 671/1000\n",
      " - 2s - loss: 0.2407 - acc: 0.9183 - val_loss: 0.3955 - val_acc: 0.8701\n",
      "Epoch 672/1000\n",
      " - 1s - loss: 0.2407 - acc: 0.9184 - val_loss: 0.3920 - val_acc: 0.8707\n",
      "Epoch 673/1000\n",
      " - 2s - loss: 0.2407 - acc: 0.9175 - val_loss: 0.3933 - val_acc: 0.8702\n",
      "Epoch 674/1000\n",
      " - 2s - loss: 0.2407 - acc: 0.9181 - val_loss: 0.3984 - val_acc: 0.8674\n",
      "Epoch 675/1000\n",
      " - 2s - loss: 0.2404 - acc: 0.9184 - val_loss: 0.3928 - val_acc: 0.8701\n",
      "Epoch 676/1000\n",
      " - 2s - loss: 0.2401 - acc: 0.9179 - val_loss: 0.3938 - val_acc: 0.8697\n",
      "Epoch 677/1000\n",
      " - 2s - loss: 0.2398 - acc: 0.9186 - val_loss: 0.3936 - val_acc: 0.8693\n",
      "Epoch 678/1000\n",
      " - 2s - loss: 0.2405 - acc: 0.9179 - val_loss: 0.3927 - val_acc: 0.8714\n",
      "Epoch 679/1000\n",
      " - 2s - loss: 0.2400 - acc: 0.9184 - val_loss: 0.3928 - val_acc: 0.8700\n",
      "Epoch 680/1000\n",
      " - 2s - loss: 0.2400 - acc: 0.9177 - val_loss: 0.3915 - val_acc: 0.8716\n",
      "Epoch 681/1000\n",
      " - 1s - loss: 0.2397 - acc: 0.9177 - val_loss: 0.3938 - val_acc: 0.8697\n",
      "Epoch 682/1000\n",
      " - 1s - loss: 0.2398 - acc: 0.9189 - val_loss: 0.3955 - val_acc: 0.8689\n",
      "Epoch 683/1000\n",
      " - 1s - loss: 0.2398 - acc: 0.9185 - val_loss: 0.3949 - val_acc: 0.8694\n",
      "Epoch 684/1000\n",
      " - 1s - loss: 0.2397 - acc: 0.9180 - val_loss: 0.3928 - val_acc: 0.8699\n",
      "Epoch 685/1000\n",
      " - 1s - loss: 0.2392 - acc: 0.9187 - val_loss: 0.3967 - val_acc: 0.8697\n",
      "Epoch 686/1000\n",
      " - 1s - loss: 0.2393 - acc: 0.9186 - val_loss: 0.3928 - val_acc: 0.8698\n",
      "Epoch 687/1000\n",
      " - 1s - loss: 0.2395 - acc: 0.9188 - val_loss: 0.3943 - val_acc: 0.8696\n",
      "Epoch 688/1000\n",
      " - 2s - loss: 0.2393 - acc: 0.9181 - val_loss: 0.3955 - val_acc: 0.8682\n",
      "Epoch 689/1000\n",
      " - 2s - loss: 0.2391 - acc: 0.9193 - val_loss: 0.3976 - val_acc: 0.8680\n",
      "Epoch 690/1000\n",
      " - 2s - loss: 0.2389 - acc: 0.9191 - val_loss: 0.3937 - val_acc: 0.8688\n",
      "Epoch 691/1000\n",
      " - 2s - loss: 0.2389 - acc: 0.9185 - val_loss: 0.3950 - val_acc: 0.8696\n",
      "Epoch 692/1000\n",
      " - 1s - loss: 0.2387 - acc: 0.9190 - val_loss: 0.3935 - val_acc: 0.8703\n",
      "Epoch 693/1000\n",
      " - 1s - loss: 0.2386 - acc: 0.9190 - val_loss: 0.3948 - val_acc: 0.8708\n",
      "Epoch 694/1000\n",
      " - 2s - loss: 0.2385 - acc: 0.9186 - val_loss: 0.3957 - val_acc: 0.8694\n",
      "Epoch 695/1000\n",
      " - 2s - loss: 0.2385 - acc: 0.9186 - val_loss: 0.3943 - val_acc: 0.8709\n",
      "Epoch 696/1000\n",
      " - 2s - loss: 0.2385 - acc: 0.9180 - val_loss: 0.3929 - val_acc: 0.8706\n",
      "Epoch 697/1000\n",
      " - 2s - loss: 0.2384 - acc: 0.9193 - val_loss: 0.3933 - val_acc: 0.8704\n",
      "Epoch 698/1000\n",
      " - 2s - loss: 0.2382 - acc: 0.9186 - val_loss: 0.3926 - val_acc: 0.8703\n",
      "Epoch 699/1000\n",
      " - 2s - loss: 0.2380 - acc: 0.9191 - val_loss: 0.3938 - val_acc: 0.8693\n",
      "Epoch 700/1000\n",
      " - 2s - loss: 0.2378 - acc: 0.9187 - val_loss: 0.3953 - val_acc: 0.8702\n",
      "Epoch 701/1000\n",
      " - 3s - loss: 0.2380 - acc: 0.9189 - val_loss: 0.3960 - val_acc: 0.8682\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000700\n",
      "Epoch 702/1000\n",
      " - 2s - loss: 0.2380 - acc: 0.9183 - val_loss: 0.3978 - val_acc: 0.8688\n",
      "Epoch 703/1000\n",
      " - 2s - loss: 0.2379 - acc: 0.9190 - val_loss: 0.3935 - val_acc: 0.8719\n",
      "Epoch 704/1000\n",
      " - 2s - loss: 0.2379 - acc: 0.9190 - val_loss: 0.3955 - val_acc: 0.8698\n",
      "Epoch 705/1000\n",
      " - 1s - loss: 0.2376 - acc: 0.9192 - val_loss: 0.3933 - val_acc: 0.8710\n",
      "Epoch 706/1000\n",
      " - 1s - loss: 0.2378 - acc: 0.9194 - val_loss: 0.3965 - val_acc: 0.8691\n",
      "Epoch 707/1000\n",
      " - 2s - loss: 0.2376 - acc: 0.9192 - val_loss: 0.3953 - val_acc: 0.8696\n",
      "Epoch 708/1000\n",
      " - 1s - loss: 0.2370 - acc: 0.9194 - val_loss: 0.3966 - val_acc: 0.8700\n",
      "Epoch 709/1000\n",
      " - 2s - loss: 0.2375 - acc: 0.9189 - val_loss: 0.3926 - val_acc: 0.8700\n",
      "Epoch 710/1000\n",
      " - 2s - loss: 0.2371 - acc: 0.9195 - val_loss: 0.3930 - val_acc: 0.8696\n",
      "Epoch 711/1000\n",
      " - 2s - loss: 0.2372 - acc: 0.9191 - val_loss: 0.3932 - val_acc: 0.8702\n",
      "Epoch 712/1000\n",
      " - 2s - loss: 0.2371 - acc: 0.9193 - val_loss: 0.3937 - val_acc: 0.8699\n",
      "Epoch 713/1000\n",
      " - 2s - loss: 0.2367 - acc: 0.9195 - val_loss: 0.3979 - val_acc: 0.8681\n",
      "Epoch 714/1000\n",
      " - 1s - loss: 0.2369 - acc: 0.9193 - val_loss: 0.3949 - val_acc: 0.8686\n",
      "Epoch 715/1000\n",
      " - 2s - loss: 0.2367 - acc: 0.9194 - val_loss: 0.3960 - val_acc: 0.8682\n",
      "Epoch 716/1000\n",
      " - 2s - loss: 0.2368 - acc: 0.9197 - val_loss: 0.3946 - val_acc: 0.8689\n",
      "Epoch 717/1000\n",
      " - 1s - loss: 0.2365 - acc: 0.9193 - val_loss: 0.3964 - val_acc: 0.8681\n",
      "Epoch 718/1000\n",
      " - 2s - loss: 0.2364 - acc: 0.9198 - val_loss: 0.3975 - val_acc: 0.8700\n",
      "Epoch 719/1000\n",
      " - 2s - loss: 0.2367 - acc: 0.9193 - val_loss: 0.3974 - val_acc: 0.8685\n",
      "Epoch 720/1000\n",
      " - 2s - loss: 0.2360 - acc: 0.9198 - val_loss: 0.3960 - val_acc: 0.8691\n",
      "Epoch 721/1000\n",
      " - 2s - loss: 0.2362 - acc: 0.9194 - val_loss: 0.3947 - val_acc: 0.8707\n",
      "Epoch 722/1000\n",
      " - 1s - loss: 0.2361 - acc: 0.9197 - val_loss: 0.3985 - val_acc: 0.8685\n",
      "Epoch 723/1000\n",
      " - 2s - loss: 0.2363 - acc: 0.9193 - val_loss: 0.3937 - val_acc: 0.8709\n",
      "Epoch 724/1000\n",
      " - 2s - loss: 0.2360 - acc: 0.9193 - val_loss: 0.3976 - val_acc: 0.8679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 725/1000\n",
      " - 1s - loss: 0.2355 - acc: 0.9199 - val_loss: 0.4026 - val_acc: 0.8675\n",
      "Epoch 726/1000\n",
      " - 2s - loss: 0.2357 - acc: 0.9194 - val_loss: 0.3939 - val_acc: 0.8698\n",
      "Epoch 727/1000\n",
      " - 2s - loss: 0.2357 - acc: 0.9193 - val_loss: 0.3976 - val_acc: 0.8680\n",
      "Epoch 728/1000\n",
      " - 2s - loss: 0.2354 - acc: 0.9199 - val_loss: 0.4008 - val_acc: 0.8690\n",
      "Epoch 729/1000\n",
      " - 2s - loss: 0.2352 - acc: 0.9193 - val_loss: 0.3953 - val_acc: 0.8705\n",
      "Epoch 730/1000\n",
      " - 2s - loss: 0.2356 - acc: 0.9198 - val_loss: 0.3977 - val_acc: 0.8682\n",
      "Epoch 731/1000\n",
      " - 2s - loss: 0.2349 - acc: 0.9208 - val_loss: 0.3969 - val_acc: 0.8696\n",
      "Epoch 732/1000\n",
      " - 2s - loss: 0.2356 - acc: 0.9198 - val_loss: 0.3941 - val_acc: 0.8711\n",
      "Epoch 733/1000\n",
      " - 2s - loss: 0.2349 - acc: 0.9202 - val_loss: 0.3958 - val_acc: 0.8681\n",
      "Epoch 734/1000\n",
      " - 2s - loss: 0.2349 - acc: 0.9201 - val_loss: 0.3981 - val_acc: 0.8690\n",
      "Epoch 735/1000\n",
      " - 2s - loss: 0.2348 - acc: 0.9199 - val_loss: 0.3953 - val_acc: 0.8712\n",
      "Epoch 736/1000\n",
      " - 2s - loss: 0.2346 - acc: 0.9202 - val_loss: 0.3969 - val_acc: 0.8700\n",
      "Epoch 737/1000\n",
      " - 2s - loss: 0.2350 - acc: 0.9199 - val_loss: 0.3950 - val_acc: 0.8698\n",
      "Epoch 738/1000\n",
      " - 2s - loss: 0.2346 - acc: 0.9202 - val_loss: 0.3977 - val_acc: 0.8708\n",
      "Epoch 739/1000\n",
      " - 2s - loss: 0.2346 - acc: 0.9200 - val_loss: 0.3974 - val_acc: 0.8688\n",
      "Epoch 740/1000\n",
      " - 2s - loss: 0.2343 - acc: 0.9201 - val_loss: 0.3958 - val_acc: 0.8705\n",
      "Epoch 741/1000\n",
      " - 2s - loss: 0.2349 - acc: 0.9201 - val_loss: 0.3964 - val_acc: 0.8701\n",
      "Epoch 742/1000\n",
      " - 2s - loss: 0.2339 - acc: 0.9205 - val_loss: 0.4000 - val_acc: 0.8692\n",
      "Epoch 743/1000\n",
      " - 2s - loss: 0.2344 - acc: 0.9209 - val_loss: 0.3984 - val_acc: 0.8677\n",
      "Epoch 744/1000\n",
      " - 2s - loss: 0.2341 - acc: 0.9201 - val_loss: 0.3999 - val_acc: 0.8681\n",
      "Epoch 745/1000\n",
      " - 2s - loss: 0.2337 - acc: 0.9205 - val_loss: 0.3954 - val_acc: 0.8704\n",
      "Epoch 746/1000\n",
      " - 2s - loss: 0.2341 - acc: 0.9206 - val_loss: 0.3998 - val_acc: 0.8681\n",
      "Epoch 747/1000\n",
      " - 2s - loss: 0.2336 - acc: 0.9208 - val_loss: 0.3950 - val_acc: 0.8693\n",
      "Epoch 748/1000\n",
      " - 2s - loss: 0.2338 - acc: 0.9211 - val_loss: 0.4062 - val_acc: 0.8674\n",
      "Epoch 749/1000\n",
      " - 2s - loss: 0.2334 - acc: 0.9206 - val_loss: 0.3970 - val_acc: 0.8698\n",
      "Epoch 750/1000\n",
      " - 1s - loss: 0.2333 - acc: 0.9207 - val_loss: 0.3951 - val_acc: 0.8697\n",
      "Epoch 751/1000\n",
      " - 2s - loss: 0.2332 - acc: 0.9213 - val_loss: 0.4006 - val_acc: 0.8686\n",
      "Epoch 752/1000\n",
      " - 2s - loss: 0.2336 - acc: 0.9206 - val_loss: 0.3985 - val_acc: 0.8697\n",
      "Epoch 753/1000\n",
      " - 2s - loss: 0.2332 - acc: 0.9205 - val_loss: 0.4055 - val_acc: 0.8671\n",
      "Epoch 754/1000\n",
      " - 2s - loss: 0.2330 - acc: 0.9211 - val_loss: 0.3988 - val_acc: 0.8678\n",
      "Epoch 755/1000\n",
      " - 2s - loss: 0.2328 - acc: 0.9215 - val_loss: 0.4021 - val_acc: 0.8673\n",
      "Epoch 756/1000\n",
      " - 2s - loss: 0.2328 - acc: 0.9215 - val_loss: 0.3969 - val_acc: 0.8693\n",
      "Epoch 757/1000\n",
      " - 1s - loss: 0.2331 - acc: 0.9207 - val_loss: 0.3969 - val_acc: 0.8697\n",
      "Epoch 758/1000\n",
      " - 2s - loss: 0.2329 - acc: 0.9203 - val_loss: 0.3962 - val_acc: 0.8698\n",
      "Epoch 759/1000\n",
      " - 1s - loss: 0.2329 - acc: 0.9213 - val_loss: 0.4008 - val_acc: 0.8684\n",
      "Epoch 760/1000\n",
      " - 2s - loss: 0.2327 - acc: 0.9210 - val_loss: 0.3964 - val_acc: 0.8699\n",
      "Epoch 761/1000\n",
      " - 2s - loss: 0.2325 - acc: 0.9215 - val_loss: 0.3964 - val_acc: 0.8704\n",
      "Epoch 762/1000\n",
      " - 1s - loss: 0.2325 - acc: 0.9217 - val_loss: 0.3979 - val_acc: 0.8707\n",
      "Epoch 763/1000\n",
      " - 2s - loss: 0.2328 - acc: 0.9213 - val_loss: 0.3965 - val_acc: 0.8696\n",
      "Epoch 764/1000\n",
      " - 2s - loss: 0.2321 - acc: 0.9216 - val_loss: 0.3971 - val_acc: 0.8691\n",
      "Epoch 765/1000\n",
      " - 2s - loss: 0.2323 - acc: 0.9212 - val_loss: 0.4018 - val_acc: 0.8700\n",
      "Epoch 766/1000\n",
      " - 1s - loss: 0.2321 - acc: 0.9209 - val_loss: 0.4003 - val_acc: 0.8689\n",
      "Epoch 767/1000\n",
      " - 1s - loss: 0.2318 - acc: 0.9217 - val_loss: 0.3965 - val_acc: 0.8701\n",
      "Epoch 768/1000\n",
      " - 2s - loss: 0.2317 - acc: 0.9210 - val_loss: 0.4055 - val_acc: 0.8677\n",
      "Epoch 769/1000\n",
      " - 2s - loss: 0.2319 - acc: 0.9215 - val_loss: 0.3992 - val_acc: 0.8689\n",
      "Epoch 770/1000\n",
      " - 2s - loss: 0.2319 - acc: 0.9213 - val_loss: 0.4040 - val_acc: 0.8678\n",
      "Epoch 771/1000\n",
      " - 2s - loss: 0.2318 - acc: 0.9209 - val_loss: 0.3962 - val_acc: 0.8705\n",
      "Epoch 772/1000\n",
      " - 2s - loss: 0.2319 - acc: 0.9211 - val_loss: 0.3966 - val_acc: 0.8702\n",
      "Epoch 773/1000\n",
      " - 2s - loss: 0.2315 - acc: 0.9209 - val_loss: 0.3975 - val_acc: 0.8704\n",
      "Epoch 774/1000\n",
      " - 2s - loss: 0.2314 - acc: 0.9219 - val_loss: 0.3996 - val_acc: 0.8695\n",
      "Epoch 775/1000\n",
      " - 2s - loss: 0.2309 - acc: 0.9215 - val_loss: 0.3976 - val_acc: 0.8698\n",
      "Epoch 776/1000\n",
      " - 1s - loss: 0.2312 - acc: 0.9217 - val_loss: 0.3977 - val_acc: 0.8702\n",
      "Epoch 777/1000\n",
      " - 1s - loss: 0.2311 - acc: 0.9207 - val_loss: 0.3971 - val_acc: 0.8685\n",
      "Epoch 778/1000\n",
      " - 1s - loss: 0.2312 - acc: 0.9212 - val_loss: 0.4008 - val_acc: 0.8694\n",
      "Epoch 779/1000\n",
      " - 2s - loss: 0.2308 - acc: 0.9219 - val_loss: 0.4014 - val_acc: 0.8681\n",
      "Epoch 780/1000\n",
      " - 1s - loss: 0.2309 - acc: 0.9218 - val_loss: 0.3991 - val_acc: 0.8697\n",
      "Epoch 781/1000\n",
      " - 2s - loss: 0.2306 - acc: 0.9220 - val_loss: 0.3980 - val_acc: 0.8689\n",
      "Epoch 782/1000\n",
      " - 2s - loss: 0.2309 - acc: 0.9210 - val_loss: 0.3997 - val_acc: 0.8687\n",
      "Epoch 783/1000\n",
      " - 1s - loss: 0.2305 - acc: 0.9216 - val_loss: 0.3979 - val_acc: 0.8700\n",
      "Epoch 784/1000\n",
      " - 1s - loss: 0.2306 - acc: 0.9217 - val_loss: 0.4000 - val_acc: 0.8698\n",
      "Epoch 785/1000\n",
      " - 2s - loss: 0.2306 - acc: 0.9215 - val_loss: 0.4000 - val_acc: 0.8697\n",
      "Epoch 786/1000\n",
      " - 2s - loss: 0.2304 - acc: 0.9218 - val_loss: 0.4029 - val_acc: 0.8703\n",
      "Epoch 787/1000\n",
      " - 2s - loss: 0.2305 - acc: 0.9219 - val_loss: 0.4027 - val_acc: 0.8702\n",
      "Epoch 788/1000\n",
      " - 2s - loss: 0.2304 - acc: 0.9222 - val_loss: 0.4030 - val_acc: 0.8677\n",
      "Epoch 789/1000\n",
      " - 2s - loss: 0.2297 - acc: 0.9220 - val_loss: 0.3999 - val_acc: 0.8684\n",
      "Epoch 790/1000\n",
      " - 1s - loss: 0.2303 - acc: 0.9220 - val_loss: 0.3973 - val_acc: 0.8703\n",
      "Epoch 791/1000\n",
      " - 1s - loss: 0.2298 - acc: 0.9221 - val_loss: 0.3990 - val_acc: 0.8707\n",
      "Epoch 792/1000\n",
      " - 1s - loss: 0.2301 - acc: 0.9220 - val_loss: 0.3981 - val_acc: 0.8703\n",
      "Epoch 793/1000\n",
      " - 1s - loss: 0.2295 - acc: 0.9223 - val_loss: 0.3991 - val_acc: 0.8701\n",
      "Epoch 794/1000\n",
      " - 2s - loss: 0.2296 - acc: 0.9228 - val_loss: 0.4000 - val_acc: 0.8689\n",
      "Epoch 795/1000\n",
      " - 1s - loss: 0.2298 - acc: 0.9220 - val_loss: 0.3996 - val_acc: 0.8699\n",
      "Epoch 796/1000\n",
      " - 1s - loss: 0.2293 - acc: 0.9219 - val_loss: 0.3982 - val_acc: 0.8699\n",
      "Epoch 797/1000\n",
      " - 2s - loss: 0.2297 - acc: 0.9218 - val_loss: 0.3993 - val_acc: 0.8685\n",
      "Epoch 798/1000\n",
      " - 2s - loss: 0.2295 - acc: 0.9222 - val_loss: 0.3997 - val_acc: 0.8697\n",
      "Epoch 799/1000\n",
      " - 2s - loss: 0.2295 - acc: 0.9219 - val_loss: 0.3969 - val_acc: 0.8700\n",
      "Epoch 800/1000\n",
      " - 2s - loss: 0.2292 - acc: 0.9224 - val_loss: 0.4017 - val_acc: 0.8690\n",
      "Epoch 801/1000\n",
      " - 3s - loss: 0.2293 - acc: 0.9226 - val_loss: 0.3994 - val_acc: 0.8688\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000800\n",
      "Epoch 802/1000\n",
      " - 2s - loss: 0.2287 - acc: 0.9222 - val_loss: 0.3988 - val_acc: 0.8688\n",
      "Epoch 803/1000\n",
      " - 2s - loss: 0.2291 - acc: 0.9218 - val_loss: 0.4033 - val_acc: 0.8688\n",
      "Epoch 804/1000\n",
      " - 2s - loss: 0.2291 - acc: 0.9227 - val_loss: 0.4003 - val_acc: 0.8689\n",
      "Epoch 805/1000\n",
      " - 2s - loss: 0.2287 - acc: 0.9226 - val_loss: 0.4064 - val_acc: 0.8681\n",
      "Epoch 806/1000\n",
      " - 1s - loss: 0.2288 - acc: 0.9227 - val_loss: 0.4008 - val_acc: 0.8706\n",
      "Epoch 807/1000\n",
      " - 2s - loss: 0.2287 - acc: 0.9231 - val_loss: 0.3992 - val_acc: 0.8702\n",
      "Epoch 808/1000\n",
      " - 2s - loss: 0.2285 - acc: 0.9223 - val_loss: 0.3982 - val_acc: 0.8699\n",
      "Epoch 809/1000\n",
      " - 2s - loss: 0.2284 - acc: 0.9230 - val_loss: 0.3995 - val_acc: 0.8686\n",
      "Epoch 810/1000\n",
      " - 1s - loss: 0.2281 - acc: 0.9225 - val_loss: 0.3988 - val_acc: 0.8698\n",
      "Epoch 811/1000\n",
      " - 2s - loss: 0.2279 - acc: 0.9230 - val_loss: 0.4035 - val_acc: 0.8697\n",
      "Epoch 812/1000\n",
      " - 1s - loss: 0.2285 - acc: 0.9225 - val_loss: 0.4014 - val_acc: 0.8689\n",
      "Epoch 813/1000\n",
      " - 1s - loss: 0.2278 - acc: 0.9229 - val_loss: 0.4118 - val_acc: 0.8667\n",
      "Epoch 814/1000\n",
      " - 2s - loss: 0.2280 - acc: 0.9229 - val_loss: 0.4020 - val_acc: 0.8691\n",
      "Epoch 815/1000\n",
      " - 1s - loss: 0.2282 - acc: 0.9229 - val_loss: 0.4022 - val_acc: 0.8690\n",
      "Epoch 816/1000\n",
      " - 2s - loss: 0.2281 - acc: 0.9223 - val_loss: 0.3999 - val_acc: 0.8706\n",
      "Epoch 817/1000\n",
      " - 1s - loss: 0.2281 - acc: 0.9226 - val_loss: 0.4000 - val_acc: 0.8698\n",
      "Epoch 818/1000\n",
      " - 1s - loss: 0.2276 - acc: 0.9228 - val_loss: 0.4003 - val_acc: 0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 819/1000\n",
      " - 2s - loss: 0.2278 - acc: 0.9232 - val_loss: 0.4003 - val_acc: 0.8682\n",
      "Epoch 820/1000\n",
      " - 2s - loss: 0.2274 - acc: 0.9231 - val_loss: 0.4011 - val_acc: 0.8683\n",
      "Epoch 821/1000\n",
      " - 2s - loss: 0.2273 - acc: 0.9230 - val_loss: 0.4006 - val_acc: 0.8685\n",
      "Epoch 822/1000\n",
      " - 2s - loss: 0.2276 - acc: 0.9227 - val_loss: 0.4067 - val_acc: 0.8672\n",
      "Epoch 823/1000\n",
      " - 2s - loss: 0.2275 - acc: 0.9235 - val_loss: 0.3999 - val_acc: 0.8699\n",
      "Epoch 824/1000\n",
      " - 2s - loss: 0.2272 - acc: 0.9231 - val_loss: 0.3997 - val_acc: 0.8702\n",
      "Epoch 825/1000\n",
      " - 2s - loss: 0.2271 - acc: 0.9233 - val_loss: 0.3998 - val_acc: 0.8699\n",
      "Epoch 826/1000\n",
      " - 1s - loss: 0.2273 - acc: 0.9232 - val_loss: 0.4009 - val_acc: 0.8687\n",
      "Epoch 827/1000\n",
      " - 1s - loss: 0.2271 - acc: 0.9230 - val_loss: 0.4042 - val_acc: 0.8690\n",
      "Epoch 828/1000\n",
      " - 2s - loss: 0.2266 - acc: 0.9233 - val_loss: 0.4018 - val_acc: 0.8705\n",
      "Epoch 829/1000\n",
      " - 2s - loss: 0.2266 - acc: 0.9234 - val_loss: 0.4055 - val_acc: 0.8670\n",
      "Epoch 830/1000\n",
      " - 2s - loss: 0.2268 - acc: 0.9236 - val_loss: 0.4012 - val_acc: 0.8678\n",
      "Epoch 831/1000\n",
      " - 2s - loss: 0.2266 - acc: 0.9236 - val_loss: 0.4031 - val_acc: 0.8689\n",
      "Epoch 832/1000\n",
      " - 2s - loss: 0.2270 - acc: 0.9227 - val_loss: 0.4019 - val_acc: 0.8672\n",
      "Epoch 833/1000\n",
      " - 2s - loss: 0.2265 - acc: 0.9227 - val_loss: 0.3998 - val_acc: 0.8680\n",
      "Epoch 834/1000\n",
      " - 1s - loss: 0.2261 - acc: 0.9237 - val_loss: 0.4067 - val_acc: 0.8673\n",
      "Epoch 835/1000\n",
      " - 2s - loss: 0.2262 - acc: 0.9242 - val_loss: 0.4060 - val_acc: 0.8687\n",
      "Epoch 836/1000\n",
      " - 2s - loss: 0.2261 - acc: 0.9237 - val_loss: 0.4014 - val_acc: 0.8701\n",
      "Epoch 837/1000\n",
      " - 2s - loss: 0.2260 - acc: 0.9236 - val_loss: 0.4018 - val_acc: 0.8698\n",
      "Epoch 838/1000\n",
      " - 2s - loss: 0.2262 - acc: 0.9242 - val_loss: 0.4010 - val_acc: 0.8704\n",
      "Epoch 839/1000\n",
      " - 1s - loss: 0.2258 - acc: 0.9238 - val_loss: 0.4042 - val_acc: 0.8700\n",
      "Epoch 840/1000\n",
      " - 1s - loss: 0.2260 - acc: 0.9240 - val_loss: 0.4030 - val_acc: 0.8689\n",
      "Epoch 841/1000\n",
      " - 2s - loss: 0.2260 - acc: 0.9232 - val_loss: 0.3998 - val_acc: 0.8701\n",
      "Epoch 842/1000\n",
      " - 1s - loss: 0.2259 - acc: 0.9240 - val_loss: 0.4035 - val_acc: 0.8686\n",
      "Epoch 843/1000\n",
      " - 2s - loss: 0.2261 - acc: 0.9236 - val_loss: 0.4012 - val_acc: 0.8684\n",
      "Epoch 844/1000\n",
      " - 2s - loss: 0.2258 - acc: 0.9238 - val_loss: 0.4003 - val_acc: 0.8685\n",
      "Epoch 845/1000\n",
      " - 2s - loss: 0.2257 - acc: 0.9236 - val_loss: 0.4027 - val_acc: 0.8680\n",
      "Epoch 846/1000\n",
      " - 1s - loss: 0.2257 - acc: 0.9236 - val_loss: 0.4022 - val_acc: 0.8688\n",
      "Epoch 847/1000\n",
      " - 1s - loss: 0.2252 - acc: 0.9239 - val_loss: 0.4029 - val_acc: 0.8695\n",
      "Epoch 848/1000\n",
      " - 2s - loss: 0.2250 - acc: 0.9238 - val_loss: 0.4070 - val_acc: 0.8671\n",
      "Epoch 849/1000\n",
      " - 1s - loss: 0.2253 - acc: 0.9238 - val_loss: 0.4040 - val_acc: 0.8689\n",
      "Epoch 850/1000\n",
      " - 2s - loss: 0.2252 - acc: 0.9237 - val_loss: 0.4032 - val_acc: 0.8694\n",
      "Epoch 851/1000\n",
      " - 2s - loss: 0.2250 - acc: 0.9239 - val_loss: 0.4028 - val_acc: 0.8694\n",
      "Epoch 852/1000\n",
      " - 2s - loss: 0.2248 - acc: 0.9244 - val_loss: 0.4022 - val_acc: 0.8682\n",
      "Epoch 853/1000\n",
      " - 2s - loss: 0.2248 - acc: 0.9251 - val_loss: 0.4028 - val_acc: 0.8710\n",
      "Epoch 854/1000\n",
      " - 1s - loss: 0.2252 - acc: 0.9237 - val_loss: 0.4027 - val_acc: 0.8676\n",
      "Epoch 855/1000\n",
      " - 1s - loss: 0.2249 - acc: 0.9233 - val_loss: 0.4089 - val_acc: 0.8677\n",
      "Epoch 856/1000\n",
      " - 1s - loss: 0.2249 - acc: 0.9242 - val_loss: 0.4025 - val_acc: 0.8698\n",
      "Epoch 857/1000\n",
      " - 1s - loss: 0.2243 - acc: 0.9244 - val_loss: 0.4032 - val_acc: 0.8694\n",
      "Epoch 858/1000\n",
      " - 2s - loss: 0.2247 - acc: 0.9241 - val_loss: 0.4044 - val_acc: 0.8689\n",
      "Epoch 859/1000\n",
      " - 2s - loss: 0.2244 - acc: 0.9240 - val_loss: 0.4029 - val_acc: 0.8693\n",
      "Epoch 860/1000\n",
      " - 2s - loss: 0.2241 - acc: 0.9247 - val_loss: 0.4077 - val_acc: 0.8687\n",
      "Epoch 861/1000\n",
      " - 2s - loss: 0.2242 - acc: 0.9241 - val_loss: 0.4021 - val_acc: 0.8694\n",
      "Epoch 862/1000\n",
      " - 2s - loss: 0.2243 - acc: 0.9243 - val_loss: 0.4012 - val_acc: 0.8684\n",
      "Epoch 863/1000\n",
      " - 2s - loss: 0.2243 - acc: 0.9241 - val_loss: 0.4035 - val_acc: 0.8697\n",
      "Epoch 864/1000\n",
      " - 1s - loss: 0.2235 - acc: 0.9245 - val_loss: 0.4014 - val_acc: 0.8705\n",
      "Epoch 865/1000\n",
      " - 1s - loss: 0.2237 - acc: 0.9242 - val_loss: 0.4046 - val_acc: 0.8686\n",
      "Epoch 866/1000\n",
      " - 2s - loss: 0.2240 - acc: 0.9240 - val_loss: 0.4022 - val_acc: 0.8677\n",
      "Epoch 867/1000\n",
      " - 2s - loss: 0.2239 - acc: 0.9248 - val_loss: 0.4039 - val_acc: 0.8704\n",
      "Epoch 868/1000\n",
      " - 2s - loss: 0.2235 - acc: 0.9250 - val_loss: 0.4022 - val_acc: 0.8700\n",
      "Epoch 869/1000\n",
      " - 1s - loss: 0.2234 - acc: 0.9248 - val_loss: 0.4031 - val_acc: 0.8695\n",
      "Epoch 870/1000\n",
      " - 2s - loss: 0.2240 - acc: 0.9248 - val_loss: 0.4046 - val_acc: 0.8689\n",
      "Epoch 871/1000\n",
      " - 2s - loss: 0.2232 - acc: 0.9247 - val_loss: 0.4057 - val_acc: 0.8691\n",
      "Epoch 872/1000\n",
      " - 1s - loss: 0.2235 - acc: 0.9241 - val_loss: 0.4013 - val_acc: 0.8697\n",
      "Epoch 873/1000\n",
      " - 1s - loss: 0.2236 - acc: 0.9242 - val_loss: 0.4030 - val_acc: 0.8690\n",
      "Epoch 874/1000\n",
      " - 2s - loss: 0.2233 - acc: 0.9241 - val_loss: 0.4072 - val_acc: 0.8687\n",
      "Epoch 875/1000\n",
      " - 1s - loss: 0.2230 - acc: 0.9244 - val_loss: 0.4007 - val_acc: 0.8689\n",
      "Epoch 876/1000\n",
      " - 1s - loss: 0.2235 - acc: 0.9248 - val_loss: 0.4032 - val_acc: 0.8697\n",
      "Epoch 877/1000\n",
      " - 1s - loss: 0.2229 - acc: 0.9255 - val_loss: 0.4027 - val_acc: 0.8696\n",
      "Epoch 878/1000\n",
      " - 1s - loss: 0.2228 - acc: 0.9244 - val_loss: 0.4018 - val_acc: 0.8702\n",
      "Epoch 879/1000\n",
      " - 1s - loss: 0.2228 - acc: 0.9247 - val_loss: 0.4050 - val_acc: 0.8690\n",
      "Epoch 880/1000\n",
      " - 1s - loss: 0.2228 - acc: 0.9247 - val_loss: 0.4039 - val_acc: 0.8695\n",
      "Epoch 881/1000\n",
      " - 2s - loss: 0.2231 - acc: 0.9243 - val_loss: 0.4067 - val_acc: 0.8682\n",
      "Epoch 882/1000\n",
      " - 2s - loss: 0.2223 - acc: 0.9259 - val_loss: 0.4042 - val_acc: 0.8685\n",
      "Epoch 883/1000\n",
      " - 2s - loss: 0.2219 - acc: 0.9248 - val_loss: 0.4038 - val_acc: 0.8697\n",
      "Epoch 884/1000\n",
      " - 2s - loss: 0.2227 - acc: 0.9244 - val_loss: 0.4087 - val_acc: 0.8684\n",
      "Epoch 885/1000\n",
      " - 2s - loss: 0.2227 - acc: 0.9247 - val_loss: 0.4082 - val_acc: 0.8697\n",
      "Epoch 886/1000\n",
      " - 2s - loss: 0.2226 - acc: 0.9248 - val_loss: 0.4081 - val_acc: 0.8674\n",
      "Epoch 887/1000\n",
      " - 2s - loss: 0.2222 - acc: 0.9247 - val_loss: 0.4075 - val_acc: 0.8684\n",
      "Epoch 888/1000\n",
      " - 2s - loss: 0.2225 - acc: 0.9250 - val_loss: 0.4096 - val_acc: 0.8679\n",
      "Epoch 889/1000\n",
      " - 2s - loss: 0.2220 - acc: 0.9251 - val_loss: 0.4050 - val_acc: 0.8688\n",
      "Epoch 890/1000\n",
      " - 1s - loss: 0.2222 - acc: 0.9250 - val_loss: 0.4055 - val_acc: 0.8669\n",
      "Epoch 891/1000\n",
      " - 1s - loss: 0.2222 - acc: 0.9251 - val_loss: 0.4046 - val_acc: 0.8692\n",
      "Epoch 892/1000\n",
      " - 2s - loss: 0.2221 - acc: 0.9246 - val_loss: 0.4048 - val_acc: 0.8693\n",
      "Epoch 893/1000\n",
      " - 2s - loss: 0.2221 - acc: 0.9260 - val_loss: 0.4067 - val_acc: 0.8697\n",
      "Epoch 894/1000\n",
      " - 1s - loss: 0.2220 - acc: 0.9256 - val_loss: 0.4044 - val_acc: 0.8686\n",
      "Epoch 895/1000\n",
      " - 1s - loss: 0.2215 - acc: 0.9252 - val_loss: 0.4065 - val_acc: 0.8694\n",
      "Epoch 896/1000\n",
      " - 1s - loss: 0.2217 - acc: 0.9246 - val_loss: 0.4043 - val_acc: 0.8697\n",
      "Epoch 897/1000\n",
      " - 1s - loss: 0.2212 - acc: 0.9254 - val_loss: 0.4066 - val_acc: 0.8683\n",
      "Epoch 898/1000\n",
      " - 1s - loss: 0.2214 - acc: 0.9253 - val_loss: 0.4043 - val_acc: 0.8689\n",
      "Epoch 899/1000\n",
      " - 1s - loss: 0.2218 - acc: 0.9254 - val_loss: 0.4061 - val_acc: 0.8696\n",
      "Epoch 900/1000\n",
      " - 1s - loss: 0.2210 - acc: 0.9254 - val_loss: 0.4048 - val_acc: 0.8694\n",
      "Epoch 901/1000\n",
      " - 3s - loss: 0.2216 - acc: 0.9253 - val_loss: 0.4048 - val_acc: 0.8705\n",
      "Saving rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000900\n",
      "Epoch 902/1000\n",
      " - 1s - loss: 0.2214 - acc: 0.9247 - val_loss: 0.4071 - val_acc: 0.8685\n",
      "Epoch 903/1000\n",
      " - 1s - loss: 0.2210 - acc: 0.9257 - val_loss: 0.4035 - val_acc: 0.8694\n",
      "Epoch 904/1000\n",
      " - 1s - loss: 0.2213 - acc: 0.9251 - val_loss: 0.4101 - val_acc: 0.8689\n",
      "Epoch 905/1000\n",
      " - 1s - loss: 0.2207 - acc: 0.9258 - val_loss: 0.4072 - val_acc: 0.8698\n",
      "Epoch 906/1000\n",
      " - 1s - loss: 0.2204 - acc: 0.9259 - val_loss: 0.4078 - val_acc: 0.8691\n",
      "Epoch 907/1000\n",
      " - 1s - loss: 0.2209 - acc: 0.9252 - val_loss: 0.4122 - val_acc: 0.8687\n",
      "Epoch 908/1000\n",
      " - 1s - loss: 0.2207 - acc: 0.9261 - val_loss: 0.4110 - val_acc: 0.8682\n",
      "Epoch 909/1000\n",
      " - 1s - loss: 0.2205 - acc: 0.9256 - val_loss: 0.4073 - val_acc: 0.8684\n",
      "Epoch 910/1000\n",
      " - 1s - loss: 0.2205 - acc: 0.9253 - val_loss: 0.4121 - val_acc: 0.8691\n",
      "Epoch 911/1000\n",
      " - 1s - loss: 0.2209 - acc: 0.9260 - val_loss: 0.4046 - val_acc: 0.8687\n",
      "Epoch 912/1000\n",
      " - 1s - loss: 0.2205 - acc: 0.9254 - val_loss: 0.4097 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/1000\n",
      " - 1s - loss: 0.2204 - acc: 0.9254 - val_loss: 0.4066 - val_acc: 0.8683\n",
      "Epoch 914/1000\n",
      " - 1s - loss: 0.2205 - acc: 0.9255 - val_loss: 0.4071 - val_acc: 0.8682\n",
      "Epoch 915/1000\n",
      " - 1s - loss: 0.2205 - acc: 0.9256 - val_loss: 0.4083 - val_acc: 0.8677\n",
      "Epoch 916/1000\n",
      " - 1s - loss: 0.2202 - acc: 0.9259 - val_loss: 0.4064 - val_acc: 0.8695\n",
      "Epoch 917/1000\n",
      " - 1s - loss: 0.2200 - acc: 0.9256 - val_loss: 0.4076 - val_acc: 0.8697\n",
      "Epoch 918/1000\n",
      " - 1s - loss: 0.2202 - acc: 0.9267 - val_loss: 0.4132 - val_acc: 0.8673\n",
      "Epoch 919/1000\n",
      " - 1s - loss: 0.2204 - acc: 0.9256 - val_loss: 0.4062 - val_acc: 0.8684\n",
      "Epoch 920/1000\n",
      " - 1s - loss: 0.2200 - acc: 0.9258 - val_loss: 0.4050 - val_acc: 0.8689\n",
      "Epoch 921/1000\n",
      " - 1s - loss: 0.2198 - acc: 0.9258 - val_loss: 0.4075 - val_acc: 0.8685\n",
      "Epoch 922/1000\n",
      " - 1s - loss: 0.2200 - acc: 0.9256 - val_loss: 0.4065 - val_acc: 0.8680\n",
      "Epoch 923/1000\n",
      " - 1s - loss: 0.2196 - acc: 0.9261 - val_loss: 0.4054 - val_acc: 0.8681\n",
      "Epoch 924/1000\n",
      " - 1s - loss: 0.2192 - acc: 0.9261 - val_loss: 0.4074 - val_acc: 0.8696\n",
      "Epoch 925/1000\n",
      " - 1s - loss: 0.2193 - acc: 0.9263 - val_loss: 0.4084 - val_acc: 0.8687\n",
      "Epoch 926/1000\n",
      " - 1s - loss: 0.2195 - acc: 0.9258 - val_loss: 0.4064 - val_acc: 0.8680\n",
      "Epoch 927/1000\n",
      " - 1s - loss: 0.2199 - acc: 0.9263 - val_loss: 0.4054 - val_acc: 0.8696\n",
      "Epoch 928/1000\n",
      " - 1s - loss: 0.2195 - acc: 0.9260 - val_loss: 0.4103 - val_acc: 0.8687\n",
      "Epoch 929/1000\n",
      " - 1s - loss: 0.2195 - acc: 0.9259 - val_loss: 0.4086 - val_acc: 0.8677\n",
      "Epoch 930/1000\n",
      " - 1s - loss: 0.2191 - acc: 0.9265 - val_loss: 0.4090 - val_acc: 0.8695\n",
      "Epoch 931/1000\n",
      " - 1s - loss: 0.2186 - acc: 0.9260 - val_loss: 0.4046 - val_acc: 0.8697\n",
      "Epoch 932/1000\n",
      " - 1s - loss: 0.2191 - acc: 0.9264 - val_loss: 0.4081 - val_acc: 0.8676\n",
      "Epoch 933/1000\n",
      " - 1s - loss: 0.2186 - acc: 0.9263 - val_loss: 0.4074 - val_acc: 0.8694\n",
      "Epoch 934/1000\n",
      " - 1s - loss: 0.2189 - acc: 0.9260 - val_loss: 0.4076 - val_acc: 0.8677\n",
      "Epoch 935/1000\n",
      " - 1s - loss: 0.2188 - acc: 0.9263 - val_loss: 0.4103 - val_acc: 0.8681\n",
      "Epoch 936/1000\n",
      " - 1s - loss: 0.2191 - acc: 0.9261 - val_loss: 0.4062 - val_acc: 0.8686\n",
      "Epoch 937/1000\n",
      " - 1s - loss: 0.2187 - acc: 0.9266 - val_loss: 0.4087 - val_acc: 0.8691\n",
      "Epoch 938/1000\n",
      " - 1s - loss: 0.2187 - acc: 0.9264 - val_loss: 0.4066 - val_acc: 0.8683\n",
      "Epoch 939/1000\n",
      " - 1s - loss: 0.2184 - acc: 0.9266 - val_loss: 0.4081 - val_acc: 0.8678\n",
      "Epoch 940/1000\n",
      " - 1s - loss: 0.2188 - acc: 0.9259 - val_loss: 0.4082 - val_acc: 0.8690\n",
      "Epoch 941/1000\n",
      " - 1s - loss: 0.2181 - acc: 0.9263 - val_loss: 0.4091 - val_acc: 0.8697\n",
      "Epoch 942/1000\n",
      " - 1s - loss: 0.2184 - acc: 0.9265 - val_loss: 0.4084 - val_acc: 0.8688\n",
      "Epoch 943/1000\n",
      " - 2s - loss: 0.2182 - acc: 0.9268 - val_loss: 0.4068 - val_acc: 0.8697\n",
      "Epoch 944/1000\n",
      " - 1s - loss: 0.2184 - acc: 0.9264 - val_loss: 0.4066 - val_acc: 0.8696\n",
      "Epoch 945/1000\n",
      " - 1s - loss: 0.2182 - acc: 0.9264 - val_loss: 0.4094 - val_acc: 0.8687\n",
      "Epoch 946/1000\n",
      " - 1s - loss: 0.2180 - acc: 0.9265 - val_loss: 0.4048 - val_acc: 0.8690\n",
      "Epoch 947/1000\n",
      " - 1s - loss: 0.2177 - acc: 0.9266 - val_loss: 0.4093 - val_acc: 0.8683\n",
      "Epoch 948/1000\n",
      " - 1s - loss: 0.2176 - acc: 0.9268 - val_loss: 0.4072 - val_acc: 0.8699\n",
      "Epoch 949/1000\n",
      " - 1s - loss: 0.2179 - acc: 0.9269 - val_loss: 0.4103 - val_acc: 0.8679\n",
      "Epoch 950/1000\n",
      " - 1s - loss: 0.2180 - acc: 0.9268 - val_loss: 0.4067 - val_acc: 0.8689\n",
      "Epoch 951/1000\n",
      " - 1s - loss: 0.2179 - acc: 0.9263 - val_loss: 0.4074 - val_acc: 0.8684\n",
      "Epoch 952/1000\n",
      " - 1s - loss: 0.2177 - acc: 0.9262 - val_loss: 0.4086 - val_acc: 0.8695\n",
      "Epoch 953/1000\n",
      " - 1s - loss: 0.2177 - acc: 0.9264 - val_loss: 0.4090 - val_acc: 0.8687\n",
      "Epoch 954/1000\n",
      " - 1s - loss: 0.2178 - acc: 0.9264 - val_loss: 0.4078 - val_acc: 0.8687\n",
      "Epoch 955/1000\n",
      " - 1s - loss: 0.2179 - acc: 0.9267 - val_loss: 0.4102 - val_acc: 0.8692\n",
      "Epoch 956/1000\n",
      " - 2s - loss: 0.2173 - acc: 0.9273 - val_loss: 0.4117 - val_acc: 0.8683\n",
      "Epoch 957/1000\n",
      " - 2s - loss: 0.2174 - acc: 0.9268 - val_loss: 0.4077 - val_acc: 0.8692\n",
      "Epoch 958/1000\n",
      " - 1s - loss: 0.2172 - acc: 0.9273 - val_loss: 0.4079 - val_acc: 0.8684\n",
      "Epoch 959/1000\n",
      " - 1s - loss: 0.2172 - acc: 0.9271 - val_loss: 0.4088 - val_acc: 0.8688\n",
      "Epoch 960/1000\n",
      " - 1s - loss: 0.2172 - acc: 0.9267 - val_loss: 0.4098 - val_acc: 0.8684\n",
      "Epoch 961/1000\n",
      " - 1s - loss: 0.2171 - acc: 0.9268 - val_loss: 0.4080 - val_acc: 0.8691\n",
      "Epoch 962/1000\n",
      " - 1s - loss: 0.2167 - acc: 0.9268 - val_loss: 0.4098 - val_acc: 0.8670\n",
      "Epoch 963/1000\n",
      " - 1s - loss: 0.2173 - acc: 0.9270 - val_loss: 0.4075 - val_acc: 0.8693\n",
      "Epoch 964/1000\n",
      " - 1s - loss: 0.2170 - acc: 0.9271 - val_loss: 0.4086 - val_acc: 0.8692\n",
      "Epoch 965/1000\n",
      " - 1s - loss: 0.2171 - acc: 0.9267 - val_loss: 0.4118 - val_acc: 0.8687\n",
      "Epoch 966/1000\n",
      " - 2s - loss: 0.2165 - acc: 0.9271 - val_loss: 0.4082 - val_acc: 0.8696\n",
      "Epoch 967/1000\n",
      " - 2s - loss: 0.2165 - acc: 0.9273 - val_loss: 0.4090 - val_acc: 0.8688\n",
      "Epoch 968/1000\n",
      " - 1s - loss: 0.2170 - acc: 0.9271 - val_loss: 0.4072 - val_acc: 0.8692\n",
      "Epoch 969/1000\n",
      " - 2s - loss: 0.2163 - acc: 0.9273 - val_loss: 0.4150 - val_acc: 0.8672\n",
      "Epoch 970/1000\n",
      " - 1s - loss: 0.2168 - acc: 0.9269 - val_loss: 0.4082 - val_acc: 0.8689\n",
      "Epoch 971/1000\n",
      " - 1s - loss: 0.2164 - acc: 0.9273 - val_loss: 0.4104 - val_acc: 0.8684\n",
      "Epoch 972/1000\n",
      " - 1s - loss: 0.2161 - acc: 0.9270 - val_loss: 0.4116 - val_acc: 0.8673\n",
      "Epoch 973/1000\n",
      " - 1s - loss: 0.2165 - acc: 0.9270 - val_loss: 0.4078 - val_acc: 0.8699\n",
      "Epoch 974/1000\n",
      " - 1s - loss: 0.2161 - acc: 0.9271 - val_loss: 0.4109 - val_acc: 0.8693\n",
      "Epoch 975/1000\n",
      " - 1s - loss: 0.2158 - acc: 0.9274 - val_loss: 0.4128 - val_acc: 0.8687\n",
      "Epoch 976/1000\n",
      " - 1s - loss: 0.2163 - acc: 0.9273 - val_loss: 0.4158 - val_acc: 0.8671\n",
      "Epoch 977/1000\n",
      " - 1s - loss: 0.2160 - acc: 0.9273 - val_loss: 0.4107 - val_acc: 0.8697\n",
      "Epoch 978/1000\n",
      " - 1s - loss: 0.2157 - acc: 0.9269 - val_loss: 0.4087 - val_acc: 0.8692\n",
      "Epoch 979/1000\n",
      " - 1s - loss: 0.2157 - acc: 0.9275 - val_loss: 0.4106 - val_acc: 0.8692\n",
      "Epoch 980/1000\n",
      " - 1s - loss: 0.2159 - acc: 0.9273 - val_loss: 0.4167 - val_acc: 0.8684\n",
      "Epoch 981/1000\n",
      " - 1s - loss: 0.2166 - acc: 0.9271 - val_loss: 0.4093 - val_acc: 0.8687\n",
      "Epoch 982/1000\n",
      " - 1s - loss: 0.2157 - acc: 0.9274 - val_loss: 0.4146 - val_acc: 0.8679\n",
      "Epoch 983/1000\n",
      " - 1s - loss: 0.2153 - acc: 0.9279 - val_loss: 0.4100 - val_acc: 0.8689\n",
      "Epoch 984/1000\n",
      " - 1s - loss: 0.2158 - acc: 0.9271 - val_loss: 0.4092 - val_acc: 0.8681\n",
      "Epoch 985/1000\n",
      " - 2s - loss: 0.2155 - acc: 0.9275 - val_loss: 0.4128 - val_acc: 0.8684\n",
      "Epoch 986/1000\n",
      " - 2s - loss: 0.2154 - acc: 0.9277 - val_loss: 0.4191 - val_acc: 0.8660\n",
      "Epoch 987/1000\n",
      " - 2s - loss: 0.2152 - acc: 0.9279 - val_loss: 0.4092 - val_acc: 0.8683\n",
      "Epoch 988/1000\n",
      " - 2s - loss: 0.2154 - acc: 0.9281 - val_loss: 0.4121 - val_acc: 0.8687\n",
      "Epoch 989/1000\n",
      " - 1s - loss: 0.2150 - acc: 0.9276 - val_loss: 0.4113 - val_acc: 0.8684\n",
      "Epoch 990/1000\n",
      " - 1s - loss: 0.2154 - acc: 0.9274 - val_loss: 0.4095 - val_acc: 0.8689\n",
      "Epoch 991/1000\n",
      " - 2s - loss: 0.2149 - acc: 0.9277 - val_loss: 0.4123 - val_acc: 0.8691\n",
      "Epoch 992/1000\n",
      " - 1s - loss: 0.2150 - acc: 0.9270 - val_loss: 0.4166 - val_acc: 0.8678\n",
      "Epoch 993/1000\n",
      " - 1s - loss: 0.2147 - acc: 0.9286 - val_loss: 0.4104 - val_acc: 0.8682\n",
      "Epoch 994/1000\n",
      " - 2s - loss: 0.2149 - acc: 0.9283 - val_loss: 0.4109 - val_acc: 0.8687\n",
      "Epoch 995/1000\n",
      " - 1s - loss: 0.2144 - acc: 0.9282 - val_loss: 0.4095 - val_acc: 0.8685\n",
      "Epoch 996/1000\n",
      " - 2s - loss: 0.2144 - acc: 0.9279 - val_loss: 0.4144 - val_acc: 0.8675\n",
      "Epoch 997/1000\n",
      " - 2s - loss: 0.2142 - acc: 0.9281 - val_loss: 0.4125 - val_acc: 0.8667\n",
      "Epoch 998/1000\n",
      " - 1s - loss: 0.2149 - acc: 0.9277 - val_loss: 0.4099 - val_acc: 0.8696\n",
      "Epoch 999/1000\n",
      " - 1s - loss: 0.2145 - acc: 0.9281 - val_loss: 0.4161 - val_acc: 0.8675\n",
      "Epoch 1000/1000\n",
      " - 1s - loss: 0.2148 - acc: 0.9270 - val_loss: 0.4134 - val_acc: 0.8676\n"
     ]
    }
   ],
   "source": [
    "def do_report(epoch):\n",
    "    # Only log activity for some epochs.  Mainly this is to make things run faster.\n",
    "    if epoch < 20:       # Log for all first 20 epochs\n",
    "        return True\n",
    "    elif epoch < 100:    # Then for every 5th epoch\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 200:    # Then every 10th\n",
    "        return (epoch % 10 == 0)\n",
    "    else:                # Then every 100th\n",
    "        return (epoch % 100 == 0)\n",
    "    \n",
    "reporter = LoggingReporter(cfg=cfg, trn=trn, tst=tst, do_save_func=do_report)\n",
    "\n",
    "r = model.fit(x=trn.X, y=trn.Y, \n",
    "              verbose    = 2, \n",
    "              batch_size = cfg['SGD_BATCHSIZE'],\n",
    "              epochs     = cfg['NUM_EPOCHS'],\n",
    "              validation_data=(tst.X, tst.Y),\n",
    "              callbacks  = [reporter,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "# import kde\n",
    "# import simplebinmi\n",
    "\n",
    "import utils\n",
    "trn, tst = get_mnist()\n",
    "\n",
    "# Which measure to plot\n",
    "infoplane_measure = 'upper'\n",
    "#infoplane_measure = 'bin'\n",
    "\n",
    "DO_SAVE        = False    # Whether to save plots or just show them\n",
    "DO_LOWER       = True    # (infoplane_measure == 'lower')   # Whether to compute lower bounds also\n",
    "DO_BINNED      = True    #(infoplane_measure == 'bin')     # Whether to compute MI estimates based on binning\n",
    "\n",
    "MAX_EPOCHS = 10000      # Max number of epoch for which to compute mutual information measure\n",
    "# MAX_EPOCHS = 1000\n",
    "COLORBAR_MAX_EPOCHS = 10000\n",
    "\n",
    "# Directories from which to load saved layer activity\n",
    "#ARCH = '1024-20-20-20'\n",
    "#ARCH = '20-20-20-20-20-20'\n",
    "#ARCH = '32-28-24-20-16-12'\n",
    "ARCH = '32-28-24-20-16-12-8-8'\n",
    "DIR_TEMPLATE = '%%s_%s'%ARCH\n",
    "\n",
    "# Functions to return upper and lower bounds on entropy of layer activity\n",
    "noise_variance = 1e-1                    # Added Gaussian noise variance\n",
    "Klayer_activity = K.placeholder(ndim=2)  # Keras placeholder \n",
    "entropy_func_upper = K.function([Klayer_activity,], [entropy_estimator_kl(Klayer_activity, noise_variance),])\n",
    "entropy_func_lower = K.function([Klayer_activity,], [entropy_estimator_bd(Klayer_activity, noise_variance),])\n",
    "\n",
    "\n",
    "# nats to bits conversion factor\n",
    "nats2bits = 1.0/np.log(2) \n",
    "\n",
    "\n",
    "# Save indexes of tests data for each of the output classes\n",
    "saved_labelixs = {}\n",
    "for i in range(10):\n",
    "    saved_labelixs[i] = tst.y == i\n",
    "\n",
    "labelprobs = np.mean(tst.Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_LAYERS    = None     # Which layers to plot.  If None, all saved layers are plotted \n",
    "\n",
    "# Data structure used to store results\n",
    "measures = OrderedDict()\n",
    "measures['relu'] = {}\n",
    "measures['tanh'] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory rawdata/relu_32-28-24-20-16-12-8-8 not found\n",
      "*** Doing rawdata/tanh_32-28-24-20-16-12-8-8 ***\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000000\n",
      "- Layer 0 upper: MI(X;M)=10.679, MI(Y;M)=2.996 | lower: MI(X;M)=6.054, MI(Y;M)=2.453 | bin: MI(X;M)=12.811, MI(Y;M)=3.263\n",
      "- Layer 1 upper: MI(X;M)=9.036, MI(Y;M)=2.855 | lower: MI(X;M)=5.095, MI(Y;M)=2.456 | bin: MI(X;M)=12.718, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.487, MI(Y;M)=2.838 | lower: MI(X;M)=5.043, MI(Y;M)=2.546 | bin: MI(X;M)=12.357, MI(Y;M)=3.237\n",
      "- Layer 3 upper: MI(X;M)=8.121, MI(Y;M)=2.835 | lower: MI(X;M)=5.109, MI(Y;M)=2.623 | bin: MI(X;M)=11.942, MI(Y;M)=3.220\n",
      "- Layer 4 upper: MI(X;M)=8.026, MI(Y;M)=2.844 | lower: MI(X;M)=5.342, MI(Y;M)=2.664 | bin: MI(X;M)=10.948, MI(Y;M)=3.136\n",
      "- Layer 5 upper: MI(X;M)=7.592, MI(Y;M)=2.830 | lower: MI(X;M)=5.376, MI(Y;M)=2.677 | bin: MI(X;M)=9.793, MI(Y;M)=3.046\n",
      "- Layer 6 upper: MI(X;M)=3.511, MI(Y;M)=2.536 | lower: MI(X;M)=1.909, MI(Y;M)=1.536 | bin: MI(X;M)=3.448, MI(Y;M)=2.419\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000001\n",
      "- Layer 0 upper: MI(X;M)=10.668, MI(Y;M)=2.995 | lower: MI(X;M)=6.046, MI(Y;M)=2.451 | bin: MI(X;M)=12.809, MI(Y;M)=3.259\n",
      "- Layer 1 upper: MI(X;M)=9.029, MI(Y;M)=2.855 | lower: MI(X;M)=5.089, MI(Y;M)=2.455 | bin: MI(X;M)=12.716, MI(Y;M)=3.264\n",
      "- Layer 2 upper: MI(X;M)=8.482, MI(Y;M)=2.839 | lower: MI(X;M)=5.039, MI(Y;M)=2.545 | bin: MI(X;M)=12.368, MI(Y;M)=3.237\n",
      "- Layer 3 upper: MI(X;M)=8.125, MI(Y;M)=2.838 | lower: MI(X;M)=5.110, MI(Y;M)=2.623 | bin: MI(X;M)=11.933, MI(Y;M)=3.225\n",
      "- Layer 4 upper: MI(X;M)=8.040, MI(Y;M)=2.849 | lower: MI(X;M)=5.350, MI(Y;M)=2.665 | bin: MI(X;M)=10.949, MI(Y;M)=3.136\n",
      "- Layer 5 upper: MI(X;M)=7.600, MI(Y;M)=2.835 | lower: MI(X;M)=5.381, MI(Y;M)=2.679 | bin: MI(X;M)=9.765, MI(Y;M)=3.052\n",
      "- Layer 6 upper: MI(X;M)=3.512, MI(Y;M)=2.536 | lower: MI(X;M)=1.914, MI(Y;M)=1.541 | bin: MI(X;M)=3.445, MI(Y;M)=2.423\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000002\n",
      "- Layer 0 upper: MI(X;M)=10.695, MI(Y;M)=2.999 | lower: MI(X;M)=6.066, MI(Y;M)=2.454 | bin: MI(X;M)=12.828, MI(Y;M)=3.263\n",
      "- Layer 1 upper: MI(X;M)=9.044, MI(Y;M)=2.856 | lower: MI(X;M)=5.099, MI(Y;M)=2.457 | bin: MI(X;M)=12.720, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.490, MI(Y;M)=2.839 | lower: MI(X;M)=5.044, MI(Y;M)=2.547 | bin: MI(X;M)=12.368, MI(Y;M)=3.237\n",
      "- Layer 3 upper: MI(X;M)=8.122, MI(Y;M)=2.836 | lower: MI(X;M)=5.107, MI(Y;M)=2.624 | bin: MI(X;M)=11.939, MI(Y;M)=3.218\n",
      "- Layer 4 upper: MI(X;M)=8.025, MI(Y;M)=2.845 | lower: MI(X;M)=5.339, MI(Y;M)=2.666 | bin: MI(X;M)=10.967, MI(Y;M)=3.133\n",
      "- Layer 5 upper: MI(X;M)=7.594, MI(Y;M)=2.831 | lower: MI(X;M)=5.375, MI(Y;M)=2.680 | bin: MI(X;M)=9.812, MI(Y;M)=3.046\n",
      "- Layer 6 upper: MI(X;M)=3.514, MI(Y;M)=2.539 | lower: MI(X;M)=1.910, MI(Y;M)=1.538 | bin: MI(X;M)=3.451, MI(Y;M)=2.425\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000003\n",
      "- Layer 0 upper: MI(X;M)=10.691, MI(Y;M)=2.998 | lower: MI(X;M)=6.062, MI(Y;M)=2.453 | bin: MI(X;M)=12.822, MI(Y;M)=3.266\n",
      "- Layer 1 upper: MI(X;M)=9.045, MI(Y;M)=2.856 | lower: MI(X;M)=5.097, MI(Y;M)=2.456 | bin: MI(X;M)=12.728, MI(Y;M)=3.264\n",
      "- Layer 2 upper: MI(X;M)=8.485, MI(Y;M)=2.838 | lower: MI(X;M)=5.038, MI(Y;M)=2.545 | bin: MI(X;M)=12.369, MI(Y;M)=3.237\n",
      "- Layer 3 upper: MI(X;M)=8.119, MI(Y;M)=2.836 | lower: MI(X;M)=5.103, MI(Y;M)=2.623 | bin: MI(X;M)=11.946, MI(Y;M)=3.221\n",
      "- Layer 4 upper: MI(X;M)=8.025, MI(Y;M)=2.845 | lower: MI(X;M)=5.335, MI(Y;M)=2.666 | bin: MI(X;M)=10.961, MI(Y;M)=3.133\n",
      "- Layer 5 upper: MI(X;M)=7.592, MI(Y;M)=2.832 | lower: MI(X;M)=5.369, MI(Y;M)=2.680 | bin: MI(X;M)=9.809, MI(Y;M)=3.047\n",
      "- Layer 6 upper: MI(X;M)=3.513, MI(Y;M)=2.538 | lower: MI(X;M)=1.908, MI(Y;M)=1.537 | bin: MI(X;M)=3.451, MI(Y;M)=2.425\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000004\n",
      "- Layer 0 upper: MI(X;M)=10.709, MI(Y;M)=3.002 | lower: MI(X;M)=6.079, MI(Y;M)=2.456 | bin: MI(X;M)=12.827, MI(Y;M)=3.267\n",
      "- Layer 1 upper: MI(X;M)=9.059, MI(Y;M)=2.859 | lower: MI(X;M)=5.108, MI(Y;M)=2.459 | bin: MI(X;M)=12.718, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.505, MI(Y;M)=2.842 | lower: MI(X;M)=5.052, MI(Y;M)=2.549 | bin: MI(X;M)=12.377, MI(Y;M)=3.238\n",
      "- Layer 3 upper: MI(X;M)=8.143, MI(Y;M)=2.840 | lower: MI(X;M)=5.117, MI(Y;M)=2.626 | bin: MI(X;M)=11.936, MI(Y;M)=3.221\n",
      "- Layer 4 upper: MI(X;M)=8.053, MI(Y;M)=2.850 | lower: MI(X;M)=5.353, MI(Y;M)=2.669 | bin: MI(X;M)=10.988, MI(Y;M)=3.138\n",
      "- Layer 5 upper: MI(X;M)=7.621, MI(Y;M)=2.837 | lower: MI(X;M)=5.391, MI(Y;M)=2.684 | bin: MI(X;M)=9.819, MI(Y;M)=3.057\n",
      "- Layer 6 upper: MI(X;M)=3.517, MI(Y;M)=2.542 | lower: MI(X;M)=1.916, MI(Y;M)=1.544 | bin: MI(X;M)=3.450, MI(Y;M)=2.429\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000005\n",
      "- Layer 0 upper: MI(X;M)=10.704, MI(Y;M)=3.001 | lower: MI(X;M)=6.075, MI(Y;M)=2.455 | bin: MI(X;M)=12.825, MI(Y;M)=3.264\n",
      "- Layer 1 upper: MI(X;M)=9.054, MI(Y;M)=2.859 | lower: MI(X;M)=5.102, MI(Y;M)=2.457 | bin: MI(X;M)=12.723, MI(Y;M)=3.262\n",
      "- Layer 2 upper: MI(X;M)=8.499, MI(Y;M)=2.842 | lower: MI(X;M)=5.046, MI(Y;M)=2.548 | bin: MI(X;M)=12.376, MI(Y;M)=3.239\n",
      "- Layer 3 upper: MI(X;M)=8.139, MI(Y;M)=2.840 | lower: MI(X;M)=5.112, MI(Y;M)=2.626 | bin: MI(X;M)=11.945, MI(Y;M)=3.222\n",
      "- Layer 4 upper: MI(X;M)=8.048, MI(Y;M)=2.850 | lower: MI(X;M)=5.348, MI(Y;M)=2.670 | bin: MI(X;M)=10.979, MI(Y;M)=3.134\n",
      "- Layer 5 upper: MI(X;M)=7.614, MI(Y;M)=2.836 | lower: MI(X;M)=5.385, MI(Y;M)=2.684 | bin: MI(X;M)=9.821, MI(Y;M)=3.050\n",
      "- Layer 6 upper: MI(X;M)=3.518, MI(Y;M)=2.543 | lower: MI(X;M)=1.917, MI(Y;M)=1.544 | bin: MI(X;M)=3.451, MI(Y;M)=2.431\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000006\n",
      "- Layer 0 upper: MI(X;M)=10.702, MI(Y;M)=3.001 | lower: MI(X;M)=6.076, MI(Y;M)=2.455 | bin: MI(X;M)=12.821, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.051, MI(Y;M)=2.859 | lower: MI(X;M)=5.102, MI(Y;M)=2.457 | bin: MI(X;M)=12.730, MI(Y;M)=3.267\n",
      "- Layer 2 upper: MI(X;M)=8.493, MI(Y;M)=2.842 | lower: MI(X;M)=5.043, MI(Y;M)=2.548 | bin: MI(X;M)=12.387, MI(Y;M)=3.237\n",
      "- Layer 3 upper: MI(X;M)=8.138, MI(Y;M)=2.841 | lower: MI(X;M)=5.113, MI(Y;M)=2.627 | bin: MI(X;M)=11.945, MI(Y;M)=3.220\n",
      "- Layer 4 upper: MI(X;M)=8.043, MI(Y;M)=2.851 | lower: MI(X;M)=5.346, MI(Y;M)=2.670 | bin: MI(X;M)=10.966, MI(Y;M)=3.137\n",
      "- Layer 5 upper: MI(X;M)=7.594, MI(Y;M)=2.836 | lower: MI(X;M)=5.375, MI(Y;M)=2.683 | bin: MI(X;M)=9.788, MI(Y;M)=3.047\n",
      "- Layer 6 upper: MI(X;M)=3.517, MI(Y;M)=2.544 | lower: MI(X;M)=1.925, MI(Y;M)=1.552 | bin: MI(X;M)=3.446, MI(Y;M)=2.429\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000007\n",
      "- Layer 0 upper: MI(X;M)=10.724, MI(Y;M)=3.004 | lower: MI(X;M)=6.090, MI(Y;M)=2.457 | bin: MI(X;M)=12.838, MI(Y;M)=3.268\n",
      "- Layer 1 upper: MI(X;M)=9.067, MI(Y;M)=2.860 | lower: MI(X;M)=5.109, MI(Y;M)=2.458 | bin: MI(X;M)=12.726, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.504, MI(Y;M)=2.842 | lower: MI(X;M)=5.046, MI(Y;M)=2.548 | bin: MI(X;M)=12.377, MI(Y;M)=3.238\n",
      "- Layer 3 upper: MI(X;M)=8.139, MI(Y;M)=2.839 | lower: MI(X;M)=5.107, MI(Y;M)=2.627 | bin: MI(X;M)=11.960, MI(Y;M)=3.225\n",
      "- Layer 4 upper: MI(X;M)=8.042, MI(Y;M)=2.848 | lower: MI(X;M)=5.339, MI(Y;M)=2.671 | bin: MI(X;M)=10.990, MI(Y;M)=3.132\n",
      "- Layer 5 upper: MI(X;M)=7.617, MI(Y;M)=2.835 | lower: MI(X;M)=5.381, MI(Y;M)=2.686 | bin: MI(X;M)=9.852, MI(Y;M)=3.050\n",
      "- Layer 6 upper: MI(X;M)=3.518, MI(Y;M)=2.544 | lower: MI(X;M)=1.913, MI(Y;M)=1.541 | bin: MI(X;M)=3.453, MI(Y;M)=2.429\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000008\n",
      "- Layer 0 upper: MI(X;M)=10.723, MI(Y;M)=3.004 | lower: MI(X;M)=6.093, MI(Y;M)=2.457 | bin: MI(X;M)=12.836, MI(Y;M)=3.268\n",
      "- Layer 1 upper: MI(X;M)=9.069, MI(Y;M)=2.861 | lower: MI(X;M)=5.112, MI(Y;M)=2.459 | bin: MI(X;M)=12.734, MI(Y;M)=3.266\n",
      "- Layer 2 upper: MI(X;M)=8.505, MI(Y;M)=2.843 | lower: MI(X;M)=5.048, MI(Y;M)=2.549 | bin: MI(X;M)=12.384, MI(Y;M)=3.238\n",
      "- Layer 3 upper: MI(X;M)=8.143, MI(Y;M)=2.841 | lower: MI(X;M)=5.112, MI(Y;M)=2.629 | bin: MI(X;M)=11.963, MI(Y;M)=3.224\n",
      "- Layer 4 upper: MI(X;M)=8.042, MI(Y;M)=2.850 | lower: MI(X;M)=5.340, MI(Y;M)=2.672 | bin: MI(X;M)=10.987, MI(Y;M)=3.132\n",
      "- Layer 5 upper: MI(X;M)=7.604, MI(Y;M)=2.836 | lower: MI(X;M)=5.375, MI(Y;M)=2.686 | bin: MI(X;M)=9.822, MI(Y;M)=3.046\n",
      "- Layer 6 upper: MI(X;M)=3.519, MI(Y;M)=2.546 | lower: MI(X;M)=1.921, MI(Y;M)=1.548 | bin: MI(X;M)=3.452, MI(Y;M)=2.433\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer 0 upper: MI(X;M)=10.723, MI(Y;M)=3.004 | lower: MI(X;M)=6.092, MI(Y;M)=2.455 | bin: MI(X;M)=12.837, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.075, MI(Y;M)=2.862 | lower: MI(X;M)=5.112, MI(Y;M)=2.457 | bin: MI(X;M)=12.743, MI(Y;M)=3.266\n",
      "- Layer 2 upper: MI(X;M)=8.514, MI(Y;M)=2.844 | lower: MI(X;M)=5.050, MI(Y;M)=2.548 | bin: MI(X;M)=12.378, MI(Y;M)=3.238\n",
      "- Layer 3 upper: MI(X;M)=8.155, MI(Y;M)=2.842 | lower: MI(X;M)=5.116, MI(Y;M)=2.629 | bin: MI(X;M)=11.999, MI(Y;M)=3.224\n",
      "- Layer 4 upper: MI(X;M)=8.066, MI(Y;M)=2.853 | lower: MI(X;M)=5.353, MI(Y;M)=2.673 | bin: MI(X;M)=10.983, MI(Y;M)=3.136\n",
      "- Layer 5 upper: MI(X;M)=7.633, MI(Y;M)=2.840 | lower: MI(X;M)=5.392, MI(Y;M)=2.689 | bin: MI(X;M)=9.841, MI(Y;M)=3.050\n",
      "- Layer 6 upper: MI(X;M)=3.521, MI(Y;M)=2.545 | lower: MI(X;M)=1.921, MI(Y;M)=1.547 | bin: MI(X;M)=3.452, MI(Y;M)=2.433\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000010\n",
      "- Layer 0 upper: MI(X;M)=10.734, MI(Y;M)=3.007 | lower: MI(X;M)=6.104, MI(Y;M)=2.459 | bin: MI(X;M)=12.830, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.075, MI(Y;M)=2.863 | lower: MI(X;M)=5.116, MI(Y;M)=2.459 | bin: MI(X;M)=12.731, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.510, MI(Y;M)=2.845 | lower: MI(X;M)=5.050, MI(Y;M)=2.550 | bin: MI(X;M)=12.387, MI(Y;M)=3.244\n",
      "- Layer 3 upper: MI(X;M)=8.154, MI(Y;M)=2.844 | lower: MI(X;M)=5.116, MI(Y;M)=2.631 | bin: MI(X;M)=11.950, MI(Y;M)=3.222\n",
      "- Layer 4 upper: MI(X;M)=8.059, MI(Y;M)=2.854 | lower: MI(X;M)=5.349, MI(Y;M)=2.675 | bin: MI(X;M)=10.996, MI(Y;M)=3.138\n",
      "- Layer 5 upper: MI(X;M)=7.621, MI(Y;M)=2.841 | lower: MI(X;M)=5.385, MI(Y;M)=2.690 | bin: MI(X;M)=9.836, MI(Y;M)=3.054\n",
      "- Layer 6 upper: MI(X;M)=3.523, MI(Y;M)=2.550 | lower: MI(X;M)=1.928, MI(Y;M)=1.555 | bin: MI(X;M)=3.451, MI(Y;M)=2.439\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000011\n",
      "- Layer 0 upper: MI(X;M)=10.740, MI(Y;M)=3.008 | lower: MI(X;M)=6.107, MI(Y;M)=2.458 | bin: MI(X;M)=12.833, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.082, MI(Y;M)=2.864 | lower: MI(X;M)=5.117, MI(Y;M)=2.458 | bin: MI(X;M)=12.732, MI(Y;M)=3.265\n",
      "- Layer 2 upper: MI(X;M)=8.517, MI(Y;M)=2.846 | lower: MI(X;M)=5.052, MI(Y;M)=2.550 | bin: MI(X;M)=12.387, MI(Y;M)=3.241\n",
      "- Layer 3 upper: MI(X;M)=8.164, MI(Y;M)=2.845 | lower: MI(X;M)=5.118, MI(Y;M)=2.631 | bin: MI(X;M)=11.983, MI(Y;M)=3.228\n",
      "- Layer 4 upper: MI(X;M)=8.079, MI(Y;M)=2.857 | lower: MI(X;M)=5.358, MI(Y;M)=2.676 | bin: MI(X;M)=11.016, MI(Y;M)=3.139\n",
      "- Layer 5 upper: MI(X;M)=7.644, MI(Y;M)=2.844 | lower: MI(X;M)=5.397, MI(Y;M)=2.692 | bin: MI(X;M)=9.856, MI(Y;M)=3.060\n",
      "- Layer 6 upper: MI(X;M)=3.524, MI(Y;M)=2.551 | lower: MI(X;M)=1.927, MI(Y;M)=1.555 | bin: MI(X;M)=3.451, MI(Y;M)=2.439\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000012\n",
      "- Layer 0 upper: MI(X;M)=10.745, MI(Y;M)=3.010 | lower: MI(X;M)=6.114, MI(Y;M)=2.460 | bin: MI(X;M)=12.835, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.090, MI(Y;M)=2.866 | lower: MI(X;M)=5.125, MI(Y;M)=2.460 | bin: MI(X;M)=12.737, MI(Y;M)=3.268\n",
      "- Layer 2 upper: MI(X;M)=8.528, MI(Y;M)=2.849 | lower: MI(X;M)=5.062, MI(Y;M)=2.552 | bin: MI(X;M)=12.393, MI(Y;M)=3.241\n",
      "- Layer 3 upper: MI(X;M)=8.178, MI(Y;M)=2.848 | lower: MI(X;M)=5.132, MI(Y;M)=2.633 | bin: MI(X;M)=11.974, MI(Y;M)=3.228\n",
      "- Layer 4 upper: MI(X;M)=8.090, MI(Y;M)=2.860 | lower: MI(X;M)=5.369, MI(Y;M)=2.678 | bin: MI(X;M)=10.995, MI(Y;M)=3.145\n",
      "- Layer 5 upper: MI(X;M)=7.644, MI(Y;M)=2.847 | lower: MI(X;M)=5.402, MI(Y;M)=2.693 | bin: MI(X;M)=9.837, MI(Y;M)=3.064\n",
      "- Layer 6 upper: MI(X;M)=3.527, MI(Y;M)=2.555 | lower: MI(X;M)=1.938, MI(Y;M)=1.564 | bin: MI(X;M)=3.450, MI(Y;M)=2.441\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000013\n",
      "- Layer 0 upper: MI(X;M)=10.751, MI(Y;M)=3.011 | lower: MI(X;M)=6.121, MI(Y;M)=2.461 | bin: MI(X;M)=12.838, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.094, MI(Y;M)=2.866 | lower: MI(X;M)=5.127, MI(Y;M)=2.461 | bin: MI(X;M)=12.733, MI(Y;M)=3.268\n",
      "- Layer 2 upper: MI(X;M)=8.532, MI(Y;M)=2.849 | lower: MI(X;M)=5.064, MI(Y;M)=2.552 | bin: MI(X;M)=12.393, MI(Y;M)=3.243\n",
      "- Layer 3 upper: MI(X;M)=8.187, MI(Y;M)=2.849 | lower: MI(X;M)=5.136, MI(Y;M)=2.635 | bin: MI(X;M)=11.956, MI(Y;M)=3.229\n",
      "- Layer 4 upper: MI(X;M)=8.097, MI(Y;M)=2.861 | lower: MI(X;M)=5.373, MI(Y;M)=2.679 | bin: MI(X;M)=10.999, MI(Y;M)=3.146\n",
      "- Layer 5 upper: MI(X;M)=7.648, MI(Y;M)=2.847 | lower: MI(X;M)=5.404, MI(Y;M)=2.694 | bin: MI(X;M)=9.829, MI(Y;M)=3.065\n",
      "- Layer 6 upper: MI(X;M)=3.529, MI(Y;M)=2.558 | lower: MI(X;M)=1.942, MI(Y;M)=1.569 | bin: MI(X;M)=3.448, MI(Y;M)=2.442\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000014\n",
      "- Layer 0 upper: MI(X;M)=10.748, MI(Y;M)=3.011 | lower: MI(X;M)=6.120, MI(Y;M)=2.461 | bin: MI(X;M)=12.834, MI(Y;M)=3.266\n",
      "- Layer 1 upper: MI(X;M)=9.089, MI(Y;M)=2.867 | lower: MI(X;M)=5.125, MI(Y;M)=2.460 | bin: MI(X;M)=12.736, MI(Y;M)=3.272\n",
      "- Layer 2 upper: MI(X;M)=8.522, MI(Y;M)=2.849 | lower: MI(X;M)=5.058, MI(Y;M)=2.552 | bin: MI(X;M)=12.392, MI(Y;M)=3.240\n",
      "- Layer 3 upper: MI(X;M)=8.177, MI(Y;M)=2.850 | lower: MI(X;M)=5.130, MI(Y;M)=2.635 | bin: MI(X;M)=11.965, MI(Y;M)=3.227\n",
      "- Layer 4 upper: MI(X;M)=8.087, MI(Y;M)=2.862 | lower: MI(X;M)=5.366, MI(Y;M)=2.680 | bin: MI(X;M)=10.989, MI(Y;M)=3.147\n",
      "- Layer 5 upper: MI(X;M)=7.633, MI(Y;M)=2.848 | lower: MI(X;M)=5.396, MI(Y;M)=2.695 | bin: MI(X;M)=9.805, MI(Y;M)=3.062\n",
      "- Layer 6 upper: MI(X;M)=3.529, MI(Y;M)=2.557 | lower: MI(X;M)=1.945, MI(Y;M)=1.571 | bin: MI(X;M)=3.448, MI(Y;M)=2.443\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000015\n",
      "- Layer 0 upper: MI(X;M)=10.754, MI(Y;M)=3.011 | lower: MI(X;M)=6.123, MI(Y;M)=2.460 | bin: MI(X;M)=12.838, MI(Y;M)=3.267\n",
      "- Layer 1 upper: MI(X;M)=9.095, MI(Y;M)=2.867 | lower: MI(X;M)=5.125, MI(Y;M)=2.459 | bin: MI(X;M)=12.746, MI(Y;M)=3.271\n",
      "- Layer 2 upper: MI(X;M)=8.527, MI(Y;M)=2.849 | lower: MI(X;M)=5.057, MI(Y;M)=2.551 | bin: MI(X;M)=12.391, MI(Y;M)=3.243\n",
      "- Layer 3 upper: MI(X;M)=8.182, MI(Y;M)=2.850 | lower: MI(X;M)=5.128, MI(Y;M)=2.635 | bin: MI(X;M)=11.968, MI(Y;M)=3.224\n",
      "- Layer 4 upper: MI(X;M)=8.095, MI(Y;M)=2.862 | lower: MI(X;M)=5.367, MI(Y;M)=2.681 | bin: MI(X;M)=11.004, MI(Y;M)=3.140\n",
      "- Layer 5 upper: MI(X;M)=7.645, MI(Y;M)=2.850 | lower: MI(X;M)=5.398, MI(Y;M)=2.696 | bin: MI(X;M)=9.834, MI(Y;M)=3.064\n",
      "- Layer 6 upper: MI(X;M)=3.528, MI(Y;M)=2.558 | lower: MI(X;M)=1.942, MI(Y;M)=1.569 | bin: MI(X;M)=3.449, MI(Y;M)=2.448\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000016\n",
      "- Layer 0 upper: MI(X;M)=10.757, MI(Y;M)=3.012 | lower: MI(X;M)=6.127, MI(Y;M)=2.461 | bin: MI(X;M)=12.833, MI(Y;M)=3.265\n",
      "- Layer 1 upper: MI(X;M)=9.098, MI(Y;M)=2.868 | lower: MI(X;M)=5.128, MI(Y;M)=2.460 | bin: MI(X;M)=12.749, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.532, MI(Y;M)=2.851 | lower: MI(X;M)=5.060, MI(Y;M)=2.552 | bin: MI(X;M)=12.404, MI(Y;M)=3.244\n",
      "- Layer 3 upper: MI(X;M)=8.195, MI(Y;M)=2.852 | lower: MI(X;M)=5.135, MI(Y;M)=2.636 | bin: MI(X;M)=11.978, MI(Y;M)=3.234\n",
      "- Layer 4 upper: MI(X;M)=8.109, MI(Y;M)=2.865 | lower: MI(X;M)=5.375, MI(Y;M)=2.682 | bin: MI(X;M)=11.017, MI(Y;M)=3.150\n",
      "- Layer 5 upper: MI(X;M)=7.654, MI(Y;M)=2.852 | lower: MI(X;M)=5.405, MI(Y;M)=2.697 | bin: MI(X;M)=9.847, MI(Y;M)=3.067\n",
      "- Layer 6 upper: MI(X;M)=3.530, MI(Y;M)=2.560 | lower: MI(X;M)=1.947, MI(Y;M)=1.573 | bin: MI(X;M)=3.448, MI(Y;M)=2.445\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000017\n",
      "- Layer 0 upper: MI(X;M)=10.771, MI(Y;M)=3.015 | lower: MI(X;M)=6.139, MI(Y;M)=2.462 | bin: MI(X;M)=12.849, MI(Y;M)=3.270\n",
      "- Layer 1 upper: MI(X;M)=9.108, MI(Y;M)=2.869 | lower: MI(X;M)=5.135, MI(Y;M)=2.461 | bin: MI(X;M)=12.736, MI(Y;M)=3.271\n",
      "- Layer 2 upper: MI(X;M)=8.538, MI(Y;M)=2.851 | lower: MI(X;M)=5.064, MI(Y;M)=2.553 | bin: MI(X;M)=12.391, MI(Y;M)=3.244\n",
      "- Layer 3 upper: MI(X;M)=8.192, MI(Y;M)=2.851 | lower: MI(X;M)=5.133, MI(Y;M)=2.637 | bin: MI(X;M)=11.967, MI(Y;M)=3.225\n",
      "- Layer 4 upper: MI(X;M)=8.105, MI(Y;M)=2.863 | lower: MI(X;M)=5.371, MI(Y;M)=2.683 | bin: MI(X;M)=11.024, MI(Y;M)=3.150\n",
      "- Layer 5 upper: MI(X;M)=7.661, MI(Y;M)=2.851 | lower: MI(X;M)=5.406, MI(Y;M)=2.699 | bin: MI(X;M)=9.866, MI(Y;M)=3.068\n",
      "- Layer 6 upper: MI(X;M)=3.531, MI(Y;M)=2.561 | lower: MI(X;M)=1.944, MI(Y;M)=1.571 | bin: MI(X;M)=3.451, MI(Y;M)=2.444\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000018\n",
      "- Layer 0 upper: MI(X;M)=10.771, MI(Y;M)=3.015 | lower: MI(X;M)=6.140, MI(Y;M)=2.462 | bin: MI(X;M)=12.840, MI(Y;M)=3.267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer 1 upper: MI(X;M)=9.113, MI(Y;M)=2.871 | lower: MI(X;M)=5.136, MI(Y;M)=2.461 | bin: MI(X;M)=12.752, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.547, MI(Y;M)=2.854 | lower: MI(X;M)=5.069, MI(Y;M)=2.553 | bin: MI(X;M)=12.411, MI(Y;M)=3.249\n",
      "- Layer 3 upper: MI(X;M)=8.211, MI(Y;M)=2.855 | lower: MI(X;M)=5.145, MI(Y;M)=2.638 | bin: MI(X;M)=11.976, MI(Y;M)=3.232\n",
      "- Layer 4 upper: MI(X;M)=8.124, MI(Y;M)=2.868 | lower: MI(X;M)=5.385, MI(Y;M)=2.684 | bin: MI(X;M)=11.030, MI(Y;M)=3.152\n",
      "- Layer 5 upper: MI(X;M)=7.664, MI(Y;M)=2.855 | lower: MI(X;M)=5.413, MI(Y;M)=2.699 | bin: MI(X;M)=9.841, MI(Y;M)=3.065\n",
      "- Layer 6 upper: MI(X;M)=3.533, MI(Y;M)=2.562 | lower: MI(X;M)=1.952, MI(Y;M)=1.578 | bin: MI(X;M)=3.448, MI(Y;M)=2.448\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000019\n",
      "- Layer 0 upper: MI(X;M)=10.786, MI(Y;M)=3.017 | lower: MI(X;M)=6.154, MI(Y;M)=2.464 | bin: MI(X;M)=12.854, MI(Y;M)=3.269\n",
      "- Layer 1 upper: MI(X;M)=9.119, MI(Y;M)=2.871 | lower: MI(X;M)=5.142, MI(Y;M)=2.462 | bin: MI(X;M)=12.742, MI(Y;M)=3.271\n",
      "- Layer 2 upper: MI(X;M)=8.544, MI(Y;M)=2.853 | lower: MI(X;M)=5.068, MI(Y;M)=2.555 | bin: MI(X;M)=12.386, MI(Y;M)=3.245\n",
      "- Layer 3 upper: MI(X;M)=8.194, MI(Y;M)=2.853 | lower: MI(X;M)=5.134, MI(Y;M)=2.638 | bin: MI(X;M)=12.000, MI(Y;M)=3.227\n",
      "- Layer 4 upper: MI(X;M)=8.099, MI(Y;M)=2.864 | lower: MI(X;M)=5.367, MI(Y;M)=2.685 | bin: MI(X;M)=11.013, MI(Y;M)=3.143\n",
      "- Layer 5 upper: MI(X;M)=7.650, MI(Y;M)=2.851 | lower: MI(X;M)=5.402, MI(Y;M)=2.700 | bin: MI(X;M)=9.838, MI(Y;M)=3.055\n",
      "- Layer 6 upper: MI(X;M)=3.533, MI(Y;M)=2.560 | lower: MI(X;M)=1.948, MI(Y;M)=1.573 | bin: MI(X;M)=3.452, MI(Y;M)=2.447\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000020\n",
      "- Layer 0 upper: MI(X;M)=10.784, MI(Y;M)=3.017 | lower: MI(X;M)=6.153, MI(Y;M)=2.463 | bin: MI(X;M)=12.849, MI(Y;M)=3.272\n",
      "- Layer 1 upper: MI(X;M)=9.120, MI(Y;M)=2.871 | lower: MI(X;M)=5.143, MI(Y;M)=2.462 | bin: MI(X;M)=12.746, MI(Y;M)=3.272\n",
      "- Layer 2 upper: MI(X;M)=8.546, MI(Y;M)=2.853 | lower: MI(X;M)=5.069, MI(Y;M)=2.554 | bin: MI(X;M)=12.402, MI(Y;M)=3.245\n",
      "- Layer 3 upper: MI(X;M)=8.203, MI(Y;M)=2.854 | lower: MI(X;M)=5.139, MI(Y;M)=2.639 | bin: MI(X;M)=11.971, MI(Y;M)=3.225\n",
      "- Layer 4 upper: MI(X;M)=8.113, MI(Y;M)=2.867 | lower: MI(X;M)=5.375, MI(Y;M)=2.686 | bin: MI(X;M)=11.029, MI(Y;M)=3.148\n",
      "- Layer 5 upper: MI(X;M)=7.658, MI(Y;M)=2.854 | lower: MI(X;M)=5.405, MI(Y;M)=2.702 | bin: MI(X;M)=9.839, MI(Y;M)=3.062\n",
      "- Layer 6 upper: MI(X;M)=3.534, MI(Y;M)=2.565 | lower: MI(X;M)=1.953, MI(Y;M)=1.580 | bin: MI(X;M)=3.450, MI(Y;M)=2.452\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000025\n",
      "- Layer 0 upper: MI(X;M)=10.790, MI(Y;M)=3.019 | lower: MI(X;M)=6.163, MI(Y;M)=2.463 | bin: MI(X;M)=12.835, MI(Y;M)=3.269\n",
      "- Layer 1 upper: MI(X;M)=9.128, MI(Y;M)=2.874 | lower: MI(X;M)=5.143, MI(Y;M)=2.459 | bin: MI(X;M)=12.741, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.555, MI(Y;M)=2.857 | lower: MI(X;M)=5.069, MI(Y;M)=2.553 | bin: MI(X;M)=12.376, MI(Y;M)=3.243\n",
      "- Layer 3 upper: MI(X;M)=8.232, MI(Y;M)=2.860 | lower: MI(X;M)=5.149, MI(Y;M)=2.642 | bin: MI(X;M)=11.997, MI(Y;M)=3.233\n",
      "- Layer 4 upper: MI(X;M)=8.163, MI(Y;M)=2.877 | lower: MI(X;M)=5.400, MI(Y;M)=2.691 | bin: MI(X;M)=11.083, MI(Y;M)=3.160\n",
      "- Layer 5 upper: MI(X;M)=7.700, MI(Y;M)=2.866 | lower: MI(X;M)=5.429, MI(Y;M)=2.708 | bin: MI(X;M)=9.854, MI(Y;M)=3.079\n",
      "- Layer 6 upper: MI(X;M)=3.540, MI(Y;M)=2.572 | lower: MI(X;M)=1.967, MI(Y;M)=1.593 | bin: MI(X;M)=3.445, MI(Y;M)=2.463\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000030\n",
      "- Layer 0 upper: MI(X;M)=10.827, MI(Y;M)=3.026 | lower: MI(X;M)=6.197, MI(Y;M)=2.467 | bin: MI(X;M)=12.832, MI(Y;M)=3.268\n",
      "- Layer 1 upper: MI(X;M)=9.156, MI(Y;M)=2.879 | lower: MI(X;M)=5.161, MI(Y;M)=2.461 | bin: MI(X;M)=12.744, MI(Y;M)=3.274\n",
      "- Layer 2 upper: MI(X;M)=8.579, MI(Y;M)=2.861 | lower: MI(X;M)=5.082, MI(Y;M)=2.555 | bin: MI(X;M)=12.373, MI(Y;M)=3.250\n",
      "- Layer 3 upper: MI(X;M)=8.260, MI(Y;M)=2.865 | lower: MI(X;M)=5.160, MI(Y;M)=2.646 | bin: MI(X;M)=12.019, MI(Y;M)=3.237\n",
      "- Layer 4 upper: MI(X;M)=8.190, MI(Y;M)=2.882 | lower: MI(X;M)=5.411, MI(Y;M)=2.696 | bin: MI(X;M)=11.092, MI(Y;M)=3.170\n",
      "- Layer 5 upper: MI(X;M)=7.731, MI(Y;M)=2.872 | lower: MI(X;M)=5.446, MI(Y;M)=2.714 | bin: MI(X;M)=9.873, MI(Y;M)=3.081\n",
      "- Layer 6 upper: MI(X;M)=3.545, MI(Y;M)=2.578 | lower: MI(X;M)=1.973, MI(Y;M)=1.599 | bin: MI(X;M)=3.448, MI(Y;M)=2.468\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000035\n",
      "- Layer 0 upper: MI(X;M)=10.855, MI(Y;M)=3.031 | lower: MI(X;M)=6.225, MI(Y;M)=2.469 | bin: MI(X;M)=12.839, MI(Y;M)=3.272\n",
      "- Layer 1 upper: MI(X;M)=9.186, MI(Y;M)=2.884 | lower: MI(X;M)=5.181, MI(Y;M)=2.464 | bin: MI(X;M)=12.749, MI(Y;M)=3.277\n",
      "- Layer 2 upper: MI(X;M)=8.602, MI(Y;M)=2.865 | lower: MI(X;M)=5.096, MI(Y;M)=2.558 | bin: MI(X;M)=12.382, MI(Y;M)=3.251\n",
      "- Layer 3 upper: MI(X;M)=8.283, MI(Y;M)=2.869 | lower: MI(X;M)=5.174, MI(Y;M)=2.649 | bin: MI(X;M)=12.030, MI(Y;M)=3.239\n",
      "- Layer 4 upper: MI(X;M)=8.203, MI(Y;M)=2.886 | lower: MI(X;M)=5.418, MI(Y;M)=2.700 | bin: MI(X;M)=11.097, MI(Y;M)=3.168\n",
      "- Layer 5 upper: MI(X;M)=7.734, MI(Y;M)=2.875 | lower: MI(X;M)=5.446, MI(Y;M)=2.718 | bin: MI(X;M)=9.866, MI(Y;M)=3.083\n",
      "- Layer 6 upper: MI(X;M)=3.548, MI(Y;M)=2.583 | lower: MI(X;M)=1.982, MI(Y;M)=1.607 | bin: MI(X;M)=3.450, MI(Y;M)=2.471\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000040\n",
      "- Layer 0 upper: MI(X;M)=10.874, MI(Y;M)=3.035 | lower: MI(X;M)=6.245, MI(Y;M)=2.470 | bin: MI(X;M)=12.830, MI(Y;M)=3.271\n",
      "- Layer 1 upper: MI(X;M)=9.198, MI(Y;M)=2.885 | lower: MI(X;M)=5.187, MI(Y;M)=2.462 | bin: MI(X;M)=12.744, MI(Y;M)=3.278\n",
      "- Layer 2 upper: MI(X;M)=8.607, MI(Y;M)=2.867 | lower: MI(X;M)=5.095, MI(Y;M)=2.558 | bin: MI(X;M)=12.347, MI(Y;M)=3.249\n",
      "- Layer 3 upper: MI(X;M)=8.291, MI(Y;M)=2.871 | lower: MI(X;M)=5.173, MI(Y;M)=2.651 | bin: MI(X;M)=12.015, MI(Y;M)=3.231\n",
      "- Layer 4 upper: MI(X;M)=8.221, MI(Y;M)=2.889 | lower: MI(X;M)=5.423, MI(Y;M)=2.704 | bin: MI(X;M)=11.144, MI(Y;M)=3.170\n",
      "- Layer 5 upper: MI(X;M)=7.756, MI(Y;M)=2.880 | lower: MI(X;M)=5.455, MI(Y;M)=2.723 | bin: MI(X;M)=9.900, MI(Y;M)=3.093\n",
      "- Layer 6 upper: MI(X;M)=3.552, MI(Y;M)=2.587 | lower: MI(X;M)=1.986, MI(Y;M)=1.612 | bin: MI(X;M)=3.449, MI(Y;M)=2.480\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000045\n",
      "- Layer 0 upper: MI(X;M)=10.878, MI(Y;M)=3.036 | lower: MI(X;M)=6.254, MI(Y;M)=2.470 | bin: MI(X;M)=12.825, MI(Y;M)=3.270\n",
      "- Layer 1 upper: MI(X;M)=9.208, MI(Y;M)=2.888 | lower: MI(X;M)=5.192, MI(Y;M)=2.461 | bin: MI(X;M)=12.734, MI(Y;M)=3.280\n",
      "- Layer 2 upper: MI(X;M)=8.613, MI(Y;M)=2.869 | lower: MI(X;M)=5.095, MI(Y;M)=2.556 | bin: MI(X;M)=12.321, MI(Y;M)=3.246\n",
      "- Layer 3 upper: MI(X;M)=8.304, MI(Y;M)=2.874 | lower: MI(X;M)=5.177, MI(Y;M)=2.653 | bin: MI(X;M)=12.032, MI(Y;M)=3.237\n",
      "- Layer 4 upper: MI(X;M)=8.235, MI(Y;M)=2.893 | lower: MI(X;M)=5.428, MI(Y;M)=2.707 | bin: MI(X;M)=11.145, MI(Y;M)=3.170\n",
      "- Layer 5 upper: MI(X;M)=7.765, MI(Y;M)=2.885 | lower: MI(X;M)=5.459, MI(Y;M)=2.727 | bin: MI(X;M)=9.895, MI(Y;M)=3.098\n",
      "- Layer 6 upper: MI(X;M)=3.555, MI(Y;M)=2.591 | lower: MI(X;M)=1.995, MI(Y;M)=1.620 | bin: MI(X;M)=3.448, MI(Y;M)=2.486\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000050\n",
      "- Layer 0 upper: MI(X;M)=10.898, MI(Y;M)=3.040 | lower: MI(X;M)=6.278, MI(Y;M)=2.474 | bin: MI(X;M)=12.826, MI(Y;M)=3.275\n",
      "- Layer 1 upper: MI(X;M)=9.230, MI(Y;M)=2.892 | lower: MI(X;M)=5.210, MI(Y;M)=2.464 | bin: MI(X;M)=12.733, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.627, MI(Y;M)=2.872 | lower: MI(X;M)=5.106, MI(Y;M)=2.559 | bin: MI(X;M)=12.350, MI(Y;M)=3.252\n",
      "- Layer 3 upper: MI(X;M)=8.325, MI(Y;M)=2.878 | lower: MI(X;M)=5.191, MI(Y;M)=2.657 | bin: MI(X;M)=12.040, MI(Y;M)=3.243\n",
      "- Layer 4 upper: MI(X;M)=8.255, MI(Y;M)=2.898 | lower: MI(X;M)=5.441, MI(Y;M)=2.711 | bin: MI(X;M)=11.154, MI(Y;M)=3.180\n",
      "- Layer 5 upper: MI(X;M)=7.767, MI(Y;M)=2.889 | lower: MI(X;M)=5.460, MI(Y;M)=2.730 | bin: MI(X;M)=9.877, MI(Y;M)=3.095\n",
      "- Layer 6 upper: MI(X;M)=3.556, MI(Y;M)=2.597 | lower: MI(X;M)=2.008, MI(Y;M)=1.634 | bin: MI(X;M)=3.445, MI(Y;M)=2.493\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000055\n",
      "- Layer 0 upper: MI(X;M)=10.932, MI(Y;M)=3.046 | lower: MI(X;M)=6.311, MI(Y;M)=2.476 | bin: MI(X;M)=12.830, MI(Y;M)=3.279\n",
      "- Layer 1 upper: MI(X;M)=9.260, MI(Y;M)=2.896 | lower: MI(X;M)=5.227, MI(Y;M)=2.465 | bin: MI(X;M)=12.723, MI(Y;M)=3.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer 2 upper: MI(X;M)=8.652, MI(Y;M)=2.876 | lower: MI(X;M)=5.120, MI(Y;M)=2.561 | bin: MI(X;M)=12.344, MI(Y;M)=3.257\n",
      "- Layer 3 upper: MI(X;M)=8.341, MI(Y;M)=2.880 | lower: MI(X;M)=5.197, MI(Y;M)=2.659 | bin: MI(X;M)=12.050, MI(Y;M)=3.242\n",
      "- Layer 4 upper: MI(X;M)=8.259, MI(Y;M)=2.899 | lower: MI(X;M)=5.441, MI(Y;M)=2.715 | bin: MI(X;M)=11.157, MI(Y;M)=3.177\n",
      "- Layer 5 upper: MI(X;M)=7.774, MI(Y;M)=2.890 | lower: MI(X;M)=5.463, MI(Y;M)=2.733 | bin: MI(X;M)=9.868, MI(Y;M)=3.092\n",
      "- Layer 6 upper: MI(X;M)=3.561, MI(Y;M)=2.599 | lower: MI(X;M)=2.009, MI(Y;M)=1.633 | bin: MI(X;M)=3.450, MI(Y;M)=2.487\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000060\n",
      "- Layer 0 upper: MI(X;M)=10.936, MI(Y;M)=3.046 | lower: MI(X;M)=6.319, MI(Y;M)=2.475 | bin: MI(X;M)=12.819, MI(Y;M)=3.277\n",
      "- Layer 1 upper: MI(X;M)=9.269, MI(Y;M)=2.897 | lower: MI(X;M)=5.233, MI(Y;M)=2.463 | bin: MI(X;M)=12.713, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.653, MI(Y;M)=2.876 | lower: MI(X;M)=5.118, MI(Y;M)=2.560 | bin: MI(X;M)=12.347, MI(Y;M)=3.254\n",
      "- Layer 3 upper: MI(X;M)=8.350, MI(Y;M)=2.882 | lower: MI(X;M)=5.198, MI(Y;M)=2.660 | bin: MI(X;M)=12.058, MI(Y;M)=3.244\n",
      "- Layer 4 upper: MI(X;M)=8.278, MI(Y;M)=2.903 | lower: MI(X;M)=5.447, MI(Y;M)=2.717 | bin: MI(X;M)=11.179, MI(Y;M)=3.183\n",
      "- Layer 5 upper: MI(X;M)=7.790, MI(Y;M)=2.895 | lower: MI(X;M)=5.468, MI(Y;M)=2.737 | bin: MI(X;M)=9.872, MI(Y;M)=3.098\n",
      "- Layer 6 upper: MI(X;M)=3.561, MI(Y;M)=2.605 | lower: MI(X;M)=2.016, MI(Y;M)=1.642 | bin: MI(X;M)=3.448, MI(Y;M)=2.506\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000065\n",
      "- Layer 0 upper: MI(X;M)=10.953, MI(Y;M)=3.049 | lower: MI(X;M)=6.337, MI(Y;M)=2.475 | bin: MI(X;M)=12.825, MI(Y;M)=3.279\n",
      "- Layer 1 upper: MI(X;M)=9.290, MI(Y;M)=2.900 | lower: MI(X;M)=5.246, MI(Y;M)=2.463 | bin: MI(X;M)=12.724, MI(Y;M)=3.275\n",
      "- Layer 2 upper: MI(X;M)=8.674, MI(Y;M)=2.879 | lower: MI(X;M)=5.128, MI(Y;M)=2.559 | bin: MI(X;M)=12.359, MI(Y;M)=3.256\n",
      "- Layer 3 upper: MI(X;M)=8.375, MI(Y;M)=2.886 | lower: MI(X;M)=5.210, MI(Y;M)=2.662 | bin: MI(X;M)=12.051, MI(Y;M)=3.242\n",
      "- Layer 4 upper: MI(X;M)=8.312, MI(Y;M)=2.908 | lower: MI(X;M)=5.466, MI(Y;M)=2.720 | bin: MI(X;M)=11.214, MI(Y;M)=3.184\n",
      "- Layer 5 upper: MI(X;M)=7.818, MI(Y;M)=2.900 | lower: MI(X;M)=5.484, MI(Y;M)=2.741 | bin: MI(X;M)=9.891, MI(Y;M)=3.102\n",
      "- Layer 6 upper: MI(X;M)=3.564, MI(Y;M)=2.608 | lower: MI(X;M)=2.022, MI(Y;M)=1.648 | bin: MI(X;M)=3.447, MI(Y;M)=2.505\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000070\n",
      "- Layer 0 upper: MI(X;M)=10.950, MI(Y;M)=3.049 | lower: MI(X;M)=6.342, MI(Y;M)=2.475 | bin: MI(X;M)=12.807, MI(Y;M)=3.275\n",
      "- Layer 1 upper: MI(X;M)=9.291, MI(Y;M)=2.900 | lower: MI(X;M)=5.248, MI(Y;M)=2.462 | bin: MI(X;M)=12.718, MI(Y;M)=3.277\n",
      "- Layer 2 upper: MI(X;M)=8.665, MI(Y;M)=2.879 | lower: MI(X;M)=5.122, MI(Y;M)=2.559 | bin: MI(X;M)=12.378, MI(Y;M)=3.255\n",
      "- Layer 3 upper: MI(X;M)=8.366, MI(Y;M)=2.885 | lower: MI(X;M)=5.203, MI(Y;M)=2.663 | bin: MI(X;M)=12.048, MI(Y;M)=3.245\n",
      "- Layer 4 upper: MI(X;M)=8.305, MI(Y;M)=2.908 | lower: MI(X;M)=5.459, MI(Y;M)=2.722 | bin: MI(X;M)=11.227, MI(Y;M)=3.185\n",
      "- Layer 5 upper: MI(X;M)=7.806, MI(Y;M)=2.901 | lower: MI(X;M)=5.473, MI(Y;M)=2.743 | bin: MI(X;M)=9.876, MI(Y;M)=3.099\n",
      "- Layer 6 upper: MI(X;M)=3.563, MI(Y;M)=2.611 | lower: MI(X;M)=2.027, MI(Y;M)=1.654 | bin: MI(X;M)=3.444, MI(Y;M)=2.510\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000075\n",
      "- Layer 0 upper: MI(X;M)=10.981, MI(Y;M)=3.055 | lower: MI(X;M)=6.375, MI(Y;M)=2.478 | bin: MI(X;M)=12.819, MI(Y;M)=3.278\n",
      "- Layer 1 upper: MI(X;M)=9.327, MI(Y;M)=2.905 | lower: MI(X;M)=5.271, MI(Y;M)=2.464 | bin: MI(X;M)=12.716, MI(Y;M)=3.280\n",
      "- Layer 2 upper: MI(X;M)=8.705, MI(Y;M)=2.885 | lower: MI(X;M)=5.145, MI(Y;M)=2.561 | bin: MI(X;M)=12.363, MI(Y;M)=3.254\n",
      "- Layer 3 upper: MI(X;M)=8.410, MI(Y;M)=2.892 | lower: MI(X;M)=5.227, MI(Y;M)=2.666 | bin: MI(X;M)=12.041, MI(Y;M)=3.245\n",
      "- Layer 4 upper: MI(X;M)=8.357, MI(Y;M)=2.917 | lower: MI(X;M)=5.491, MI(Y;M)=2.727 | bin: MI(X;M)=11.255, MI(Y;M)=3.196\n",
      "- Layer 5 upper: MI(X;M)=7.856, MI(Y;M)=2.911 | lower: MI(X;M)=5.507, MI(Y;M)=2.749 | bin: MI(X;M)=9.902, MI(Y;M)=3.115\n",
      "- Layer 6 upper: MI(X;M)=3.568, MI(Y;M)=2.617 | lower: MI(X;M)=2.037, MI(Y;M)=1.664 | bin: MI(X;M)=3.442, MI(Y;M)=2.514\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000080\n",
      "- Layer 0 upper: MI(X;M)=10.993, MI(Y;M)=3.057 | lower: MI(X;M)=6.389, MI(Y;M)=2.478 | bin: MI(X;M)=12.825, MI(Y;M)=3.278\n",
      "- Layer 1 upper: MI(X;M)=9.336, MI(Y;M)=2.906 | lower: MI(X;M)=5.277, MI(Y;M)=2.463 | bin: MI(X;M)=12.711, MI(Y;M)=3.276\n",
      "- Layer 2 upper: MI(X;M)=8.711, MI(Y;M)=2.886 | lower: MI(X;M)=5.146, MI(Y;M)=2.561 | bin: MI(X;M)=12.382, MI(Y;M)=3.258\n",
      "- Layer 3 upper: MI(X;M)=8.416, MI(Y;M)=2.894 | lower: MI(X;M)=5.228, MI(Y;M)=2.667 | bin: MI(X;M)=12.033, MI(Y;M)=3.242\n",
      "- Layer 4 upper: MI(X;M)=8.362, MI(Y;M)=2.918 | lower: MI(X;M)=5.491, MI(Y;M)=2.729 | bin: MI(X;M)=11.256, MI(Y;M)=3.193\n",
      "- Layer 5 upper: MI(X;M)=7.862, MI(Y;M)=2.914 | lower: MI(X;M)=5.509, MI(Y;M)=2.752 | bin: MI(X;M)=9.909, MI(Y;M)=3.114\n",
      "- Layer 6 upper: MI(X;M)=3.570, MI(Y;M)=2.621 | lower: MI(X;M)=2.043, MI(Y;M)=1.669 | bin: MI(X;M)=3.443, MI(Y;M)=2.517\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000085\n",
      "- Layer 0 upper: MI(X;M)=11.007, MI(Y;M)=3.059 | lower: MI(X;M)=6.404, MI(Y;M)=2.479 | bin: MI(X;M)=12.827, MI(Y;M)=3.282\n",
      "- Layer 1 upper: MI(X;M)=9.363, MI(Y;M)=2.910 | lower: MI(X;M)=5.293, MI(Y;M)=2.464 | bin: MI(X;M)=12.725, MI(Y;M)=3.278\n",
      "- Layer 2 upper: MI(X;M)=8.734, MI(Y;M)=2.888 | lower: MI(X;M)=5.160, MI(Y;M)=2.562 | bin: MI(X;M)=12.390, MI(Y;M)=3.257\n",
      "- Layer 3 upper: MI(X;M)=8.447, MI(Y;M)=2.896 | lower: MI(X;M)=5.246, MI(Y;M)=2.670 | bin: MI(X;M)=12.051, MI(Y;M)=3.244\n",
      "- Layer 4 upper: MI(X;M)=8.396, MI(Y;M)=2.921 | lower: MI(X;M)=5.512, MI(Y;M)=2.732 | bin: MI(X;M)=11.331, MI(Y;M)=3.196\n",
      "- Layer 5 upper: MI(X;M)=7.886, MI(Y;M)=2.915 | lower: MI(X;M)=5.522, MI(Y;M)=2.754 | bin: MI(X;M)=9.939, MI(Y;M)=3.115\n",
      "- Layer 6 upper: MI(X;M)=3.572, MI(Y;M)=2.623 | lower: MI(X;M)=2.046, MI(Y;M)=1.673 | bin: MI(X;M)=3.444, MI(Y;M)=2.519\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000090\n",
      "- Layer 0 upper: MI(X;M)=11.014, MI(Y;M)=3.061 | lower: MI(X;M)=6.415, MI(Y;M)=2.478 | bin: MI(X;M)=12.819, MI(Y;M)=3.277\n",
      "- Layer 1 upper: MI(X;M)=9.375, MI(Y;M)=2.911 | lower: MI(X;M)=5.303, MI(Y;M)=2.463 | bin: MI(X;M)=12.732, MI(Y;M)=3.279\n",
      "- Layer 2 upper: MI(X;M)=8.747, MI(Y;M)=2.890 | lower: MI(X;M)=5.166, MI(Y;M)=2.562 | bin: MI(X;M)=12.401, MI(Y;M)=3.258\n",
      "- Layer 3 upper: MI(X;M)=8.455, MI(Y;M)=2.899 | lower: MI(X;M)=5.250, MI(Y;M)=2.671 | bin: MI(X;M)=12.054, MI(Y;M)=3.250\n",
      "- Layer 4 upper: MI(X;M)=8.415, MI(Y;M)=2.926 | lower: MI(X;M)=5.523, MI(Y;M)=2.734 | bin: MI(X;M)=11.332, MI(Y;M)=3.199\n",
      "- Layer 5 upper: MI(X;M)=7.905, MI(Y;M)=2.922 | lower: MI(X;M)=5.534, MI(Y;M)=2.759 | bin: MI(X;M)=9.934, MI(Y;M)=3.120\n",
      "- Layer 6 upper: MI(X;M)=3.572, MI(Y;M)=2.627 | lower: MI(X;M)=2.056, MI(Y;M)=1.683 | bin: MI(X;M)=3.440, MI(Y;M)=2.521\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000095\n",
      "- Layer 0 upper: MI(X;M)=11.026, MI(Y;M)=3.062 | lower: MI(X;M)=6.433, MI(Y;M)=2.480 | bin: MI(X;M)=12.821, MI(Y;M)=3.280\n",
      "- Layer 1 upper: MI(X;M)=9.393, MI(Y;M)=2.915 | lower: MI(X;M)=5.318, MI(Y;M)=2.465 | bin: MI(X;M)=12.733, MI(Y;M)=3.281\n",
      "- Layer 2 upper: MI(X;M)=8.756, MI(Y;M)=2.893 | lower: MI(X;M)=5.175, MI(Y;M)=2.564 | bin: MI(X;M)=12.414, MI(Y;M)=3.265\n",
      "- Layer 3 upper: MI(X;M)=8.464, MI(Y;M)=2.902 | lower: MI(X;M)=5.258, MI(Y;M)=2.674 | bin: MI(X;M)=12.050, MI(Y;M)=3.248\n",
      "- Layer 4 upper: MI(X;M)=8.413, MI(Y;M)=2.928 | lower: MI(X;M)=5.524, MI(Y;M)=2.737 | bin: MI(X;M)=11.356, MI(Y;M)=3.202\n",
      "- Layer 5 upper: MI(X;M)=7.883, MI(Y;M)=2.922 | lower: MI(X;M)=5.522, MI(Y;M)=2.759 | bin: MI(X;M)=9.914, MI(Y;M)=3.124\n",
      "- Layer 6 upper: MI(X;M)=3.572, MI(Y;M)=2.627 | lower: MI(X;M)=2.062, MI(Y;M)=1.688 | bin: MI(X;M)=3.439, MI(Y;M)=2.528\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000100\n",
      "- Layer 0 upper: MI(X;M)=11.040, MI(Y;M)=3.064 | lower: MI(X;M)=6.448, MI(Y;M)=2.481 | bin: MI(X;M)=12.821, MI(Y;M)=3.279\n",
      "- Layer 1 upper: MI(X;M)=9.404, MI(Y;M)=2.914 | lower: MI(X;M)=5.324, MI(Y;M)=2.464 | bin: MI(X;M)=12.725, MI(Y;M)=3.280\n",
      "- Layer 2 upper: MI(X;M)=8.763, MI(Y;M)=2.893 | lower: MI(X;M)=5.175, MI(Y;M)=2.563 | bin: MI(X;M)=12.405, MI(Y;M)=3.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer 3 upper: MI(X;M)=8.470, MI(Y;M)=2.901 | lower: MI(X;M)=5.255, MI(Y;M)=2.674 | bin: MI(X;M)=12.047, MI(Y;M)=3.250\n",
      "- Layer 4 upper: MI(X;M)=8.422, MI(Y;M)=2.928 | lower: MI(X;M)=5.524, MI(Y;M)=2.739 | bin: MI(X;M)=11.347, MI(Y;M)=3.199\n",
      "- Layer 5 upper: MI(X;M)=7.902, MI(Y;M)=2.924 | lower: MI(X;M)=5.530, MI(Y;M)=2.763 | bin: MI(X;M)=9.941, MI(Y;M)=3.118\n",
      "- Layer 6 upper: MI(X;M)=3.575, MI(Y;M)=2.632 | lower: MI(X;M)=2.063, MI(Y;M)=1.690 | bin: MI(X;M)=3.441, MI(Y;M)=2.531\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000110\n",
      "- Layer 0 upper: MI(X;M)=11.071, MI(Y;M)=3.069 | lower: MI(X;M)=6.481, MI(Y;M)=2.483 | bin: MI(X;M)=12.826, MI(Y;M)=3.277\n",
      "- Layer 1 upper: MI(X;M)=9.442, MI(Y;M)=2.919 | lower: MI(X;M)=5.351, MI(Y;M)=2.465 | bin: MI(X;M)=12.743, MI(Y;M)=3.281\n",
      "- Layer 2 upper: MI(X;M)=8.790, MI(Y;M)=2.896 | lower: MI(X;M)=5.189, MI(Y;M)=2.564 | bin: MI(X;M)=12.451, MI(Y;M)=3.264\n",
      "- Layer 3 upper: MI(X;M)=8.488, MI(Y;M)=2.903 | lower: MI(X;M)=5.263, MI(Y;M)=2.676 | bin: MI(X;M)=12.073, MI(Y;M)=3.246\n",
      "- Layer 4 upper: MI(X;M)=8.437, MI(Y;M)=2.929 | lower: MI(X;M)=5.530, MI(Y;M)=2.743 | bin: MI(X;M)=11.372, MI(Y;M)=3.205\n",
      "- Layer 5 upper: MI(X;M)=7.918, MI(Y;M)=2.926 | lower: MI(X;M)=5.534, MI(Y;M)=2.766 | bin: MI(X;M)=9.952, MI(Y;M)=3.118\n",
      "- Layer 6 upper: MI(X;M)=3.577, MI(Y;M)=2.636 | lower: MI(X;M)=2.066, MI(Y;M)=1.694 | bin: MI(X;M)=3.443, MI(Y;M)=2.529\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000120\n",
      "- Layer 0 upper: MI(X;M)=11.096, MI(Y;M)=3.073 | lower: MI(X;M)=6.513, MI(Y;M)=2.484 | bin: MI(X;M)=12.845, MI(Y;M)=3.279\n",
      "- Layer 1 upper: MI(X;M)=9.475, MI(Y;M)=2.923 | lower: MI(X;M)=5.376, MI(Y;M)=2.465 | bin: MI(X;M)=12.757, MI(Y;M)=3.282\n",
      "- Layer 2 upper: MI(X;M)=8.820, MI(Y;M)=2.900 | lower: MI(X;M)=5.206, MI(Y;M)=2.566 | bin: MI(X;M)=12.457, MI(Y;M)=3.261\n",
      "- Layer 3 upper: MI(X;M)=8.518, MI(Y;M)=2.909 | lower: MI(X;M)=5.280, MI(Y;M)=2.680 | bin: MI(X;M)=12.075, MI(Y;M)=3.246\n",
      "- Layer 4 upper: MI(X;M)=8.471, MI(Y;M)=2.936 | lower: MI(X;M)=5.552, MI(Y;M)=2.748 | bin: MI(X;M)=11.384, MI(Y;M)=3.201\n",
      "- Layer 5 upper: MI(X;M)=7.939, MI(Y;M)=2.934 | lower: MI(X;M)=5.551, MI(Y;M)=2.772 | bin: MI(X;M)=9.931, MI(Y;M)=3.126\n",
      "- Layer 6 upper: MI(X;M)=3.580, MI(Y;M)=2.642 | lower: MI(X;M)=2.080, MI(Y;M)=1.707 | bin: MI(X;M)=3.442, MI(Y;M)=2.539\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000130\n",
      "- Layer 0 upper: MI(X;M)=11.106, MI(Y;M)=3.074 | lower: MI(X;M)=6.528, MI(Y;M)=2.484 | bin: MI(X;M)=12.845, MI(Y;M)=3.282\n",
      "- Layer 1 upper: MI(X;M)=9.499, MI(Y;M)=2.925 | lower: MI(X;M)=5.392, MI(Y;M)=2.464 | bin: MI(X;M)=12.756, MI(Y;M)=3.284\n",
      "- Layer 2 upper: MI(X;M)=8.839, MI(Y;M)=2.902 | lower: MI(X;M)=5.216, MI(Y;M)=2.566 | bin: MI(X;M)=12.471, MI(Y;M)=3.262\n",
      "- Layer 3 upper: MI(X;M)=8.543, MI(Y;M)=2.912 | lower: MI(X;M)=5.293, MI(Y;M)=2.683 | bin: MI(X;M)=12.094, MI(Y;M)=3.248\n",
      "- Layer 4 upper: MI(X;M)=8.504, MI(Y;M)=2.941 | lower: MI(X;M)=5.571, MI(Y;M)=2.752 | bin: MI(X;M)=11.413, MI(Y;M)=3.205\n",
      "- Layer 5 upper: MI(X;M)=7.965, MI(Y;M)=2.939 | lower: MI(X;M)=5.566, MI(Y;M)=2.777 | bin: MI(X;M)=9.937, MI(Y;M)=3.129\n",
      "- Layer 6 upper: MI(X;M)=3.581, MI(Y;M)=2.646 | lower: MI(X;M)=2.090, MI(Y;M)=1.718 | bin: MI(X;M)=3.440, MI(Y;M)=2.545\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000140\n",
      "- Layer 0 upper: MI(X;M)=11.131, MI(Y;M)=3.078 | lower: MI(X;M)=6.557, MI(Y;M)=2.485 | bin: MI(X;M)=12.861, MI(Y;M)=3.282\n",
      "- Layer 1 upper: MI(X;M)=9.534, MI(Y;M)=2.929 | lower: MI(X;M)=5.417, MI(Y;M)=2.466 | bin: MI(X;M)=12.772, MI(Y;M)=3.285\n",
      "- Layer 2 upper: MI(X;M)=8.866, MI(Y;M)=2.905 | lower: MI(X;M)=5.231, MI(Y;M)=2.567 | bin: MI(X;M)=12.499, MI(Y;M)=3.267\n",
      "- Layer 3 upper: MI(X;M)=8.557, MI(Y;M)=2.914 | lower: MI(X;M)=5.300, MI(Y;M)=2.685 | bin: MI(X;M)=12.119, MI(Y;M)=3.247\n",
      "- Layer 4 upper: MI(X;M)=8.508, MI(Y;M)=2.942 | lower: MI(X;M)=5.572, MI(Y;M)=2.755 | bin: MI(X;M)=11.412, MI(Y;M)=3.199\n",
      "- Layer 5 upper: MI(X;M)=7.956, MI(Y;M)=2.939 | lower: MI(X;M)=5.558, MI(Y;M)=2.778 | bin: MI(X;M)=9.908, MI(Y;M)=3.122\n",
      "- Layer 6 upper: MI(X;M)=3.581, MI(Y;M)=2.646 | lower: MI(X;M)=2.094, MI(Y;M)=1.720 | bin: MI(X;M)=3.439, MI(Y;M)=2.542\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000150\n",
      "- Layer 0 upper: MI(X;M)=11.152, MI(Y;M)=3.080 | lower: MI(X;M)=6.580, MI(Y;M)=2.485 | bin: MI(X;M)=12.863, MI(Y;M)=3.280\n",
      "- Layer 1 upper: MI(X;M)=9.558, MI(Y;M)=2.931 | lower: MI(X;M)=5.433, MI(Y;M)=2.465 | bin: MI(X;M)=12.787, MI(Y;M)=3.286\n",
      "- Layer 2 upper: MI(X;M)=8.883, MI(Y;M)=2.907 | lower: MI(X;M)=5.237, MI(Y;M)=2.566 | bin: MI(X;M)=12.488, MI(Y;M)=3.265\n",
      "- Layer 3 upper: MI(X;M)=8.572, MI(Y;M)=2.916 | lower: MI(X;M)=5.304, MI(Y;M)=2.687 | bin: MI(X;M)=12.122, MI(Y;M)=3.244\n",
      "- Layer 4 upper: MI(X;M)=8.528, MI(Y;M)=2.945 | lower: MI(X;M)=5.582, MI(Y;M)=2.759 | bin: MI(X;M)=11.428, MI(Y;M)=3.206\n",
      "- Layer 5 upper: MI(X;M)=7.978, MI(Y;M)=2.944 | lower: MI(X;M)=5.570, MI(Y;M)=2.783 | bin: MI(X;M)=9.924, MI(Y;M)=3.126\n",
      "- Layer 6 upper: MI(X;M)=3.583, MI(Y;M)=2.652 | lower: MI(X;M)=2.099, MI(Y;M)=1.727 | bin: MI(X;M)=3.439, MI(Y;M)=2.553\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000160\n",
      "- Layer 0 upper: MI(X;M)=11.167, MI(Y;M)=3.083 | lower: MI(X;M)=6.597, MI(Y;M)=2.486 | bin: MI(X;M)=12.878, MI(Y;M)=3.281\n",
      "- Layer 1 upper: MI(X;M)=9.589, MI(Y;M)=2.935 | lower: MI(X;M)=5.453, MI(Y;M)=2.465 | bin: MI(X;M)=12.806, MI(Y;M)=3.284\n",
      "- Layer 2 upper: MI(X;M)=8.916, MI(Y;M)=2.910 | lower: MI(X;M)=5.254, MI(Y;M)=2.567 | bin: MI(X;M)=12.496, MI(Y;M)=3.271\n",
      "- Layer 3 upper: MI(X;M)=8.617, MI(Y;M)=2.921 | lower: MI(X;M)=5.328, MI(Y;M)=2.689 | bin: MI(X;M)=12.168, MI(Y;M)=3.251\n",
      "- Layer 4 upper: MI(X;M)=8.587, MI(Y;M)=2.951 | lower: MI(X;M)=5.618, MI(Y;M)=2.763 | bin: MI(X;M)=11.494, MI(Y;M)=3.213\n",
      "- Layer 5 upper: MI(X;M)=8.042, MI(Y;M)=2.951 | lower: MI(X;M)=5.611, MI(Y;M)=2.788 | bin: MI(X;M)=9.971, MI(Y;M)=3.133\n",
      "- Layer 6 upper: MI(X;M)=3.586, MI(Y;M)=2.656 | lower: MI(X;M)=2.106, MI(Y;M)=1.734 | bin: MI(X;M)=3.440, MI(Y;M)=2.552\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000170\n",
      "- Layer 0 upper: MI(X;M)=11.187, MI(Y;M)=3.086 | lower: MI(X;M)=6.619, MI(Y;M)=2.487 | bin: MI(X;M)=12.879, MI(Y;M)=3.284\n",
      "- Layer 1 upper: MI(X;M)=9.615, MI(Y;M)=2.938 | lower: MI(X;M)=5.471, MI(Y;M)=2.464 | bin: MI(X;M)=12.815, MI(Y;M)=3.286\n",
      "- Layer 2 upper: MI(X;M)=8.940, MI(Y;M)=2.914 | lower: MI(X;M)=5.266, MI(Y;M)=2.567 | bin: MI(X;M)=12.497, MI(Y;M)=3.271\n",
      "- Layer 3 upper: MI(X;M)=8.634, MI(Y;M)=2.924 | lower: MI(X;M)=5.336, MI(Y;M)=2.691 | bin: MI(X;M)=12.175, MI(Y;M)=3.248\n",
      "- Layer 4 upper: MI(X;M)=8.603, MI(Y;M)=2.955 | lower: MI(X;M)=5.630, MI(Y;M)=2.766 | bin: MI(X;M)=11.482, MI(Y;M)=3.218\n",
      "- Layer 5 upper: MI(X;M)=8.048, MI(Y;M)=2.955 | lower: MI(X;M)=5.617, MI(Y;M)=2.792 | bin: MI(X;M)=9.957, MI(Y;M)=3.144\n",
      "- Layer 6 upper: MI(X;M)=3.587, MI(Y;M)=2.659 | lower: MI(X;M)=2.115, MI(Y;M)=1.743 | bin: MI(X;M)=3.435, MI(Y;M)=2.559\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000180\n",
      "- Layer 0 upper: MI(X;M)=11.211, MI(Y;M)=3.089 | lower: MI(X;M)=6.644, MI(Y;M)=2.488 | bin: MI(X;M)=12.890, MI(Y;M)=3.283\n",
      "- Layer 1 upper: MI(X;M)=9.644, MI(Y;M)=2.940 | lower: MI(X;M)=5.493, MI(Y;M)=2.466 | bin: MI(X;M)=12.818, MI(Y;M)=3.287\n",
      "- Layer 2 upper: MI(X;M)=8.957, MI(Y;M)=2.914 | lower: MI(X;M)=5.275, MI(Y;M)=2.569 | bin: MI(X;M)=12.502, MI(Y;M)=3.271\n",
      "- Layer 3 upper: MI(X;M)=8.642, MI(Y;M)=2.924 | lower: MI(X;M)=5.340, MI(Y;M)=2.693 | bin: MI(X;M)=12.183, MI(Y;M)=3.250\n",
      "- Layer 4 upper: MI(X;M)=8.598, MI(Y;M)=2.953 | lower: MI(X;M)=5.623, MI(Y;M)=2.768 | bin: MI(X;M)=11.477, MI(Y;M)=3.213\n",
      "- Layer 5 upper: MI(X;M)=8.043, MI(Y;M)=2.953 | lower: MI(X;M)=5.610, MI(Y;M)=2.793 | bin: MI(X;M)=9.956, MI(Y;M)=3.132\n",
      "- Layer 6 upper: MI(X;M)=3.586, MI(Y;M)=2.660 | lower: MI(X;M)=2.116, MI(Y;M)=1.744 | bin: MI(X;M)=3.438, MI(Y;M)=2.551\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000190\n",
      "- Layer 0 upper: MI(X;M)=11.215, MI(Y;M)=3.089 | lower: MI(X;M)=6.650, MI(Y;M)=2.485 | bin: MI(X;M)=12.890, MI(Y;M)=3.282\n",
      "- Layer 1 upper: MI(X;M)=9.654, MI(Y;M)=2.940 | lower: MI(X;M)=5.496, MI(Y;M)=2.461 | bin: MI(X;M)=12.815, MI(Y;M)=3.285\n",
      "- Layer 2 upper: MI(X;M)=8.968, MI(Y;M)=2.915 | lower: MI(X;M)=5.273, MI(Y;M)=2.564 | bin: MI(X;M)=12.522, MI(Y;M)=3.274\n",
      "- Layer 3 upper: MI(X;M)=8.657, MI(Y;M)=2.927 | lower: MI(X;M)=5.340, MI(Y;M)=2.692 | bin: MI(X;M)=12.193, MI(Y;M)=3.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer 4 upper: MI(X;M)=8.631, MI(Y;M)=2.959 | lower: MI(X;M)=5.640, MI(Y;M)=2.770 | bin: MI(X;M)=11.488, MI(Y;M)=3.223\n",
      "- Layer 5 upper: MI(X;M)=8.073, MI(Y;M)=2.962 | lower: MI(X;M)=5.631, MI(Y;M)=2.797 | bin: MI(X;M)=9.976, MI(Y;M)=3.151\n",
      "- Layer 6 upper: MI(X;M)=3.588, MI(Y;M)=2.662 | lower: MI(X;M)=2.126, MI(Y;M)=1.753 | bin: MI(X;M)=3.434, MI(Y;M)=2.559\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000200\n",
      "- Layer 0 upper: MI(X;M)=11.235, MI(Y;M)=3.092 | lower: MI(X;M)=6.673, MI(Y;M)=2.488 | bin: MI(X;M)=12.898, MI(Y;M)=3.284\n",
      "- Layer 1 upper: MI(X;M)=9.688, MI(Y;M)=2.945 | lower: MI(X;M)=5.525, MI(Y;M)=2.466 | bin: MI(X;M)=12.826, MI(Y;M)=3.290\n",
      "- Layer 2 upper: MI(X;M)=8.999, MI(Y;M)=2.919 | lower: MI(X;M)=5.299, MI(Y;M)=2.570 | bin: MI(X;M)=12.514, MI(Y;M)=3.274\n",
      "- Layer 3 upper: MI(X;M)=8.689, MI(Y;M)=2.931 | lower: MI(X;M)=5.367, MI(Y;M)=2.698 | bin: MI(X;M)=12.222, MI(Y;M)=3.255\n",
      "- Layer 4 upper: MI(X;M)=8.653, MI(Y;M)=2.962 | lower: MI(X;M)=5.660, MI(Y;M)=2.774 | bin: MI(X;M)=11.501, MI(Y;M)=3.216\n",
      "- Layer 5 upper: MI(X;M)=8.082, MI(Y;M)=2.962 | lower: MI(X;M)=5.639, MI(Y;M)=2.799 | bin: MI(X;M)=9.970, MI(Y;M)=3.145\n",
      "- Layer 6 upper: MI(X;M)=3.588, MI(Y;M)=2.666 | lower: MI(X;M)=2.132, MI(Y;M)=1.760 | bin: MI(X;M)=3.435, MI(Y;M)=2.562\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000300\n",
      "- Layer 0 upper: MI(X;M)=11.348, MI(Y;M)=3.103 | lower: MI(X;M)=6.793, MI(Y;M)=2.482 | bin: MI(X;M)=12.957, MI(Y;M)=3.287\n",
      "- Layer 1 upper: MI(X;M)=9.876, MI(Y;M)=2.958 | lower: MI(X;M)=5.643, MI(Y;M)=2.457 | bin: MI(X;M)=12.869, MI(Y;M)=3.290\n",
      "- Layer 2 upper: MI(X;M)=9.167, MI(Y;M)=2.930 | lower: MI(X;M)=5.373, MI(Y;M)=2.564 | bin: MI(X;M)=12.594, MI(Y;M)=3.274\n",
      "- Layer 3 upper: MI(X;M)=8.836, MI(Y;M)=2.945 | lower: MI(X;M)=5.433, MI(Y;M)=2.707 | bin: MI(X;M)=12.367, MI(Y;M)=3.262\n",
      "- Layer 4 upper: MI(X;M)=8.796, MI(Y;M)=2.977 | lower: MI(X;M)=5.741, MI(Y;M)=2.791 | bin: MI(X;M)=11.575, MI(Y;M)=3.220\n",
      "- Layer 5 upper: MI(X;M)=8.176, MI(Y;M)=2.979 | lower: MI(X;M)=5.701, MI(Y;M)=2.817 | bin: MI(X;M)=10.033, MI(Y;M)=3.155\n",
      "- Layer 6 upper: MI(X;M)=3.590, MI(Y;M)=2.682 | lower: MI(X;M)=2.167, MI(Y;M)=1.796 | bin: MI(X;M)=3.429, MI(Y;M)=2.583\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000400\n",
      "- Layer 0 upper: MI(X;M)=11.429, MI(Y;M)=3.108 | lower: MI(X;M)=6.869, MI(Y;M)=2.472 | bin: MI(X;M)=12.975, MI(Y;M)=3.285\n",
      "- Layer 1 upper: MI(X;M)=10.024, MI(Y;M)=2.965 | lower: MI(X;M)=5.733, MI(Y;M)=2.449 | bin: MI(X;M)=12.926, MI(Y;M)=3.292\n",
      "- Layer 2 upper: MI(X;M)=9.314, MI(Y;M)=2.940 | lower: MI(X;M)=5.447, MI(Y;M)=2.560 | bin: MI(X;M)=12.630, MI(Y;M)=3.277\n",
      "- Layer 3 upper: MI(X;M)=8.946, MI(Y;M)=2.956 | lower: MI(X;M)=5.492, MI(Y;M)=2.713 | bin: MI(X;M)=12.407, MI(Y;M)=3.265\n",
      "- Layer 4 upper: MI(X;M)=8.860, MI(Y;M)=2.988 | lower: MI(X;M)=5.787, MI(Y;M)=2.800 | bin: MI(X;M)=11.531, MI(Y;M)=3.225\n",
      "- Layer 5 upper: MI(X;M)=8.180, MI(Y;M)=2.989 | lower: MI(X;M)=5.725, MI(Y;M)=2.826 | bin: MI(X;M)=9.973, MI(Y;M)=3.159\n",
      "- Layer 6 upper: MI(X;M)=3.592, MI(Y;M)=2.690 | lower: MI(X;M)=2.194, MI(Y;M)=1.819 | bin: MI(X;M)=3.425, MI(Y;M)=2.592\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000500\n",
      "- Layer 0 upper: MI(X;M)=11.483, MI(Y;M)=3.109 | lower: MI(X;M)=6.906, MI(Y;M)=2.459 | bin: MI(X;M)=12.985, MI(Y;M)=3.287\n",
      "- Layer 1 upper: MI(X;M)=10.150, MI(Y;M)=2.972 | lower: MI(X;M)=5.811, MI(Y;M)=2.441 | bin: MI(X;M)=12.981, MI(Y;M)=3.293\n",
      "- Layer 2 upper: MI(X;M)=9.453, MI(Y;M)=2.948 | lower: MI(X;M)=5.524, MI(Y;M)=2.557 | bin: MI(X;M)=12.671, MI(Y;M)=3.279\n",
      "- Layer 3 upper: MI(X;M)=9.054, MI(Y;M)=2.967 | lower: MI(X;M)=5.560, MI(Y;M)=2.718 | bin: MI(X;M)=12.442, MI(Y;M)=3.271\n",
      "- Layer 4 upper: MI(X;M)=8.930, MI(Y;M)=3.000 | lower: MI(X;M)=5.844, MI(Y;M)=2.807 | bin: MI(X;M)=11.541, MI(Y;M)=3.227\n",
      "- Layer 5 upper: MI(X;M)=8.168, MI(Y;M)=2.997 | lower: MI(X;M)=5.740, MI(Y;M)=2.831 | bin: MI(X;M)=9.874, MI(Y;M)=3.152\n",
      "- Layer 6 upper: MI(X;M)=3.588, MI(Y;M)=2.688 | lower: MI(X;M)=2.217, MI(Y;M)=1.840 | bin: MI(X;M)=3.418, MI(Y;M)=2.591\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000600\n",
      "- Layer 0 upper: MI(X;M)=11.518, MI(Y;M)=3.108 | lower: MI(X;M)=6.921, MI(Y;M)=2.443 | bin: MI(X;M)=13.003, MI(Y;M)=3.286\n",
      "- Layer 1 upper: MI(X;M)=10.257, MI(Y;M)=2.975 | lower: MI(X;M)=5.872, MI(Y;M)=2.431 | bin: MI(X;M)=13.019, MI(Y;M)=3.300\n",
      "- Layer 2 upper: MI(X;M)=9.582, MI(Y;M)=2.954 | lower: MI(X;M)=5.589, MI(Y;M)=2.550 | bin: MI(X;M)=12.708, MI(Y;M)=3.284\n",
      "- Layer 3 upper: MI(X;M)=9.176, MI(Y;M)=2.975 | lower: MI(X;M)=5.625, MI(Y;M)=2.718 | bin: MI(X;M)=12.461, MI(Y;M)=3.274\n",
      "- Layer 4 upper: MI(X;M)=9.023, MI(Y;M)=3.009 | lower: MI(X;M)=5.906, MI(Y;M)=2.810 | bin: MI(X;M)=11.585, MI(Y;M)=3.232\n",
      "- Layer 5 upper: MI(X;M)=8.216, MI(Y;M)=3.005 | lower: MI(X;M)=5.785, MI(Y;M)=2.834 | bin: MI(X;M)=9.861, MI(Y;M)=3.160\n",
      "- Layer 6 upper: MI(X;M)=3.587, MI(Y;M)=2.684 | lower: MI(X;M)=2.232, MI(Y;M)=1.849 | bin: MI(X;M)=3.412, MI(Y;M)=2.594\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000700\n",
      "- Layer 0 upper: MI(X;M)=11.545, MI(Y;M)=3.107 | lower: MI(X;M)=6.931, MI(Y;M)=2.434 | bin: MI(X;M)=13.019, MI(Y;M)=3.284\n",
      "- Layer 1 upper: MI(X;M)=10.351, MI(Y;M)=2.982 | lower: MI(X;M)=5.937, MI(Y;M)=2.430 | bin: MI(X;M)=13.060, MI(Y;M)=3.303\n",
      "- Layer 2 upper: MI(X;M)=9.694, MI(Y;M)=2.962 | lower: MI(X;M)=5.661, MI(Y;M)=2.552 | bin: MI(X;M)=12.721, MI(Y;M)=3.284\n",
      "- Layer 3 upper: MI(X;M)=9.284, MI(Y;M)=2.984 | lower: MI(X;M)=5.700, MI(Y;M)=2.724 | bin: MI(X;M)=12.490, MI(Y;M)=3.274\n",
      "- Layer 4 upper: MI(X;M)=9.078, MI(Y;M)=3.015 | lower: MI(X;M)=5.958, MI(Y;M)=2.813 | bin: MI(X;M)=11.564, MI(Y;M)=3.237\n",
      "- Layer 5 upper: MI(X;M)=8.211, MI(Y;M)=3.005 | lower: MI(X;M)=5.802, MI(Y;M)=2.834 | bin: MI(X;M)=9.794, MI(Y;M)=3.156\n",
      "- Layer 6 upper: MI(X;M)=3.584, MI(Y;M)=2.681 | lower: MI(X;M)=2.243, MI(Y;M)=1.856 | bin: MI(X;M)=3.413, MI(Y;M)=2.594\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000800\n",
      "- Layer 0 upper: MI(X;M)=11.584, MI(Y;M)=3.108 | lower: MI(X;M)=6.960, MI(Y;M)=2.428 | bin: MI(X;M)=13.007, MI(Y;M)=3.283\n",
      "- Layer 1 upper: MI(X;M)=10.452, MI(Y;M)=2.988 | lower: MI(X;M)=6.010, MI(Y;M)=2.429 | bin: MI(X;M)=13.068, MI(Y;M)=3.301\n",
      "- Layer 2 upper: MI(X;M)=9.808, MI(Y;M)=2.967 | lower: MI(X;M)=5.736, MI(Y;M)=2.551 | bin: MI(X;M)=12.733, MI(Y;M)=3.285\n",
      "- Layer 3 upper: MI(X;M)=9.374, MI(Y;M)=2.988 | lower: MI(X;M)=5.755, MI(Y;M)=2.724 | bin: MI(X;M)=12.502, MI(Y;M)=3.276\n",
      "- Layer 4 upper: MI(X;M)=9.105, MI(Y;M)=3.013 | lower: MI(X;M)=5.977, MI(Y;M)=2.813 | bin: MI(X;M)=11.555, MI(Y;M)=3.231\n",
      "- Layer 5 upper: MI(X;M)=8.193, MI(Y;M)=2.999 | lower: MI(X;M)=5.797, MI(Y;M)=2.833 | bin: MI(X;M)=9.745, MI(Y;M)=3.153\n",
      "- Layer 6 upper: MI(X;M)=3.583, MI(Y;M)=2.678 | lower: MI(X;M)=2.250, MI(Y;M)=1.859 | bin: MI(X;M)=3.413, MI(Y;M)=2.593\n",
      "Doing rawdata/tanh_32-28-24-20-16-12-8-8/epoch00000900\n",
      "- Layer 0 upper: MI(X;M)=11.612, MI(Y;M)=3.108 | lower: MI(X;M)=6.979, MI(Y;M)=2.422 | bin: MI(X;M)=13.017, MI(Y;M)=3.286\n",
      "- Layer 1 upper: MI(X;M)=10.537, MI(Y;M)=2.994 | lower: MI(X;M)=6.078, MI(Y;M)=2.431 | bin: MI(X;M)=13.069, MI(Y;M)=3.303\n",
      "- Layer 2 upper: MI(X;M)=9.915, MI(Y;M)=2.975 | lower: MI(X;M)=5.814, MI(Y;M)=2.555 | bin: MI(X;M)=12.796, MI(Y;M)=3.285\n",
      "- Layer 3 upper: MI(X;M)=9.460, MI(Y;M)=2.997 | lower: MI(X;M)=5.817, MI(Y;M)=2.728 | bin: MI(X;M)=12.468, MI(Y;M)=3.278\n",
      "- Layer 4 upper: MI(X;M)=9.144, MI(Y;M)=3.020 | lower: MI(X;M)=6.017, MI(Y;M)=2.815 | bin: MI(X;M)=11.521, MI(Y;M)=3.235\n",
      "- Layer 5 upper: MI(X;M)=8.182, MI(Y;M)=3.005 | lower: MI(X;M)=5.807, MI(Y;M)=2.835 | bin: MI(X;M)=9.686, MI(Y;M)=3.145\n",
      "- Layer 6 upper: MI(X;M)=3.579, MI(Y;M)=2.678 | lower: MI(X;M)=2.263, MI(Y;M)=1.871 | bin: MI(X;M)=3.411, MI(Y;M)=2.598\n"
     ]
    }
   ],
   "source": [
    "for activation in measures.keys():\n",
    "    cur_dir = 'rawdata/' + DIR_TEMPLATE % activation\n",
    "    if not os.path.exists(cur_dir):\n",
    "        print(\"Directory %s not found\" % cur_dir)\n",
    "        continue\n",
    "        \n",
    "    # Load files saved during each epoch, and compute MI measures of the activity in that epoch\n",
    "    print('*** Doing %s ***' % cur_dir)\n",
    "    for epochfile in sorted(os.listdir(cur_dir)):\n",
    "        if not epochfile.startswith('epoch'):\n",
    "            continue\n",
    "            \n",
    "        fname = cur_dir + \"/\" + epochfile\n",
    "        with open(fname, 'rb') as f:\n",
    "            d = cPickle.load(f)\n",
    "\n",
    "        epoch = d['epoch']\n",
    "        if epoch in measures[activation]: # Skip this epoch if its already been processed\n",
    "            continue                      # this is a trick to allow us to rerun this cell multiple times)\n",
    "            \n",
    "        if epoch > MAX_EPOCHS:\n",
    "            continue\n",
    "\n",
    "        print(\"Doing\", fname)\n",
    "        \n",
    "        num_layers = len(d['data']['activity_tst'])\n",
    "\n",
    "        if PLOT_LAYERS is None:\n",
    "            PLOT_LAYERS = []\n",
    "            for lndx in range(num_layers):\n",
    "                #if d['data']['activity_tst'][lndx].shape[1] < 200 and lndx != num_layers - 1:\n",
    "                PLOT_LAYERS.append(lndx)\n",
    "                \n",
    "        cepochdata = defaultdict(list)\n",
    "        for lndx in range(num_layers):\n",
    "            activity = d['data']['activity_tst'][lndx]\n",
    "\n",
    "            # Compute marginal entropies\n",
    "            h_upper = entropy_func_upper([activity,])[0]\n",
    "            if DO_LOWER:\n",
    "                h_lower = entropy_func_lower([activity,])[0]\n",
    "                \n",
    "            # Layer activity given input. This is simply the entropy of the Gaussian noise\n",
    "            hM_given_X = kde_condentropy(activity, noise_variance)\n",
    "\n",
    "            # Compute conditional entropies of layer activity given output\n",
    "            hM_given_Y_upper=0.\n",
    "            for i in range(10):\n",
    "                hcond_upper = entropy_func_upper([activity[saved_labelixs[i],:],])[0]\n",
    "                hM_given_Y_upper += labelprobs[i] * hcond_upper\n",
    "                \n",
    "            if DO_LOWER:\n",
    "                hM_given_Y_lower=0.\n",
    "                for i in range(10):\n",
    "                    hcond_lower = entropy_func_lower([activity[saved_labelixs[i],:],])[0]\n",
    "                    hM_given_Y_lower += labelprobs[i] * hcond_lower\n",
    "                    \n",
    "                    \n",
    "            # # It's also possible to treat the last layer probabilistically. Here is the \n",
    "            # # code to do so. Should only be applied when lndx == num_layers - 1\n",
    "\n",
    "            # ps = activity.mean(axis=0)\n",
    "            # h_lower = h_upper = sum([-p*np.log(p) for p in ps if p != 0])\n",
    "\n",
    "            # x = -activity * np.log(activity)\n",
    "            # x[activity == 0] = 0.\n",
    "            # hM_given_X = np.mean(x.sum(axis=1))\n",
    "\n",
    "            # hM_given_Y=0.\n",
    "            # for i in range(10):\n",
    "            #     ixs = tst.y[::subsample] == i\n",
    "            #     ps = activity[ixs,:].mean(axis=0)\n",
    "            #     hcond = sum([-p*np.log(p) for p in ps if p != 0])\n",
    "            #     prob = np.mean(ixs)\n",
    "            #     hM_given_Y += l * hcond\n",
    "            # hM_given_Y_lower = hM_given_Y_upper = hM_given_Y\n",
    "            # del hM_given_Y\n",
    "                \n",
    "            cepochdata['MI_XM_upper'].append( nats2bits * (h_upper - hM_given_X) )\n",
    "            cepochdata['MI_YM_upper'].append( nats2bits * (h_upper - hM_given_Y_upper) )\n",
    "            cepochdata['H_M_upper'  ].append( nats2bits * h_upper )\n",
    "\n",
    "            pstr = 'upper: MI(X;M)=%0.3f, MI(Y;M)=%0.3f' % (cepochdata['MI_XM_upper'][-1], cepochdata['MI_YM_upper'][-1])\n",
    "            if DO_LOWER:  # Compute lower bounds\n",
    "                cepochdata['MI_XM_lower'].append( nats2bits * (h_lower - hM_given_X) )\n",
    "                cepochdata['MI_YM_lower'].append( nats2bits * (h_lower - hM_given_Y_lower) )\n",
    "                cepochdata['H_M_lower'  ].append( nats2bits * h_lower )\n",
    "                pstr += ' | lower: MI(X;M)=%0.3f, MI(Y;M)=%0.3f' % (cepochdata['MI_XM_lower'][-1], cepochdata['MI_YM_lower'][-1])\n",
    "\n",
    "            if DO_BINNED: # Compute binner estimates\n",
    "                binxm, binym = bin_calc_information2(saved_labelixs, activity, 0.5)\n",
    "                cepochdata['MI_XM_bin'].append( nats2bits * binxm )\n",
    "                cepochdata['MI_YM_bin'].append( nats2bits * binym )\n",
    "                pstr += ' | bin: MI(X;M)=%0.3f, MI(Y;M)=%0.3f' % (cepochdata['MI_XM_bin'][-1], cepochdata['MI_YM_bin'][-1])\n",
    "            \n",
    "            print('- Layer %d %s' % (lndx, pstr) )\n",
    "\n",
    "        measures[activation][epoch] = cepochdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_epoch = max( (max(vals.keys()) if len(vals) else 0) for vals in measures.values())\n",
    "sm = plt.cm.ScalarMappable(cmap='gnuplot', norm=plt.Normalize(vmin=0, vmax=COLORBAR_MAX_EPOCHS))\n",
    "sm._A = []\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "for actndx, (activation, vals) in enumerate(measures.items()):\n",
    "    epochs = sorted(vals.keys())\n",
    "    if not len(epochs):\n",
    "        continue\n",
    "    plt.subplot(1,2,actndx+1)    \n",
    "    for epoch in epochs:\n",
    "        c = sm.to_rgba(epoch)\n",
    "        xmvals = np.array(vals[epoch]['MI_XM_'+infoplane_measure])[PLOT_LAYERS]\n",
    "        ymvals = np.array(vals[epoch]['MI_YM_'+infoplane_measure])[PLOT_LAYERS]\n",
    "\n",
    "        plt.plot(xmvals, ymvals, c=c, alpha=0.1, zorder=1)\n",
    "        plt.scatter(xmvals, ymvals, s=20, facecolors=[c for _ in PLOT_LAYERS], edgecolor='none', zorder=2)\n",
    "    \n",
    "    plt.ylim([0, 3.5])\n",
    "    plt.xlim([0, 14])\n",
    "    plt.xlabel('I(X;M)')\n",
    "    plt.ylabel('I(Y;M)')\n",
    "    plt.title(activation)\n",
    "    \n",
    "cbaxes = fig.add_axes([1.0, 0.125, 0.03, 0.8]) \n",
    "plt.colorbar(sm, label='Epoch', cax=cbaxes)\n",
    "plt.show()\n",
    "\n",
    "if DO_SAVE:\n",
    "    plt.savefig('plots/' + DIR_TEMPLATE % ('infoplane_'+ARCH),bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm running on a CPU, so I can't do a TON of epochs. Some interesting next steps would be to tweak the model, adding some layers that aren't fully connected and a dropout layer and then seeing where things go. In general, however, it seems like the same general pattern would be displayed.\n",
    "\n",
    "Thanks for letting me take the challenge- I definitely learned a lot during the process.\n",
    "-Stephen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
